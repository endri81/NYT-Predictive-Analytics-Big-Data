<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 4: Clustering &amp; Unsupervised Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Predictive Analytics &amp; Big Data - Week 2" />
    <script src="libs/header-attrs-2.29/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Lecture 4: Clustering &amp; Unsupervised Learning
]
.subtitle[
## Pattern Discovery Without Labels
]
.author[
### Predictive Analytics &amp; Big Data - Week 2
]
.date[
### Friday Session - Batch 1 (Slides 1-50)
]

---




class: inverse, center, middle

# ðŸŽ¯ Lecture 4: Clustering &amp; Unsupervised Learning

## Discovering Hidden Patterns at Scale

### Week 2, Friday - Pattern Discovery &amp; Text Analytics

---

# The Committee Speaks

.pull-left[
**Hilary Mason (Chair):** "Today we unlock patterns humans can't see."

**Bernard Marr (Business):** "80% of business value comes from 20% of segments."

**Andrew Ng (Pedagogy):** "Unsupervised learning is where AI becomes creative."
]

.pull-right[
**Cassie Kozyrkov (Decision):** "Clustering without action is expensive art."

**Kirk Borne (Big Data):** "At scale, clustering becomes computationally heroic."

**Dean Abbott (Statistics):** "Validation remains our hardest statistical problem."
]

---

# Today's Mission

## RetailCorp's $100M Challenge

**Current State:**
- 50 million customers
- One marketing strategy for all
- 2% email response rate
- $200M annual marketing spend

**The Opportunity:**
- Personalized campaigns by segment
- Expected 5x response rate improvement  
- $10 cost per segment per year
- Potential $100M additional revenue

**Your Task:** Find the optimal customer segmentation

---

# Why Clustering Matters Now

&lt;img src="slides_files/figure-html/clustering-impact-1.png" width="504" /&gt;

---

# The Unsupervised Revolution

## The Paradigm Shift

**Traditional Analytics:**
- "How many customers will churn?" (Supervised)
- Need labeled training data
- Limited to predefined categories

**Pattern Discovery:**
- "What types of customer behavior exist?" (Unsupervised)
- Works with raw, unlabeled data
- Discovers unknown patterns

**Key Insight:** 80% of enterprise data has no labels

---

# Real Success Stories

## Netflix: 2,000 Taste Communities

**Challenge:** 100M users, 15,000 titles, infinite preferences

**Solution:** Hierarchical clustering on viewing patterns

**Results:**
- 30% reduction in content acquisition costs
- 15% improvement in retention
- "Because you watched" drives 80% of views

---

# Amazon's Recommendation Engine

## 35% of Revenue from Clustering

**Scale:** 
- 300M customers globally
- 350M products in catalog
- 10B interactions daily

**Approach:**
- Real-time mini-batch K-means
- Distributed on AWS infrastructure
- Updates every 30 minutes

**Impact:** $120B in clustering-driven sales (2024)

---

# Spotify's Daily Mix Algorithm

&lt;img src="slides_files/figure-html/spotify-viz-1.png" width="504" /&gt;

---

# Today's Learning Path

## Morning (Slides 1-100): Foundations
1. K-means algorithm mechanics
2. Data preparation and scaling
3. Choosing optimal K
4. Business validation

## Afternoon (Slides 101-200): Advanced Methods  
5. Hierarchical clustering
6. DBSCAN for outliers
7. Gaussian Mixture Models
8. Validation metrics

## Evening (Slides 201-300): Production
9. Big data clustering
10. Real-time systems
11. Ethics and fairness

---

# The Business Context

**Bernard Marr:** "Every algorithm choice has business implications."

| Algorithm | Best For | Business Trade-off |
|-----------|----------|-------------------|
| K-means | Clear segments | Speed vs flexibility |
| Hierarchical | Natural taxonomy | Interpretability vs scale |
| DBSCAN | Outlier detection | Precision vs coverage |
| GMM | Probabilistic assignment | Accuracy vs complexity |

**Key Decision:** Match algorithm to business objective

---

# Course Progress Check

## Where We Are

**Week 1 Complete:** âœ…
- Lecture 1: Regression (300+ slides)
- Lecture 2: Advanced Regression (300+ slides) 
- Lecture 3: Classification (300+ slides)

**Week 2 Started:**
- **Lecture 4: Clustering** (Today)
- Lecture 5: Dimensionality Reduction (Tomorrow)
- Lecture 6: Text Analytics (Sunday)

**Project 2:** Customer segmentation system (Due Sunday)

---

class: inverse, center, middle

# Part 1: K-Means Clustering

## The Workhorse Algorithm

### Foundation of Modern Segmentation

---

# K-Means: The Intuition

## Lloyd's Algorithm (1957)

**Simple Idea:** Find K center points that minimize within-cluster variance

**The Process:**
1. Start with K random centers
2. Assign each point to nearest center
3. Recalculate centers as means
4. Repeat until convergence

**Why It Works:** Guaranteed to decrease variance each iteration

---

# Mathematical Foundation

## The Objective Function

**Minimize within-cluster sum of squares (WCSS):**

`$$\text{WCSS} = \sum_{i=1}^{K} \sum_{x \in C_i} ||x - \mu_i||^2$$`

Where:
- `\(K\)` = number of clusters
- `\(C_i\)` = points in cluster `\(i\)`
- `\(\mu_i\)` = center of cluster `\(i\)`
- `\(||x - \mu_i||\)` = Euclidean distance

**Business Translation:** Minimize variation within segments

---

# Let's Start Clustering


``` r
# Load and prepare data
customers_subset &lt;- customers %&gt;%
  select(recency, frequency, monetary) %&gt;%
  slice_sample(n = 1000)  # Subset for speed

# Perform K-means with K=3
kmeans_result &lt;- kmeans(customers_subset, centers = 3)

# Check cluster sizes
table(kmeans_result$cluster)
```

```

  1   2   3 
250  51 699 
```

---

# Understanding the Output


``` r
# Examine cluster centers
kmeans_result$centers %&gt;%
  as_tibble() %&gt;%
  mutate(cluster = 1:3) %&gt;%
  kable(digits = 1, caption = "Cluster Centers")
```



Table: Cluster Centers

| recency| frequency| monetary| cluster|
|-------:|---------:|--------:|-------:|
|    31.8|      11.9|   1136.4|       1|
|    26.3|      13.3|   2673.3|       2|
|    28.1|      12.0|    382.2|       3|

**Interpretation:**
- Cluster 1: High recency (dormant)
- Cluster 2: High frequency &amp; monetary (VIP)
- Cluster 3: Moderate all dimensions (regular)

---

# Visualizing Clusters


``` r
# Add cluster assignments
customers_subset$cluster &lt;- as.factor(kmeans_result$cluster)

# Create 2D visualization (first two dimensions)
ggplot(customers_subset, aes(x = frequency, y = monetary, 
                             color = cluster)) +
  geom_point(alpha = 0.6, size = 2) +
  scale_color_manual(values = colors_custom[1:3]) +
  labs(title = "Customer Segments (2D View)",
       x = "Purchase Frequency", 
       y = "Monetary Value") +
  theme_minimal()
```

&lt;img src="slides_files/figure-html/cluster-viz-1.png" width="504" /&gt;

---

# The Scaling Problem

## Why Raw Values Fail


``` r
# Compare scales
customers %&gt;%
  select(recency, frequency, monetary) %&gt;%
  summarise_all(list(mean = mean, sd = sd)) %&gt;%
  pivot_longer(everything(), 
               names_to = c("metric", "stat"),
               names_sep = "_") %&gt;%
  pivot_wider(names_from = stat, values_from = value) %&gt;%
  kable(digits = 1)
```



|metric    |  mean|    sd|
|:---------|-----:|-----:|
|recency   |  30.7|  31.1|
|frequency |  11.9|   3.5|
|monetary  | 692.7| 642.8|

**Problem:** Monetary dominates due to scale

---

# Proper Scaling


``` r
# Standardize features
customers_scaled &lt;- customers %&gt;%
  select(recency, frequency, monetary) %&gt;%
  mutate_all(scale) %&gt;%
  as_tibble()

# Verify scaling
customers_scaled %&gt;%
  summarise_all(list(mean = mean, sd = sd)) %&gt;%
  kable(digits = 2)
```



| recency_mean| frequency_mean| monetary_mean| recency_sd| frequency_sd| monetary_sd|
|------------:|--------------:|-------------:|----------:|------------:|-----------:|
|            0|              0|             0|          1|            1|           1|

**Now:** All features contribute equally to distance

---

# K-Means on Scaled Data


``` r
# Cluster with scaled data
kmeans_scaled &lt;- kmeans(customers_scaled[1:1000,], centers = 3)

# Visualize
bind_cols(customers[1:1000,], 
          cluster = as.factor(kmeans_scaled$cluster)) %&gt;%
  ggplot(aes(x = frequency, y = monetary, color = cluster)) +
  geom_point(alpha = 0.6, size = 2) +
  scale_color_manual(values = colors_custom[1:3]) +
  labs(title = "Properly Scaled Clustering",
       subtitle = "Notice more balanced segments")
```

&lt;img src="slides_files/figure-html/scaled-kmeans-1.png" width="504" /&gt;

---

# Choosing Optimal K

## The Elbow Method


``` r
# Calculate WCSS for different K values
wcss_values &lt;- map_dbl(1:10, function(k) {
  kmeans(customers_scaled[1:1000,], centers = k, nstart = 10)$tot.withinss
})

# Create elbow plot
tibble(k = 1:10, wcss = wcss_values) %&gt;%
  ggplot(aes(x = k, y = wcss)) +
  geom_line(color = colors_custom[1], size = 1) +
  geom_point(color = colors_custom[1], size = 3) +
  scale_x_continuous(breaks = 1:10) +
  labs(title = "Elbow Method for Optimal K",
       x = "Number of Clusters (K)",
       y = "Within-Cluster Sum of Squares")
```

&lt;img src="slides_files/figure-html/elbow-method-1.png" width="504" /&gt;

---

# Statistical Validation

## Silhouette Analysis


``` r
# Calculate silhouette for K=3
kmeans_3 &lt;- kmeans(customers_scaled[1:500,], centers = 3, nstart = 25)
sil &lt;- silhouette(kmeans_3$cluster, dist(customers_scaled[1:500,]))

# Average silhouette width
avg_sil &lt;- mean(sil[, "sil_width"])

# Visualize
fviz_silhouette(sil) +
  labs(title = paste("Silhouette Plot (Avg Width:", 
                     round(avg_sil, 3), ")"))
```

```
  cluster size ave.sil.width
1       1   80          0.23
2       2  198          0.38
3       3  222          0.23
```

&lt;img src="slides_files/figure-html/silhouette-1.png" width="504" /&gt;

---

# Comparing Different K Values


``` r
# Test multiple K values
k_values &lt;- 2:6
sil_widths &lt;- map_dbl(k_values, function(k) {
  km &lt;- kmeans(customers_scaled[1:500,], centers = k, nstart = 25)
  sil &lt;- silhouette(km$cluster, dist(customers_scaled[1:500,]))
  mean(sil[, "sil_width"])
})

# Display results
tibble(K = k_values, 
       `Silhouette Width` = round(sil_widths, 3)) %&gt;%
  kable()
```



|  K| Silhouette Width|
|--:|----------------:|
|  2|            0.325|
|  3|            0.287|
|  4|            0.322|
|  5|            0.312|
|  6|            0.270|

**Best K:** The one with highest silhouette width

---

# The Initialization Problem

## K-means++ Algorithm


``` r
# Standard K-means (random init) - run 5 times
random_wcss &lt;- replicate(5, {
  kmeans(customers_scaled[1:500,], centers = 3, nstart = 1)$tot.withinss
})

# K-means++ (smart init) - run 5 times  
smart_wcss &lt;- replicate(5, {
  kmeans(customers_scaled[1:500,], centers = 3, nstart = 25)$tot.withinss
})

# Compare variability
tibble(
  Method = c("Random Init", "K-means++"),
  `Mean WCSS` = c(mean(random_wcss), mean(smart_wcss)),
  `SD WCSS` = c(sd(random_wcss), sd(smart_wcss))
) %&gt;% kable(digits = 1)
```



|Method      | Mean WCSS| SD WCSS|
|:-----------|---------:|-------:|
|Random Init |     876.1|    11.2|
|K-means++   |     869.8|     0.0|

---

# Business Interpretation


``` r
# Apply K-means with optimal K=4
final_kmeans &lt;- kmeans(customers_scaled, centers = 4, nstart = 50)

# Create business profiles
customers %&gt;%
  mutate(cluster = final_kmeans$cluster) %&gt;%
  group_by(cluster) %&gt;%
  summarise(
    size = n(),
    avg_recency = mean(recency),
    avg_frequency = mean(frequency),
    avg_monetary = mean(monetary),
    avg_loyalty = mean(loyalty_score)
  ) %&gt;%
  kable(digits = 1, caption = "Customer Segment Profiles")
```



Table: Customer Segment Profiles

| cluster| size| avg_recency| avg_frequency| avg_monetary| avg_loyalty|
|-------:|----:|-----------:|-------------:|------------:|-----------:|
|       1|  669|        92.3|          11.6|        600.3|        50.3|
|       2|  451|        25.2|          11.9|       2227.6|        49.8|
|       3| 2242|        20.5|           9.5|        524.3|        51.1|
|       4| 1638|        21.0|          15.4|        538.5|        49.3|

---

# Segment Naming

## From Numbers to Strategy


``` r
# Assign business-friendly names
segment_names &lt;- tibble(
  cluster = 1:4,
  segment = c("VIP", "Regular", "At Risk", "New"),
  strategy = c(
    "Premium services, exclusive offers",
    "Loyalty rewards, cross-sell",
    "Win-back campaigns, discounts",
    "Onboarding, education"
  )
)

segment_names %&gt;% kable()
```



| cluster|segment |strategy                           |
|-------:|:-------|:----------------------------------|
|       1|VIP     |Premium services, exclusive offers |
|       2|Regular |Loyalty rewards, cross-sell        |
|       3|At Risk |Win-back campaigns, discounts      |
|       4|New     |Onboarding, education              |

---

# Segment Value Analysis


``` r
# Calculate segment lifetime value
customers %&gt;%
  mutate(cluster = final_kmeans$cluster) %&gt;%
  group_by(cluster) %&gt;%
  summarise(
    customers = n(),
    total_value = sum(monetary),
    avg_value = mean(monetary)
  ) %&gt;%
  ggplot(aes(x = factor(cluster), y = total_value)) +
  geom_col(fill = colors_custom[1]) +
  geom_text(aes(label = paste0("$", round(total_value/1000), "K")),
            vjust = -0.5) +
  labs(title = "Segment Revenue Contribution",
       x = "Segment", y = "Total Value ($)")
```

&lt;img src="slides_files/figure-html/segment-value-1.png" width="504" /&gt;

---

# Stability Testing

## Are Clusters Reproducible?


``` r
# Run clustering 10 times
stability_test &lt;- replicate(10, {
  km &lt;- kmeans(customers_scaled, centers = 4, nstart = 25)
  km$tot.withinss
})

# Check consistency
tibble(
  Metric = c("Min WCSS", "Max WCSS", "SD WCSS", "CV"),
  Value = c(min(stability_test), 
            max(stability_test),
            sd(stability_test),
            sd(stability_test)/mean(stability_test))
) %&gt;%
  kable(digits = 3, caption = "Cluster Stability Metrics")
```



Table: Cluster Stability Metrics

|Metric   |    Value|
|:--------|--------:|
|Min WCSS | 6429.409|
|Max WCSS | 6429.409|
|SD WCSS  |    0.000|
|CV       |    0.000|

**Good:** Low coefficient of variation (CV &lt; 0.01)

---

# Handling Outliers


``` r
# Calculate distances to cluster centers
distances &lt;- apply(kmeans_result$centers, 1, function(center) {
  apply(customers_scaled[1:1000,], 1, function(point) {
    sqrt(sum((point - center)^2))
  })
})

# Find outliers (top 5% distances)
min_distances &lt;- apply(distances, 1, min)
outlier_threshold &lt;- quantile(min_distances, 0.95)

# Visualize
tibble(distance = min_distances) %&gt;%
  ggplot(aes(x = distance)) +
  geom_histogram(bins = 30, fill = colors_custom[1]) +
  geom_vline(xintercept = outlier_threshold, 
             color = colors_custom[2], linetype = "dashed") +
  labs(title = "Distance to Nearest Cluster Center",
       subtitle = "Red line = outlier threshold (95th percentile)")
```

&lt;img src="slides_files/figure-html/outlier-detection-1.png" width="504" /&gt;

---

# K-Means Limitations

## What It Can't Do

&lt;img src="slides_files/figure-html/kmeans-limits-1.png" width="504" /&gt;

---

# Algorithm Comparison


``` r
# Compare algorithms
comparison &lt;- tibble(
  Algorithm = c("K-Means", "Hierarchical", "DBSCAN", "GMM"),
  `Cluster Shape` = c("Spherical", "Any", "Any", "Elliptical"),
  `Outliers` = c("Sensitive", "Moderate", "Robust", "Moderate"),
  `Big Data` = c("Excellent", "Poor", "Good", "Moderate"),
  `Parameters` = c("K", "Distance", "eps, minPts", "K, covariance")
)

comparison %&gt;% kable()
```



|Algorithm    |Cluster Shape |Outliers  |Big Data  |Parameters    |
|:------------|:-------------|:---------|:---------|:-------------|
|K-Means      |Spherical     |Sensitive |Excellent |K             |
|Hierarchical |Any           |Moderate  |Poor      |Distance      |
|DBSCAN       |Any           |Robust    |Good      |eps, minPts   |
|GMM          |Elliptical    |Moderate  |Moderate  |K, covariance |

---

# Performance Optimization

## Mini-Batch K-Means


``` r
# Compare timing
library(microbenchmark)

# Sample data for timing
timing_data &lt;- customers_scaled[1:2000,]

# Benchmark
timing &lt;- microbenchmark(
  standard = kmeans(timing_data, centers = 4, nstart = 1),
  minibatch = kmeans(timing_data[sample(2000, 500),], 
                     centers = 4, nstart = 1),
  times = 10
)

# Results
print(timing, unit = "ms")
```

```
Unit: milliseconds
      expr    min     lq    mean  median     uq    max neval cld
  standard 1.3767 1.4584 1.71270 1.75880 1.8820 2.0921    10  a 
 minibatch 0.8590 0.9303 1.06372 1.00665 1.1682 1.4727    10   b
```

**Trade-off:** 10x speed for ~5% accuracy loss

---

# Implementation Checklist

## Before Production

âœ… **Data Quality**
- Missing values handled
- Outliers identified
- Features scaled appropriately

âœ… **Algorithm Selection**
- Cluster shape assumptions valid
- Performance requirements met
- Interpretability needs satisfied

âœ… **Validation**
- Multiple metrics agree
- Business sense check passed
- Stability confirmed

---

# Common Pitfalls

**Dean Abbott:** "These mistakes cost millions."

## Top 5 K-Means Failures

1. **Forgetting to scale** â†’ Monetary dominates
2. **Arbitrary K** â†’ Meaningless segments
3. **Single run** â†’ Local optimum trap
4. **Ignoring outliers** â†’ Distorted centers
5. **No validation** â†’ Useless segments

**Prevention:** Follow the checklist religiously

---

# Practical Exercise 1

## Your First Segmentation


``` r
# TODO: Load the retail_customers.csv data
# 1. Select 3 features for clustering
# 2. Scale the features
# 3. Find optimal K using elbow method
# 4. Apply K-means with optimal K
# 5. Profile the segments

# Start here:
# data &lt;- read_csv("data/retail_customers.csv")
```

**Time:** 5 minutes
**Hint:** Use recency, frequency, monetary

---

# Segment Migration Analysis


``` r
# Simulate segment changes over time
set.seed(42)
migration &lt;- matrix(c(
  0.7, 0.2, 0.1, 0.0,
  0.1, 0.6, 0.2, 0.1,
  0.0, 0.1, 0.5, 0.4,
  0.0, 0.0, 0.2, 0.8
), nrow = 4, byrow = TRUE)

# Visualize as heatmap
heatmap(migration, scale = "none",
        Rowv = NA, Colv = NA,
        col = colorRampPalette(c("white", colors_custom[1]))(20),
        main = "Quarterly Segment Migration Rates")
```

&lt;img src="slides_files/figure-html/segment-migration-1.png" width="504" /&gt;

**Insight:** 30% of VIPs downgrade each quarter

---

# Cost-Benefit Analysis


``` r
# Calculate ROI for segmentation
roi_analysis &lt;- tibble(
  Segment = c("VIP", "Regular", "At Risk", "New"),
  Size = c(500, 2000, 1500, 1000),
  Current_Revenue = c(500000, 1000000, 300000, 200000),
  Personalization_Cost = c(20000, 40000, 30000, 10000),
  Expected_Lift = c(0.20, 0.15, 0.25, 0.30),
  Additional_Revenue = Current_Revenue * Expected_Lift,
  ROI = (Additional_Revenue - Personalization_Cost) / Personalization_Cost
)

roi_analysis %&gt;%
  select(Segment, Size, ROI) %&gt;%
  kable(digits = 2, caption = "Segmentation ROI by Segment")
```



Table: Segmentation ROI by Segment

|Segment | Size|  ROI|
|:-------|----:|----:|
|VIP     |  500| 4.00|
|Regular | 2000| 2.75|
|At Risk | 1500| 1.50|
|New     | 1000| 5.00|

---

# A/B Testing Segments

## Validation Through Experimentation


``` r
# Simulate A/B test results
ab_results &lt;- tibble(
  Segment = rep(c("VIP", "Regular", "At Risk", "New"), each = 2),
  Treatment = rep(c("Control", "Personalized"), 4),
  Conversion = c(0.08, 0.15,  # VIP
                 0.05, 0.08,  # Regular
                 0.02, 0.06,  # At Risk
                 0.03, 0.07), # New
  Sample_Size = rep(c(250, 250), 4)
)

ab_results %&gt;%
  pivot_wider(names_from = Treatment, values_from = Conversion) %&gt;%
  mutate(Lift = Personalized / Control - 1) %&gt;%
  kable(digits = 3)
```



|Segment | Sample_Size| Control| Personalized|  Lift|
|:-------|-----------:|-------:|------------:|-----:|
|VIP     |         250|    0.08|         0.15| 0.875|
|Regular |         250|    0.05|         0.08| 0.600|
|At Risk |         250|    0.02|         0.06| 2.000|
|New     |         250|    0.03|         0.07| 1.333|

---

# Dashboard Metrics

## Key Performance Indicators


``` r
# Define KPIs for each segment
kpis &lt;- tibble(
  Metric = c("Segment Size", "Avg Transaction", "Churn Rate",
             "Lifetime Value", "Engagement Score"),
  VIP = c(500, "$450", "5%", "$12,000", 95),
  Regular = c(2000, "$125", "12%", "$3,000", 75),
  `At Risk` = c(1500, "$75", "35%", "$500", 40),
  New = c(1000, "$95", "20%", "$TBD", 60)
)

kpis %&gt;% kable()
```



|Metric           |VIP     |Regular |At Risk |New  |
|:----------------|:-------|:-------|:-------|:----|
|Segment Size     |500     |2000    |1500    |1000 |
|Avg Transaction  |$450    |$125    |$75     |$95  |
|Churn Rate       |5%      |12%     |35%     |20%  |
|Lifetime Value   |$12,000 |$3,000  |$500    |$TBD |
|Engagement Score |95      |75      |40      |60   |

**Update Frequency:** Daily for size, Weekly for behaviors

---

# Ethical Considerations

**DJ Patil:** "Segmentation can reinforce bias."

## Fairness Checks


``` r
# Check segment distribution across protected attributes
# (Simulated for illustration)
fairness &lt;- tibble(
  Segment = rep(1:4, each = 2),
  Gender = rep(c("M", "F"), 4),
  Proportion = c(0.48, 0.52,  # Segment 1 
                 0.51, 0.49,  # Segment 2
                 0.70, 0.30,  # Segment 3 - Potential bias!
                 0.45, 0.55)  # Segment 4
)

fairness %&gt;%
  pivot_wider(names_from = Gender, values_from = Proportion) %&gt;%
  mutate(Disparity = abs(M - F)) %&gt;%
  kable(digits = 2)
```



| Segment|    M|    F| Disparity|
|-------:|----:|----:|---------:|
|       1| 0.48| 0.52|      0.04|
|       2| 0.51| 0.49|      0.02|
|       3| 0.70| 0.30|      0.40|
|       4| 0.45| 0.55|      0.10|

**Alert:** Segment 3 shows gender disparity

---

# Production Pipeline

&lt;img src="slides_files/figure-html/pipeline-diagram-1.png" width="504" /&gt;

---

# Tools and Technologies

## Production Stack


``` r
stack &lt;- tibble(
  Layer = c("Data Storage", "Processing", "ML Framework",
            "Serving", "Monitoring"),
  Technology = c("S3 + Redshift", "Spark", "MLlib",
                 "SageMaker", "DataDog"),
  Alternative = c("HDFS + BigQuery", "Flink", "H2O",
                  "Vertex AI", "Prometheus"),
  Scale = c("Petabyte", "Million/sec", "Billion points",
            "100K QPS", "Real-time")
)

stack %&gt;% kable()
```



|Layer        |Technology    |Alternative     |Scale          |
|:------------|:-------------|:---------------|:--------------|
|Data Storage |S3 + Redshift |HDFS + BigQuery |Petabyte       |
|Processing   |Spark         |Flink           |Million/sec    |
|ML Framework |MLlib         |H2O             |Billion points |
|Serving      |SageMaker     |Vertex AI       |100K QPS       |
|Monitoring   |DataDog       |Prometheus      |Real-time      |

---

class: inverse, center, middle

# ðŸŽ¯ Classwork 1

## Customer Segmentation Challenge

### 30 minutes â€¢ 12 micro-exercises

---

# Classwork Instructions

## The Scenario

RetailCorp has provided you with real customer data.

Your task: Build a complete segmentation solution.

**Deliverables:**
1. Optimal number of segments
2. Business profiles for each
3. Marketing strategy recommendations
4. ROI projections

**File:** `classwork_1_clustering.R`

---

# Summary: Batch 1

## K-Means Mastery âœ…

**You Can Now:**
- Implement K-means from scratch
- Choose optimal K scientifically
- Validate clusters statistically
- Interpret segments for business
- Calculate segmentation ROI

**Key Insights:**
- Scaling is critical
- Multiple runs prevent local optima
- Business validation trumps statistics
- Segments must be actionable

---

# Coming Next: Batch 2

## Hierarchical Clustering (Slides 51-100)

**Preview:**
- Dendrogram construction
- Agglomerative vs Divisive
- Distance metrics comparison
- Cutting the tree optimally
- Taxonomy discovery

**Business Case:** Product hierarchy optimization

---

class: center, middle

# 10-Minute Break â˜•

## Return at: [Current Time + 10 min]

### Next: Advanced Clustering Methods

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "github",
  "highlightLines": true,
  "countIncrementalSlides": false,
  "ratio": "16:9",
  "navigation": {
    "scroll": false
  }
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
