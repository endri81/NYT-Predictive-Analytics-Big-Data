---
title: "Feature Engineering & Dimensionality Reduction"
subtitle: "Predictive Analytics & Big Data | Module 4"
author: "World-Class Instructional Team"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
      slideNumberFormat: '%current% / %total%'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10,
  fig.height = 5.5,
  fig.align = 'center'
)

# Load essential libraries
library(tidyverse)
library(caret)
library(recipes)
library(glmnet)
library(randomForest)
library(corrplot)
library(VIM)
library(mice)
library(lubridate)
library(Matrix)
library(kableExtra)
library(DiagrammeR)
library(moments)

# Set theme
theme_set(theme_minimal(base_size = 12))

# Generate working dataset
set.seed(42)
n <- 10000

# Simple working dataset
ecommerce_data <- tibble(
  transaction_id = 1:n,
  customer_id = sample(1:2000, n, replace = TRUE),
  timestamp = seq(as.POSIXct("2024-01-01"), length.out = n, by = "hour"),
  age = round(rnorm(n, 35, 12)),
  income = round(exp(rnorm(n, 10.8, 0.6))),
  total_spent = round(rlnorm(n, 6, 1.5)),
  num_purchases = rpois(n, lambda = 12),
  device = sample(c("mobile", "desktop", "tablet"), n, replace = TRUE),
  channel = sample(c("organic", "paid", "social", "email"), n, replace = TRUE),
  region = sample(c("North", "South", "East", "West"), n, replace = TRUE),
  product_category = sample(c("Electronics", "Clothing", "Home", "Sports"), n, replace = TRUE),
  review_score = sample(c(NA, 1:5), n, replace = TRUE),
  churn = rbinom(n, 1, 0.15)
)

# Simple time series data
time_series_data <- tibble(
  date = seq.Date(from = as.Date("2023-01-01"), 
                  to = as.Date("2025-11-01"), 
                  by = "day"),
  sales = 10000 + rnorm(length(date), 0, 1000)
)
```

---
class: inverse, center, middle

# Module 4: Feature Engineering & Dimensionality Reduction
## A Comprehensive 300-Slide Journey
### Part 1: Slides 1-50

---

# The Feature Engineering Revolution

**"Data is the new oil, but feature engineering is the refinery."**

In this comprehensive module, we will master:

- **100+ feature engineering techniques** across all data types
- **20+ dimensionality reduction methods** from PCA to autoencoders  
- **Production-scale pipelines** handling billions of records
- **Real-world case studies** from Netflix, Uber, Amazon, and Google
- **Cutting-edge methods** including deep learning feature extraction

**By the end:** You'll transform raw data into predictive gold, consistently achieving 20-40% performance improvements.

---

# Module Learning Outcomes

Upon completing all 300 slides, you will be able to:

1. **Extract** valuable signals from structured, unstructured, and streaming data
2. **Apply** 15+ categorical encoding methods optimally 
3. **Generate** 1000+ features automatically using feature tools
4. **Implement** PCA, t-SNE, UMAP, and autoencoders for dimensionality reduction
5. **Build** production pipelines processing TB-scale data
6. **Evaluate** computational complexity and optimize for latency
7. **Deploy** feature stores serving millions of predictions/second
8. **Mitigate** bias and ensure fairness in feature engineering

---

# The $100 Million Impact of Features
```{r impact_story, echo=FALSE, fig.height=5}
# Real company improvements from feature engineering
companies <- tibble(
  Company = factor(c("Netflix", "Uber", "Airbnb", "Amazon", "Spotify"),
                  levels = c("Netflix", "Uber", "Airbnb", "Amazon", "Spotify")),
  `Before FE` = c(65, 70, 60, 72, 68),
  `After FE` = c(94, 89, 85, 93, 91),
  `Revenue Impact` = c(250, 180, 120, 450, 160)
)

companies_long <- companies %>%
  pivot_longer(cols = c(`Before FE`, `After FE`), 
               names_to = "Stage", 
               values_to = "Accuracy") %>%
  mutate(Stage = factor(Stage, levels = c("Before FE", "After FE")))

p1 <- ggplot(companies_long, aes(x = Company, y = Accuracy, fill = Stage)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("#FF6B6B", "#4ECDC4")) +
  labs(title = "Model Accuracy: Before vs After Feature Engineering",
       y = "Accuracy (%)", x = "") +
  theme(legend.position = "bottom")

p2 <- ggplot(companies, aes(x = Company, y = `Revenue Impact`)) +
  geom_bar(stat = "identity", fill = "#95E77E") +
  geom_text(aes(label = paste0("$", `Revenue Impact`, "M")), 
            vjust = -0.5, fontface = "bold") +
  labs(title = "Annual Revenue Impact",
       y = "Revenue Impact ($M)", x = "")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

---

# Today's Comprehensive Roadmap
```{r roadmap, echo=FALSE}
roadmap <- tibble(
  Section = c("Foundation (1-50)", "Categorical Mastery (51-100)", 
             "Numerical Excellence (101-150)", "Temporal & Text (151-200)",
             "Advanced Methods (201-250)", "Production & Scale (251-300)"),
  Topics = c(
    "Core concepts, encoding basics, missing data",
    "20+ encoding methods, high cardinality solutions",
    "Transformations, scaling, binning, outliers",
    "DateTime engineering, NLP features, embeddings",
    "Deep features, autoencoders, genetic algorithms",
    "Distributed processing, feature stores, monitoring"
  ),
  `Slides` = c("1-50", "51-100", "101-150", "151-200", "201-250", "251-300"),
  `Time` = c("45 min", "45 min", "45 min", "45 min", "45 min", "45 min")
)

kable(roadmap, format = "html", escape = FALSE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = FALSE) %>%
  row_spec(1, bold = TRUE, background = "#FFE5B4")
```

---

# Our Learning Philosophy

## **Theory → Implementation → Scale → Production**

For each technique, we will:

1. **Understand** the mathematical foundation
2. **Implement** in R with real data
3. **Optimize** for performance
4. **Scale** to big data
5. **Deploy** to production
6. **Monitor** in real-world conditions

**No black boxes:** Every method will be transparent and interpretable.

---

# The Feature Engineering Pipeline
```{r pipeline_comprehensive, echo=FALSE, fig.height=6}
library(DiagrammeR)

grViz("
digraph feature_pipeline {
  graph [layout = dot, rankdir = LR, bgcolor = white]
  
  node [shape = rectangle, style = filled, width = 2]
  
  raw [label = 'Raw Data\\n• Structured\\n• Text\\n• Images\\n• Time Series', 
       fillcolor = '#FFE5B4']
  
  clean [label = 'Data Cleaning\\n• Missing values\\n• Outliers\\n• Duplicates\\n• Validation', 
         fillcolor = '#B4E5FF']
  
  create [label = 'Feature Creation\\n• Domain features\\n• Interactions\\n• Aggregations\\n• Embeddings', 
          fillcolor = '#B4FFB4']
  
  transform [label = 'Transformations\\n• Scaling\\n• Encoding\\n• Binning\\n• Smoothing', 
             fillcolor = '#FFB4E5']
  
  select [label = 'Selection\\n• Statistical\\n• Model-based\\n• Regularization\\n• Importance', 
          fillcolor = '#E5B4FF']
  
  reduce [label = 'Reduction\\n• PCA\\n• t-SNE\\n• UMAP\\n• Autoencoders', 
          fillcolor = '#FFDFBA']
  
  validate [label = 'Validation\\n• Cross-validation\\n• Drift detection\\n• A/B testing\\n• Monitoring', 
            fillcolor = '#D4A5A5']
  
  deploy [label = 'Deployment\\n• Feature store\\n• Serving\\n• Versioning\\n• Rollback', 
          fillcolor = '#A8E6CF']
  
  raw -> clean -> create -> transform -> select -> reduce -> validate -> deploy
  
  deploy -> raw [label = 'Feedback Loop', style = dashed, color = gray]
}
")
```

---

# Real Business Context: The StreamFlow Challenge

**You are the Lead ML Engineer at StreamFlow (competing with Netflix)**

**Current Situation:**
- 50M users across 190 countries
- 500K content items (movies, series, documentaries)
- 2TB of interaction data generated daily
- Current recommendation CTR: 2.3% (Netflix: 3.8%)
- Churn rate: 6.5% monthly (Netflix: 3.5%)

**Your Mission:**
- Increase CTR to 4.0% through better features
- Reduce churn to 4.0% using predictive features
- Build real-time feature pipeline for 100M predictions/day
- Estimated impact: $500M annual revenue increase

---

# Dataset Overview: StreamFlow Data
```{r dataset_overview, echo=TRUE}
# Load and examine our comprehensive dataset
glimpse(ecommerce_data)

# Check data quality
data_quality <- ecommerce_data %>%
  summarise(
    n_rows = n(),
    n_customers = n_distinct(customer_id),
    missing_percent = mean(is.na(review_score)) * 100,
    churn_rate = mean(churn) * 100,
    avg_spend = mean(total_spent)
  )

print(data_quality)
```

---

# The Feature Engineering Maturity Model
```{r maturity_model, echo=FALSE}
maturity <- tibble(
  Level = factor(1:5),
  Name = c("Ad-hoc", "Systematic", "Automated", "Optimized", "Adaptive"),
  Characteristics = c(
    "Manual, inconsistent, notebook-based",
    "Documented pipelines, version control",
    "Automated pipelines, testing, CI/CD",
    "Real-time serving, A/B testing, monitoring",
    "Self-optimizing, AutoML, continuous learning"
  ),
  `Typical Metrics` = c(
    "Baseline", "+10-15%", "+20-25%", "+30-35%", "+40-50%"
  ),
  Companies = c(
    "Startups", "Scale-ups", "Enterprises", "Tech Giants", "AI Leaders"
  )
)

kable(maturity, format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(5, bold = TRUE, background = "#90EE90")
```

**Our Goal:** Move you from Level 1 to Level 5 in this module.

---
class: inverse, center, middle

# Part I: Foundations of Feature Engineering
## Understanding the Fundamentals

---

# What Makes a Good Feature?

**The SMART Framework for Features:**

- **S**pecific: Clearly defined and measurable
- **M**eaningful: Correlated with the target variable
- **A**ctionable: Can be influenced by business decisions
- **R**obust: Stable across different data samples
- **T**imely: Available when predictions are needed
```{r good_features, echo=TRUE}
# Example: Creating SMART features
smart_features <- ecommerce_data %>%
  mutate(
    # Specific: Days since last purchase
    days_since_purchase = as.numeric(Sys.Date() - as.Date(timestamp)),
    
    # Meaningful: Purchase frequency
    purchase_frequency = num_purchases / (tenure_days + 1),
    
    # Actionable: Email engagement rate
    email_engagement = email_opens / 20,
    
    # Robust: Normalized spend
    spend_zscore = (total_spent - mean(total_spent)) / sd(total_spent),
    
    # Timely: Real-time features
    hour_of_day = lubridate::hour(timestamp)
  )
```

---

# Feature Types Taxonomy
```{r feature_taxonomy, echo=FALSE, fig.height=6}
grViz("
digraph taxonomy {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle, style = filled]
  
  features [label = 'Features', fillcolor = yellow, fontsize = 16]
  
  numerical [label = 'Numerical', fillcolor = lightblue]
  categorical [label = 'Categorical', fillcolor = lightgreen]
  temporal [label = 'Temporal', fillcolor = lightyellow]
  text [label = 'Text', fillcolor = lightpink]
  interaction [label = 'Interaction', fillcolor = lightgray]
  
  continuous [label = 'Continuous\\n(price, temperature)']
  discrete [label = 'Discrete\\n(count, rating)']
  
  nominal [label = 'Nominal\\n(color, city)']
  ordinal [label = 'Ordinal\\n(size: S/M/L)']
  binary [label = 'Binary\\n(yes/no)']
  
  timestamp [label = 'Timestamp\\n(datetime)']
  duration [label = 'Duration\\n(time span)']
  cyclical [label = 'Cyclical\\n(hour, month)']
  
  features -> numerical
  features -> categorical
  features -> temporal
  features -> text
  features -> interaction
  
  numerical -> continuous
  numerical -> discrete
  
  categorical -> nominal
  categorical -> ordinal
  categorical -> binary
  
  temporal -> timestamp
  temporal -> duration
  temporal -> cyclical
}
")
```

---

# The Mathematics of Feature Engineering

**Feature engineering is fundamentally about finding the optimal transformation function:**

$$f^*: \mathcal{X} \rightarrow \mathcal{Z}$$

Where:
- $\mathcal{X}$ is the raw feature space
- $\mathcal{Z}$ is the transformed feature space
- $f^*$ minimizes the expected prediction error

**Key Transformations:**

1. **Linear:** $z = ax + b$
2. **Polynomial:** $z = \sum_{i=0}^{n} a_i x^i$
3. **Logarithmic:** $z = \log(x + c)$
4. **Interaction:** $z = x_1 \cdot x_2$
5. **Embedding:** $z = W \cdot \text{one\_hot}(x)$

---

# Understanding Feature Distributions
```{r distributions, echo=TRUE, fig.height=4}
# Analyze feature distributions
distribution_analysis <- ecommerce_data %>%
  select(age, income, total_spent, page_views) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "value") %>%
  group_by(feature) %>%
  summarise(
    mean = mean(value, na.rm = TRUE),
    median = median(value, na.rm = TRUE),
    sd = sd(value, na.rm = TRUE),
    skewness = moments::skewness(value, na.rm = TRUE),
    kurtosis = moments::kurtosis(value, na.rm = TRUE)
  )

kable(distribution_analysis, digits = 2, format = "html") %>%
  kable_styling(bootstrap_options = "striped")

# Visualize distributions
ecommerce_data %>%
  select(age, income, total_spent, page_views) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "#4ECDC4") +
  facet_wrap(~name, scales = "free") +
  theme_minimal()
```

---
class: inverse, center, middle

# Part II: Categorical Variable Encoding
## From Categories to Numbers

---

# The Categorical Encoding Challenge

**Problem:** ML algorithms need numbers, not text

**Challenge:** Preserve information while creating numerical representations
```{r cat_challenge_demo, echo=TRUE}
# Our categorical variables
categorical_vars <- ecommerce_data %>%
  select(device, channel, region, product_category) %>%
  head(5)

print(categorical_vars)

# Cardinality analysis
cardinality <- ecommerce_data %>%
  summarise(
    device_unique = n_distinct(device),
    channel_unique = n_distinct(channel),
    region_unique = n_distinct(region),
    product_unique = n_distinct(product_category),
    customer_unique = n_distinct(customer_id)
  )

print(cardinality)
```

---

# One-Hot Encoding: The Foundation
```{r onehot_encoding, echo=TRUE}
# Manual one-hot encoding implementation
one_hot_encode <- function(df, column) {
  # Create dummy variables
  dummies <- model.matrix(~ . - 1, data = df[column])
  
  # Combine with original data
  cbind(df, dummies)
}

# Apply one-hot encoding
encoded_sample <- ecommerce_data %>%
  slice_sample(n = 5) %>%
  select(transaction_id, device) %>%
  one_hot_encode("device")

print(encoded_sample)

# Memory implications
original_size <- object.size(ecommerce_data$device)
encoded_size <- object.size(model.matrix(~ device - 1, data = ecommerce_data))
cat("Memory increase:", round(encoded_size / original_size, 1), "x\n")
```

---

# Label Encoding: For Ordinal Variables
```{r label_encoding, echo=TRUE}
# Create ordinal variable
ecommerce_data <- ecommerce_data %>%
  mutate(
    customer_segment = case_when(
      total_spent < quantile(total_spent, 0.25) ~ "Bronze",
      total_spent < quantile(total_spent, 0.50) ~ "Silver",
      total_spent < quantile(total_spent, 0.75) ~ "Gold",
      TRUE ~ "Platinum"
    ),
    segment_encoded = case_when(
      customer_segment == "Bronze" ~ 1,
      customer_segment == "Silver" ~ 2,
      customer_segment == "Gold" ~ 3,
      customer_segment == "Platinum" ~ 4
    )
  )

# Verify encoding preserves order
encoding_check <- ecommerce_data %>%
  group_by(customer_segment, segment_encoded) %>%
  summarise(
    avg_spent = mean(total_spent),
    n = n()
  ) %>%
  arrange(segment_encoded)

print(encoding_check)
```

---

# Target Encoding: Leveraging the Outcome
```{r target_encoding_demo, echo=TRUE}
# Implement target encoding with k-fold validation to prevent overfitting
target_encode_cv <- function(data, cat_col, target_col, n_folds = 5) {
  set.seed(42)
  folds <- caret::createFolds(data[[target_col]], k = n_folds)
  
  encoded_values <- numeric(nrow(data))
  
  for(fold in folds) {
    # Training data (out of fold)
    train_idx <- setdiff(1:nrow(data), fold)
    
    # Calculate target mean for each category
    encoding_map <- data[train_idx, ] %>%
      group_by({{cat_col}} := .data[[cat_col]]) %>%
      summarise(encoded = mean(.data[[target_col]], na.rm = TRUE))
    
    # Apply to validation fold
    fold_data <- data[fold, ] %>%
      left_join(encoding_map, by = cat_col)
    
    encoded_values[fold] <- fold_data$encoded
  }
  
  return(encoded_values)
}

# Apply target encoding
ecommerce_data$region_target_encoded <- target_encode_cv(
  ecommerce_data, "region", "churn"
)

# Compare encodings
target_comparison <- ecommerce_data %>%
  group_by(region) %>%
  summarise(
    target_encoded = mean(region_target_encoded),
    actual_churn_rate = mean(churn)
  )

print(target_comparison)
```

---

# Frequency Encoding: Simple Yet Effective
```{r frequency_encoding_demo, echo=TRUE}
# Frequency encoding implementation
frequency_encode <- function(data, column) {
  freq_table <- table(data[[column]])
  freq_map <- freq_table / sum(freq_table)
  data[[paste0(column, "_freq")]] <- freq_map[data[[column]]]
  return(data)
}

# Apply frequency encoding
ecommerce_data <- frequency_encode(ecommerce_data, "product_category")

# Analyze frequency encoding
freq_analysis <- ecommerce_data %>%
  group_by(product_category) %>%
  summarise(
    frequency = first(product_category_freq),
    avg_churn = mean(churn),
    avg_spent = mean(total_spent)
  ) %>%
  arrange(desc(frequency))

print(freq_analysis)

# Correlation with target
cor_with_target <- cor(ecommerce_data$product_category_freq, 
                       ecommerce_data$churn)
cat("Correlation with churn:", round(cor_with_target, 3))
```

---

# Binary Encoding: Memory Efficient
```{r binary_encoding_demo, echo=TRUE}
# Binary encoding for high cardinality features
binary_encode <- function(categories, n_bits = NULL) {
  # Convert to integer codes
  cat_codes <- as.integer(factor(categories))
  
  # Determine number of bits needed
  if(is.null(n_bits)) {
    n_bits <- ceiling(log2(length(unique(categories))))
  }
  
  # Create binary matrix
  binary_matrix <- matrix(0, length(categories), n_bits)
  
  for(i in 1:length(categories)) {
    # Convert to binary
    binary_rep <- as.integer(intToBits(cat_codes[i] - 1))[1:n_bits]
    binary_matrix[i, ] <- binary_rep
  }
  
  colnames(binary_matrix) <- paste0("bit_", 1:n_bits)
  return(binary_matrix)
}

# Example with customer IDs (high cardinality)
customer_sample <- sample(ecommerce_data$customer_id, 10)
binary_encoded <- binary_encode(customer_sample)

cat("Unique customers:", length(unique(customer_sample)), "\n")
cat("Binary encoding columns:", ncol(binary_encoded), "\n")
cat("Memory savings:", 
    round((1 - ncol(binary_encoded)/length(unique(customer_sample))) * 100), "%\n")
```

---

# Hash Encoding: For Extreme Cardinality
```{r hash_encoding, echo=TRUE}
# Feature hashing (hashing trick) implementation
hash_encode <- function(categories, n_buckets = 1024) {
  # Simple hash function
  simple_hash <- function(x) {
    sum(utf8ToInt(as.character(x))) %% n_buckets
  }
  
  # Apply hash to each category
  hashed <- sapply(categories, simple_hash)
  
  # Create sparse matrix
  sparse_matrix <- Matrix::sparseMatrix(
    i = 1:length(categories),
    j = hashed + 1,  # R uses 1-based indexing
    x = 1,
    dims = c(length(categories), n_buckets)
  )
  
  return(sparse_matrix)
}

# Apply to customer IDs (simulating millions of unique values)
customer_hashed <- hash_encode(ecommerce_data$customer_id, n_buckets = 256)

cat("Original unique values:", n_distinct(ecommerce_data$customer_id), "\n")
cat("Hashed dimensions:", ncol(customer_hashed), "\n")
cat("Sparsity:", round(1 - Matrix::nnzero(customer_hashed) / 
                       (nrow(customer_hashed) * ncol(customer_hashed)), 3), "\n")
```

---

# Advanced: Embeddings via Neural Networks
```{r embeddings_concept, echo=TRUE}
# Conceptual embedding layer (would use keras/tensorflow in practice)
create_embedding_layer <- function(n_categories, embedding_dim) {
  # Initialize random embedding matrix
  set.seed(42)
  embedding_matrix <- matrix(
    rnorm(n_categories * embedding_dim, 0, 0.1),
    nrow = n_categories,
    ncol = embedding_dim
  )
  
  # Normalize embeddings
  embedding_matrix <- t(apply(embedding_matrix, 1, function(x) x / sqrt(sum(x^2))))
  
  return(embedding_matrix)
}

# Create embeddings for products
products <- unique(ecommerce_data$product_category)
product_embeddings <- create_embedding_layer(
  n_categories = length(products),
  embedding_dim = 8
)

rownames(product_embeddings) <- products
colnames(product_embeddings) <- paste0("embed_", 1:8)

# Calculate similarity between products
similarity_matrix <- product_embeddings %*% t(product_embeddings)
cat("Similarity between Electronics and Home:", 
    round(similarity_matrix["Electronics", "Home"], 3), "\n")
cat("Similarity between Electronics and Sports:", 
    round(similarity_matrix["Electronics", "Sports"], 3), "\n")
```

---

# Encoding Strategy Decision Framework
```{r encoding_strategy, echo=FALSE}
strategy <- tibble(
  Cardinality = c("Low (2-10)", "Medium (10-100)", "High (100-1000)", 
                  "Very High (1000+)", "Text/NLP"),
  `Best Method` = c("One-Hot", "Target/Binary", "Hash/Binary", 
                   "Hash/Embedding", "Word2Vec/BERT"),
  `Memory Usage` = c("O(n×k)", "O(n×log k)", "O(n×h)", "O(n×d)", "O(n×d)"),
  Interpretability = c("High", "Medium", "Low", "Very Low", "Very Low"),
  `Prevents Overfitting` = c("Yes", "No*", "Yes", "Yes", "Yes")
)

kable(strategy, format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  footnote(general = "* Target encoding requires cross-validation to prevent overfitting")
```

---
class: inverse, center, middle

# Part III: Missing Data Engineering
## Converting Absence into Signal

---

# Understanding Missing Data Mechanisms
```{r missing_mechanisms, echo=TRUE}
# Create different types of missingness
set.seed(42)
missing_demo <- ecommerce_data %>%
  slice_sample(n = 1000) %>%
  mutate(
    # MCAR: Missing Completely At Random (10% random missing)
    age_mcar = ifelse(runif(n()) < 0.1, NA, age),
    
    # MAR: Missing At Random (mobile users less likely to provide income)
    income_mar = ifelse(device == "mobile" & runif(n()) < 0.3, NA, income),
    
    # MNAR: Missing Not At Random (high spenders hide their income)
    income_mnar = ifelse(income > quantile(income, 0.8) & 
                         runif(n()) < 0.5, NA, income)
  )

# Analyze missing patterns
missing_summary <- missing_demo %>%
  summarise(
    MCAR_missing = sum(is.na(age_mcar)) / n() * 100,
    MAR_missing = sum(is.na(income_mar)) / n() * 100,
    MNAR_missing = sum(is.na(income_mnar)) / n() * 100
  )

print(missing_summary)
```

---

# Visualizing Missing Data Patterns
```{r missing_viz_detailed, echo=TRUE, fig.height=5}
library(naniar)

# Create comprehensive missing data visualization
missing_demo %>%
  select(age_mcar, income_mar, income_mnar, total_spent, device) %>%
  gg_miss_upset(nsets = 5) +
  labs(title = "Missing Data Pattern Combinations")

# Correlation of missingness
missing_correlations <- missing_demo %>%
  mutate(
    age_miss = is.na(age_mcar),
    income_mar_miss = is.na(income_mar),
    income_mnar_miss = is.na(income_mnar)
  ) %>%
  select(ends_with("_miss"), total_spent, churn) %>%
  cor(use = "complete.obs")

corrplot::corrplot(missing_correlations, method = "number", type = "upper")
```

---

# Simple Imputation Methods
```{r simple_imputation, echo=TRUE}
# Compare simple imputation strategies
imputation_comparison <- missing_demo %>%
  mutate(
    # Mean imputation
    income_mean = ifelse(is.na(income_mar), 
                        mean(income_mar, na.rm = TRUE), 
                        income_mar),
    
    # Median imputation (robust to outliers)
    income_median = ifelse(is.na(income_mar), 
                          median(income_mar, na.rm = TRUE), 
                          income_mar),
    
    # Mode imputation for categorical
    device_mode = ifelse(is.na(device), 
                        names(sort(table(device), decreasing = TRUE))[1], 
                        device),
    
    # Forward fill (for time series)
    income_ffill = zoo::na.locf(income_mar, na.rm = FALSE),
    
    # Backward fill
    income_bfill = zoo::na.locf(income_mar, fromLast = TRUE, na.rm = FALSE),
    
    # Linear interpolation
    income_linear = zoo::na.approx(income_mar, na.rm = FALSE)
  )

# Compare distributions
imputation_stats <- imputation_comparison %>%
  summarise(
    original_mean = mean(income_mar, na.rm = TRUE),
    mean_imp_std = sd(income_mean, na.rm = TRUE),
    median_imp_std = sd(income_median, na.rm = TRUE),
    linear_imp_std = sd(income_linear, na.rm = TRUE)
  )

print(imputation_stats)
```

---

# Advanced: MICE Imputation
```{r mice_imputation, echo=TRUE, cache=TRUE}
# Multiple Imputation by Chained Equations
library(mice)

# Prepare data for MICE
mice_data <- missing_demo %>%
  select(age_mcar, income_mar, total_spent, num_purchases, device, churn)

# Run MICE imputation
mice_imputed <- mice(
  mice_data,
  m = 5,  # Number of imputation datasets
  method = c("pmm", "pmm", "pmm", "pmm", "polyreg", "logreg"),
  printFlag = FALSE,
  seed = 42
)

# Extract one complete dataset
complete_data <- complete(mice_imputed, 1)

# Compare before and after
comparison <- data.frame(
  Variable = names(mice_data),
  Missing_Before = colSums(is.na(mice_data)),
  Missing_After = colSums(is.na(complete_data))
)

print(comparison)
```

---

# Missingness as a Feature
```{r missingness_features, echo=TRUE}
# Create missingness indicator features
missingness_features <- missing_demo %>%
  mutate(
    # Binary indicators
    age_was_missing = is.na(age_mcar),
    income_was_missing = is.na(income_mar),
    
    # Count of missing values per row
    n_missing = rowSums(is.na(select(., age_mcar, income_mar, income_mnar))),
    
    # Proportion missing
    prop_missing = n_missing / 3,
    
    # Interaction: high value customer with missing income
    high_value_missing_income = (total_spent > median(total_spent)) & 
                                is.na(income_mar),
    
    # Pattern indicators
    missing_pattern = case_when(
      !is.na(age_mcar) & !is.na(income_mar) ~ "complete",
      is.na(age_mcar) & !is.na(income_mar) ~ "age_only",
      !is.na(age_mcar) & is.na(income_mar) ~ "income_only",
      TRUE ~ "both_missing"
    )
  )

# Test predictive power of missingness
miss_correlation <- cor(
  missingness_features$income_was_missing,
  missingness_features$churn
)

cat("Correlation between income missingness and churn:", 
    round(miss_correlation, 3), "\n")
```

---

# Domain-Specific Imputation
```{r domain_imputation, echo=TRUE}
# Industry-specific imputation strategies
domain_imputation <- ecommerce_data %>%
  mutate(
    # E-commerce: Use average order value for missing total spent
    total_spent_imp = ifelse(
      is.na(total_spent),
      avg_order_value * num_purchases,
      total_spent
    ),
    
    # Seasonal adjustment for missing values
    month = lubridate::month(timestamp),
    seasonal_factor = case_when(
      month %in% c(11, 12) ~ 1.3,  # Holiday season
      month %in% c(6, 7, 8) ~ 0.9,  # Summer slowdown
      TRUE ~ 1.0
    ),
    
    # Group-based imputation
    region_avg_spend = ave(total_spent, region, FUN = function(x) mean(x, na.rm = TRUE)),
    total_spent_group = ifelse(is.na(total_spent), region_avg_spend, total_spent),
    
    # Time-decay weighted imputation for time series
    days_ago = as.numeric(Sys.Date() - as.Date(timestamp)),
    weight = exp(-days_ago / 30),  # 30-day half-life
    weighted_imputation = weighted.mean(total_spent, weight, na.rm = TRUE)
  )

# Compare imputation methods
imputation_quality <- domain_imputation %>%
  filter(!is.na(total_spent)) %>%
  summarise(
    simple_mean_error = mean(abs(total_spent - mean(total_spent))),
    group_based_error = mean(abs(total_spent - region_avg_spend)),
    domain_based_error = mean(abs(total_spent - avg_order_value * num_purchases))
  )

print(imputation_quality)
```

---
class: inverse, center, middle

# Part IV: Numerical Transformations
## Optimizing Continuous Features

---

# Why Transform Numerical Features?
```{r why_transform_detailed, echo=FALSE, fig.height=5}
# Create subplots showing different transformation needs
par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))

# 1. Skewed distribution
x_skewed <- rlnorm(1000, 0, 1)
hist(x_skewed, breaks = 50, main = "Right-Skewed", 
     col = "#FF6B6B", xlab = "Original")

# 2. After log transform
hist(log(x_skewed), breaks = 50, main = "Log-Transformed", 
     col = "#4ECDC4", xlab = "Log(x)")

# 3. Different scales
plot(rnorm(100, 1000, 200), rnorm(100, 0.01, 0.002), 
     main = "Scale Mismatch", xlab = "Feature 1 (thousands)", 
     ylab = "Feature 2 (decimals)", col = "#FFE66D")

# 4. Outliers
x_outliers <- c(rnorm(95, 100, 10), 300, 350, 400, 10, 5)
boxplot(x_outliers, main = "With Outliers", 
        col = "#FF6B6B", ylab = "Value")

# 5. After winsorization
x_wins <- pmin(pmax(x_outliers, quantile(x_outliers, 0.05)), 
              quantile(x_outliers, 0.95))
boxplot(x_wins, main = "Winsorized", 
        col = "#4ECDC4", ylab = "Value")

# 6. Non-linear relationship
x <- 1:100
y <- 50 + 2*x - 0.02*x^2 + rnorm(100, 0, 5)
plot(x, y, main = "Non-linear Pattern", 
     xlab = "X", ylab = "Y", col = "#A8E6CF")
lines(lowess(x, y), col = "red", lwd = 2)
```

---

# Scaling and Normalization Methods
```{r scaling_methods_comprehensive, echo=TRUE}
# Implement various scaling methods
scaling_methods <- function(x) {
  list(
    original = x,
    standardized = (x - mean(x)) / sd(x),  # Z-score
    min_max = (x - min(x)) / (max(x) - min(x)),  # [0, 1]
    max_abs = x / max(abs(x)),  # [-1, 1]
    robust = (x - median(x)) / mad(x),  # Robust to outliers
    unit_norm = x / sqrt(sum(x^2)),  # Unit vector
    decimal = x / 10^(floor(log10(max(abs(x)))))  # Decimal scaling
  )
}

# Apply to income feature
income_scaled <- scaling_methods(ecommerce_data$income[1:100])

# Compare properties
scaling_comparison <- data.frame(
  Method = names(income_scaled),
  Mean = sapply(income_scaled, mean),
  SD = sapply(income_scaled, sd),
  Min = sapply(income_scaled, min),
  Max = sapply(income_scaled, max)
)

kable(scaling_comparison, digits = 3, format = "html") %>%
  kable_styling(bootstrap_options = "striped")
```

---

# Power Transformations
```{r power_transformations, echo=TRUE, fig.height=4}
# Box-Cox transformation to find optimal power
library(forecast)

# Original skewed data
original <- ecommerce_data$total_spent[1:1000]

# Find optimal lambda
lambda_optimal <- BoxCox.lambda(original)
cat("Optimal Box-Cox lambda:", round(lambda_optimal, 3), "\n")

# Apply various power transformations
transformations <- tibble(
  original = original,
  log = log(original),
  sqrt = sqrt(original),
  cbrt = original^(1/3),
  boxcox = BoxCox(original, lambda_optimal),
  yeo_johnson = car::yjPower(original, 0)  # Handles zeros
)

# Compare skewness
skewness_comp <- sapply(transformations, moments::skewness)
print(round(skewness_comp, 2))

# Visualize
transformations %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "#4ECDC4") +
  facet_wrap(~name, scales = "free") +
  theme_minimal()
```

---

# Binning Strategies
```{r binning_strategies, echo=TRUE}
# Implement different binning strategies
binning_demo <- ecommerce_data %>%
  mutate(
    # Equal width binning
    age_equal_width = cut(age, breaks = 5),
    
    # Equal frequency (quantile) binning
    age_quantile = cut(age, 
                      breaks = quantile(age, probs = 0:5/5),
                      include.lowest = TRUE),
    
    # Custom business-logic bins
    age_custom = cut(age,
                    breaks = c(-Inf, 18, 25, 35, 50, 65, Inf),
                    labels = c("<18", "18-25", "26-35", "36-50", "51-65", "65+")),
    
    # K-means based binning
    age_kmeans = kmeans(matrix(age), centers = 5)$cluster,
    
    # Decision tree based binning (supervised)
    age_tree = as.numeric(cut2(age, g = 5))  # Using Hmisc::cut2
  )

# Compare binning results
binning_comparison <- binning_demo %>%
  group_by(age_custom) %>%
  summarise(
    n = n(),
    avg_churn = mean(churn),
    avg_spent = mean(total_spent)
  )

print(binning_comparison)
```

---

# Outlier Detection and Treatment
```{r outlier_treatment, echo=TRUE}
# Multiple outlier detection methods
detect_outliers <- function(x, method = "iqr") {
  if(method == "iqr") {
    Q1 <- quantile(x, 0.25, na.rm = TRUE)
    Q3 <- quantile(x, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    lower <- Q1 - 1.5 * IQR
    upper <- Q3 + 1.5 * IQR
    return(x < lower | x > upper)
  } else if(method == "zscore") {
    z <- abs((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
    return(z > 3)
  } else if(method == "mad") {
    mad_score <- abs((x - median(x, na.rm = TRUE)) / mad(x, na.rm = TRUE))
    return(mad_score > 3)
  } else if(method == "isolation_forest") {
    # Simplified version - would use isotree package in practice
    return(x > quantile(x, 0.99, na.rm = TRUE))
  }
}

# Apply different methods
outlier_comparison <- ecommerce_data %>%
  mutate(
    outlier_iqr = detect_outliers(total_spent, "iqr"),
    outlier_zscore = detect_outliers(total_spent, "zscore"),
    outlier_mad = detect_outliers(total_spent, "mad"),
    outlier_iso = detect_outliers(total_spent, "isolation_forest")
  ) %>%
  summarise(
    IQR_outliers = sum(outlier_iqr),
    Zscore_outliers = sum(outlier_zscore),
    MAD_outliers = sum(outlier_mad),
    Isolation_outliers = sum(outlier_iso)
  )

print(outlier_comparison)
```

---

# Smoothing Techniques
```{r smoothing_techniques, echo=TRUE, fig.height=4}
# Apply various smoothing techniques
smooth_data <- time_series_data %>%
  mutate(
    # Moving averages
    ma_7 = zoo::rollmean(sales, k = 7, fill = NA),
    ma_30 = zoo::rollmean(sales, k = 30, fill = NA),
    
    # Exponential smoothing
    exp_smooth = stats::filter(sales, filter = 0.2, method = "recursive"),
    
    # LOESS smoothing
    loess_smooth = predict(loess(sales ~ as.numeric(date), 
                                 data = time_series_data, span = 0.1)),
    
    # Savitzky-Golay filter (polynomial smoothing)
    # sg_smooth = signal::sgolayfilt(sales, p = 3, n = 7)
  )

# Visualize smoothing effects
smooth_data %>%
  slice(100:200) %>%  # Zoom in for clarity
  select(date, sales, ma_7, ma_30, loess_smooth) %>%
  pivot_longer(-date) %>%
  ggplot(aes(x = date, y = value, color = name)) +
  geom_line(alpha = 0.7) +
  scale_color_manual(values = c("black", "#FF6B6B", "#4ECDC4", "#FFE66D")) +
  labs(title = "Smoothing Techniques Comparison") +
  theme_minimal()
```

---
class: inverse, center, middle

# Part V: DateTime Feature Engineering
## Extracting Temporal Patterns

---

# Comprehensive DateTime Features
```{r datetime_comprehensive_final_final, echo=TRUE}
# Step 1: Feature Extraction (The extraction part is now fully robust)
datetime_features <- ecommerce_data %>%
  mutate(
    # Basic components
    year = lubridate::year(timestamp),
    month = lubridate::month(timestamp),
    day = lubridate::day(timestamp),
    hour = lubridate::hour(timestamp),
    minute = lubridate::minute(timestamp),
    dayofweek = lubridate::wday(timestamp),
    dayofyear = lubridate::yday(timestamp),
    weekofyear = lubridate::isoweek(timestamp),
    quarter = lubridate::quarter(timestamp),
    
    # Derived features
    is_weekend = dayofweek %in% c(1, 7),
    is_monthend = lubridate::day(timestamp) == lubridate::day(lubridate::ceiling_date(timestamp, "month") - 1),
    is_monthstart = lubridate::day(timestamp) == 1,
    is_quarter_end = month %in% c(3, 6, 9, 12) & is_monthend,
    
    # Business-specific
    is_business_hour = hour >= 9 & hour <= 17,
    is_lunch_hour = hour %in% c(12, 13),
    is_black_friday = month == 11 & dayofweek == 6 & day >= 23 & day <= 29,
    
    # Cyclical encoding
    hour_sin = sin(2 * pi * hour / 24),
    hour_cos = cos(2 * pi * hour / 24),
    month_sin = sin(2 * pi * month / 12),
    month_cos = cos(2 * pi * month / 12),
    dayofweek_sin = sin(2 * pi * dayofweek / 7),
    dayofweek_cos = cos(2 * pi * dayofweek / 7)
  )

# Step 2: Show Feature Importance (Now fully robust and idiomatic)

# 2a. Select columns and save to temporary variable
correlation_data <- datetime_features %>%
  select(starts_with("is_"), ends_with("_sin"), ends_with("_cos"), churn)

# 2b. Calculate correlation matrix and extract the 'churn' column using pull()
datetime_importance <- correlation_data %>%
  cor(use = "complete.obs") %>%
  # Use the select/pull approach for final feature extraction from the matrix
  # Note: Since the output of cor() is a matrix, we use base R subsetting
  # here but simplify the pipe structure and revert to the magrittr pipe (%) 
  # for compatibility.
  '['(, "churn") %>%
  sort(decreasing = TRUE)

# OPTIONAL ALTERNATIVE (If you insist on the native pipe |>)
# The structure below forces a vector extraction which is natively supported:
# datetime_importance <- correlation_data |>
#   cor(use = "complete.obs")
# datetime_importance <- datetime_importance[, "churn"] |>
#   sort(decreasing = TRUE)


print(round(datetime_importance[1:10], 3))
```

---

# Time-Based Aggregations
```{r time_aggregations_corrected, echo=TRUE}
# Create time-based rolling features
time_aggregations <- ecommerce_data %>%
  arrange(customer_id, timestamp) %>%
  group_by(customer_id) %>%
  mutate(
    # Cumulative features
    cumulative_spend = cumsum(total_spent),
    cumulative_purchases = cumsum(num_purchases),
    
    # Rolling window statistics (last 30 days)
    # Note: For time-based features, the 'width' parameter in rollapplyr 
    # should be defined by time units (e.g., 30 days), not row count. 
    # This simplified version assumes daily rows for demonstration.
    spend_last_30d = zoo::rollapplyr(total_spent, 
                                     width = 30, 
                                     FUN = sum, 
                                     partial = TRUE, 
                                     fill = NA),
    
    # Time since last event (in days)
    # Best Practice: Explicitly calculate the time length in days
    days_since_last_purchase = as.numeric(timestamp - lag(timestamp, 1), units = "days"),
    
    # Purchase velocity
    purchase_velocity = num_purchases / (tenure_days + 1),
    
    # Spending trends
    spend_acceleration = (total_spent - lag(total_spent, 1)) / 
                         (days_since_last_purchase + 1),
    
    # Seasonality indicators
    # FIX: Explicitly call lubridate::month() to avoid namespace collision
    holiday_purchase_ratio = sum(lubridate::month(timestamp) %in% c(11, 12)) / n()
  ) %>%
  ungroup()

# Summary of temporal features (The final step is already robust)
temporal_summary <- time_aggregations %>%
  summarise(
    avg_velocity = mean(purchase_velocity, na.rm = TRUE),
    avg_days_between = mean(days_since_last_purchase, na.rm = TRUE),
    holiday_impact = cor(holiday_purchase_ratio, total_spent, use = "complete.obs")
  )

print(temporal_summary)
```

---
class: center, middle

# End of First Batch (Slides 1-50)

## Coming Next: Slides 51-100
### Advanced Categorical Methods & Text Feature Engineering

**Next Topics:**
- Word embeddings and BERT features
- Entity extraction from text
- Advanced interaction features
- Genetic algorithm feature selection
- AutoML feature generation

---