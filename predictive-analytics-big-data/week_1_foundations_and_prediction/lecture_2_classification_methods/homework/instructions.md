# Homework 2: Production-Ready Customer Lifetime Value Prediction

**Course:** MSc in Data Science and Business Analytics  
**Lecture:** 2 - Advanced Regression & Regularization  
**Due Date:** 14 days from release  
**Checkpoint:** Day 7 (EDA and Feature Engineering sections)  
**Weight:** 15% of course grade

## Overview

This homework requires building a production-ready regularized regression model predicting 12-month customer lifetime value (CLV) for an e-commerce platform. You will perform complete end-to-end workflow from exploratory analysis through deployment planning, demonstrating mastery of regularization techniques in realistic business context.

High-quality submissions suitable for inclusion in professional portfolios will showcase technical competency, business reasoning, and communication effectiveness to potential employers.

## Learning Objectives

By completing this homework, you will demonstrate ability to:

1. Conduct comprehensive exploratory data analysis identifying data quality issues, feature relationships, and patterns requiring attention

2. Engineer domain-informed features transforming raw data into predictive signals using business logic and statistical principles

3. Implement and compare Ridge, LASSO, and Elastic Net regression with appropriate cross-validation and hyperparameter tuning

4. Evaluate models comprehensively using multiple validation strategies, performance metrics, and business-relevant criteria

5. Design production deployment plans addressing monitoring, maintenance, and stakeholder communication

6. Communicate technical results effectively to both technical and business audiences

## Business Context

You are a senior data scientist at **GlobalShop**, a mid-size e-commerce platform with 2 million active customers. The marketing team currently segments customers using simple rules (purchase recency, frequency, monetary value) and applies uniform retention strategies within segments.

The CMO wants predictive customer lifetime value model enabling personalized retention investments. High-CLV customers warrant expensive retention efforts (personal account managers, exclusive discounts, priority support). Low-CLV customers receive only automated email campaigns. Accurate predictions enable optimal resource allocation maximizing customer lifetime value portfolio-wide.

Your mandate: Build production-ready CLV prediction model achieving at least 20% improvement in prediction accuracy over current RFM-based approach while remaining interpretable enough for marketing teams to understand and trust. The model must handle monthly retraining as customer behavior evolves and feature distributions shift.

## Dataset Description

**File:** `customer_data.csv`

**Observations:** 100,000 customers randomly sampled from GlobalShop's customer database

**Time Period:** Features calculated from 24 months of historical data (months -24 to 0), outcome measured in following 12 months (months 1 to 12)

**Target Variable:** `clv_12m` - Total revenue generated by customer in 12 months following feature calculation (continuous, right-skewed, range $0-$50,000, median $320)

**Feature Categories:**

- **Demographics (15 features):** Age, gender, location (country, region, city tier), language, device preference, customer since date

- **Transaction History (40 features):** Purchase counts (total, by category, by channel), revenue (total, average order value, by time period), refund rates, cart abandonment rates, time between orders

- **Browsing Behavior (30 features):** Session counts, page views, time on site, product views by category, search queries, wishlist additions, price comparison behavior

- **Marketing Interactions (25 features):** Email engagement (opens, clicks, conversions), SMS responses, push notification settings, promotional response rates, channel preferences

- **Customer Service (15 features):** Support tickets (count, types, resolution time), satisfaction scores, complaint indicators, refund requests

- **Product Preferences (25 features):** Category affinities, brand preferences, price sensitivity indicators, purchase diversity, seasonal shopping patterns

**Data Quality Issues (intentionally included for realistic practice):**

- Missing values: 5-15% missing in some features (not missing completely at random)
- Outliers: Extreme values in transaction amounts and browsing behavior
- Skewness: Heavy right skew in monetary features requiring transformation
- Multicollinearity: High correlation among features within categories
- Temporal patterns: Seasonality in purchasing behavior requiring feature engineering

**Supporting Files:**

- `feature_dictionary.csv`: Detailed descriptions of all 150 features including data types, units, and business definitions
- `rfm_baseline.csv`: Predictions from current RFM-based segmentation approach for performance comparison
- `data_quality_report.txt`: Known data collection issues documented by data engineering team

## Required Deliverables

### 1. Exploratory Data Analysis Report (15 points)

Comprehensive EDA document (5-10 pages) analyzing dataset characteristics and informing modeling decisions. Required sections:

**Data Overview:** Summary statistics for all features (mean, median, standard deviation, range, missing percentages), outcome variable distribution analysis, sample size adequacy assessment for intended modeling approach.

**Univariate Analysis:** Distribution visualizations for key features (histograms, box plots, density plots), identification of skewness requiring transformation, outlier detection with business context (are extreme values errors or legitimate customers?), missing value patterns by feature.

**Bivariate Analysis:** Scatter plots and correlation analysis between features and outcome, identification of strong predictor candidates, detection of non-linear relationships suggesting transformations, correlation heatmaps within feature categories showing multicollinearity.

**Segment Analysis:** CLV distribution across customer segments (geography, acquisition channel, customer tenure, purchase categories), identification of heterogeneous segments requiring special treatment, visualization of segment-specific patterns.

**Data Quality Assessment:** Documentation of missing value mechanisms (MCAR, MAR, MNAR), outlier investigation determining whether to retain, cap, or remove extreme values, assessment of feature measurement reliability, temporal stability of features across time periods.

**Modeling Implications:** Summary of EDA findings informing feature engineering decisions, transformations needed for modeling, potential interactions worth exploring, expected challenges in modeling (class imbalance, multicollinearity, etc.).

---

### 2. Feature Engineering Pipeline (20 points)

Documented R script implementing complete feature engineering workflow with clear rationale for each transformation. Required components:

**Missing Value Handling:** Explicit strategy for each feature with missing data. Justify imputation method (mean, median, mode, model-based, indicator variable) based on missing mechanism and feature characteristics. Document percentage of missing values and imputation impact on distributions.

**Derived Features:** Create business-logic-driven features including RFM (recency, frequency, monetary) metrics at multiple time windows (30-day, 90-day, 12-month), engagement scores combining multiple behavioral signals, trend features capturing growth or decline in customer activity, ratio features (e.g., refund rate, email click-through rate), categorical interactions (e.g., category preference × geographic region).

**Transformations:** Apply appropriate transformations to skewed variables (log, square root, Box-Cox), create polynomial features for suspected non-linear relationships, standardize continuous features enabling fair regularization penalties, encode categorical variables (one-hot encoding, target encoding, or embedding).

**Feature Selection Pre-processing:** Remove features with >50% missing values unless business justification for retention, eliminate features with near-zero variance providing minimal information, address multicollinearity by removing redundant features or combining via PCA when appropriate.

**Validation Split:** Create time-based splits reflecting production scenario where models predict future from past. Training set: 60% earliest customers, validation set: 20% middle customers, test set: 20% most recent customers. Ensure no temporal leakage where future information influences feature calculation.

**Pipeline Documentation:** Each transformation must include inline comments explaining business rationale and statistical justification. Pipeline should be reproducible and maintainable by other team members.

---

### 3. Model Development and Comparison (25 points)

Systematic comparison of regularization approaches with rigorous validation. Required analysis:

**Baseline Model:** Fit OLS regression as performance floor. Document overfitting severity and identify need for regularization. Compare to RFM-based baseline provided in `rfm_baseline.csv`. Calculate baseline performance metrics (RMSE, MAE, R²) on validation set.

**Ridge Regression:** Implement Ridge using cv.glmnet() with alpha=0. Tune lambda using 10-fold cross-validation on training set. Extract optimal lambda via both lambda.min and lambda.1se, justify final selection. Visualize regularization path showing coefficient shrinkage. Evaluate performance on validation set. Analyze coefficient estimates identifying important features.

**LASSO Regression:** Implement LASSO using cv.glmnet() with alpha=1. Use identical cross-validation structure as Ridge for fair comparison. Document selected features at optimal lambda. Compare LASSO feature selection to domain expert expectations. Evaluate validation performance. Assess whether feature reduction improves interpretability sufficiently to justify any performance sacrifice.

**Elastic Net Regression:** Implement Elastic Net testing alpha values {0.1, 0.3, 0.5, 0.7, 0.9} combined with lambda tuning. Use nested cross-validation or validation set to select optimal alpha-lambda combination. Document final model selection and rationale. Evaluate whether Elastic Net provides best of Ridge (handles correlation) and LASSO (feature selection).

**Hyperparameter Tuning:** Document complete tuning process including search spaces explored, selection criteria applied, and sensitivity analysis showing performance across hyperparameter values. Verify no overfitting to validation set through final test set evaluation.

**Model Comparison Framework:** Create comprehensive comparison table including predictive metrics (RMSE, MAE, R² on validation and test sets), complexity metrics (number of features, effective degrees of freedom), training efficiency (time to fit, cross-validation duration), interpretability (coefficient stability, feature count), business metrics (error cost weighted by CLV magnitude, segment-specific accuracy).

---

### 4. Model Evaluation and Diagnostics (20 points)

Comprehensive evaluation of selected model using multiple validation approaches:

**Test Set Performance:** Final model evaluation on held-out test set never used during development. Calculate all performance metrics matching validation set calculations. Assess whether test performance matches validation performance (if substantial gap exists, investigate overfitting to validation set).

**Residual Analysis:** Generate residual plots detecting heteroscedasticity (variance increasing with predicted value), non-linearity (patterns in residuals vs. fitted values), outliers with high influence (Cook's distance, leverage plots). Interpret diagnostic plots and document any patterns suggesting model deficiencies.

**Prediction Intervals:** Calculate prediction intervals quantifying uncertainty around point predictions. Use cross-validation residuals or bootstrap methods to generate interval estimates. Assess calibration by comparing nominal coverage rates (e.g., 90% intervals) to empirical coverage in test set.

**Segment-Specific Evaluation:** Assess model performance across customer segments (high-value vs. low-value, new customers vs. established, geographic regions, product categories). Identify segments where model performs poorly, suggesting need for segment-specific models or additional features.

**Feature Importance Analysis:** For selected model, rank features by coefficient magnitude (after standardization) or permutation importance. Validate that important features align with business intuition about CLV drivers. Visualize top 20 features with confidence intervals around coefficient estimates.

**Error Analysis:** Examine customers with largest prediction errors. Investigate common characteristics of underpredicted customers (model underestimates CLV) and overpredicted customers (model overestimates CLV). Determine whether error patterns suggest systematic model deficiencies or reflect genuine uncertainty in customer behavior.

**Comparison to Baseline:** Calculate improvement over RFM baseline in percentage terms. Assess whether 20% improvement target achieved. Conduct statistical tests (paired t-test or Wilcoxon signed-rank test) determining if performance improvement is statistically significant.

---

### 5. Deployment Plan (15 points)

Production deployment documentation suitable for engineering and operations teams (3-5 pages):

**Feature Pipeline Architecture:** Diagram showing data flow from source systems through feature engineering to model scoring. Document feature calculation frequency (real-time, daily batch, monthly aggregation). Specify data dependencies and failure modes (what happens when upstream data unavailable?).

**Monitoring Dashboard:** Design monitoring system tracking model health through prediction accuracy trends (RMSE, MAE calculated on recent predictions vs. actual outcomes), feature drift detection (distribution shifts in key features), data quality metrics (missing rates, outlier frequencies), prediction distribution monitoring (alerts if predictions shift dramatically), business impact metrics (incremental revenue from model-driven decisions).

**Alert Thresholds:** Define yellow alerts (investigate within 24 hours) and red alerts (immediate action required). Example thresholds: Yellow if RMSE increases >10% from baseline, red if >20%. Yellow if any feature shows >20% distribution shift, red if >30%. Yellow if missing rate increases >5 percentage points, red if >10 points.

**Retraining Strategy:** Specify scheduled retraining (monthly with 24-month rolling training window) and triggered retraining (when drift thresholds exceeded or performance degrades beyond acceptable levels). Document retraining process including validation protocol, A/B testing period, rollback procedures. Estimate retraining costs (data scientist time, computational resources, validation effort).

**Model Versioning:** Describe version control system maintaining model artifacts (coefficients, hyperparameters, feature definitions, training data metadata). Enable rollback to previous model version if new model underperforms. Track model performance by version over time.

**Stakeholder Communication:** Design reporting for different audiences. Marketing team receives weekly dashboards showing model performance and customer segment insights. Engineering team receives daily monitoring metrics and alert notifications. Executive leadership receives quarterly reports on business impact (revenue lift, retention improvement, ROI on modeling investment).

**Risk Mitigation:** Identify deployment risks (model bugs producing nonsensical predictions, feature pipeline failures creating missing data, concept drift degrading accuracy) and mitigation strategies (sanity checks rejecting impossible predictions, fallback to rule-based system during outages, automated drift detection triggering retraining).

**Rollback Procedures:** Define conditions triggering model rollback (test set performance degrades >15% from validation, production monitoring shows sustained alert conditions, business stakeholders report systematic errors). Document rollback process and responsible parties.

---

### 6. Executive Summary (5 points)

One-page business-focused memo for C-level executives (CMO, CFO, CTO) in Word or PDF format:

**Problem Statement:** Concisely describe business need for improved CLV prediction and current approach limitations (2-3 sentences).

**Solution Approach:** Explain regularized regression modeling in non-technical terms emphasizing business benefits (enables personalized retention investment, improves resource allocation, reduces wasted marketing spend) (3-4 sentences).

**Model Performance:** Present results using business metrics. "New model predicts customer value with 28% greater accuracy than current approach, enabling $2.4M additional revenue through optimized retention spending." Avoid statistical jargon (RMSE, R²) in favor of business impact (revenue, cost savings, efficiency gains) (2-3 sentences).

**Implementation Plan:** Timeline for deployment (2 weeks testing, 4 weeks phased rollout), resource requirements (1 data scientist for monitoring, engineering support for pipeline integration), success metrics (prediction accuracy, business outcomes) (3-4 sentences).

**Risk Assessment:** Acknowledge model limitations (predictions uncertain for new customers, performance degrades without retraining, requires ongoing monitoring) and mitigation strategies (confidence intervals flag uncertain predictions, monthly retraining prevents staleness, automated alerts detect issues) (2-3 sentences).

**Recommendation:** Clear go/no-go recommendation with justification. If recommending deployment, specify phased approach (pilot on subset before full rollout). If recommending additional work, specify what gaps need addressing (2 sentences).

---

## Evaluation Rubric

### Technical Analysis Quality (40 points)

- **Exploratory Data Analysis (15 points):** Thorough univariate/bivariate/multivariate analysis, appropriate visualizations, insightful interpretation, documented implications for modeling

- **Feature Engineering (20 points):** Well-justified transformations, domain-informed derived features, appropriate handling of missing values and outliers, documented reproducible pipeline

- **Model Development (25 points):** Proper implementation of Ridge/LASSO/Elastic Net, rigorous cross-validation, systematic hyperparameter tuning, fair model comparison

- **Model Evaluation (20 points):** Comprehensive test set evaluation, thorough diagnostics, segment-specific analysis, feature importance assessment, error analysis

- **Deployment Plan (15 points):** Realistic monitoring strategy, clear alert thresholds, thoughtful retraining protocol, appropriate risk mitigation

**Scoring:** Exceptional (36-40) demonstrates mastery with sophisticated techniques correctly applied. Proficient (32-35) demonstrates competence with all required analyses completed correctly. Developing (28-31) demonstrates partial competence with some gaps or errors. Unsatisfactory (<28) demonstrates insufficient understanding.

### Business Reasoning (30 points)

- **Feature Engineering Rationale (10 points):** Features motivated by business logic not just statistical exploration, transformations justified by domain knowledge, derived features capture meaningful business concepts

- **Model Selection Justification (10 points):** Model choice balances statistical performance with business constraints (interpretability, maintainability, stakeholder acceptance), trade-offs explicitly analyzed and justified

- **Deployment Feasibility (10 points):** Deployment plan reflects realistic operational constraints, monitoring strategy addresses genuine risk factors, stakeholder communication appropriate for audiences

**Scoring:** Exceptional (27-30) demonstrates sophisticated business thinking with deep integration of technical and business considerations. Proficient (24-26) demonstrates solid business reasoning connecting technical choices to business needs. Developing (21-23) demonstrates basic business awareness with limited integration. Unsatisfactory (<21) treats problem as purely technical exercise ignoring business context.

### Documentation Quality (20 points)

- **Code Quality (8 points):** Well-organized R scripts with meaningful variable names, appropriate comments explaining logic, modular structure enabling maintenance and extension

- **Narrative Clarity (8 points):** Clear written explanations in R Markdown, logical flow from problem through solution, appropriate technical detail for audience, grammar and spelling professional

- **Visualization Quality (4 points):** Effective plots with clear labels and titles, appropriate plot types for data characteristics, visualizations enhance understanding rather than merely decorating

**Scoring:** Exceptional (18-20) demonstrates professional-grade documentation suitable for stakeholder presentation. Proficient (16-17) demonstrates clear documentation enabling understanding and reproduction. Developing (14-15) demonstrates adequate documentation with some clarity or organization issues. Unsatisfactory (<14) demonstrates poor documentation impeding understanding.

### Presentation Quality (10 points)

- **Overall Organization (5 points):** Logical structure, complete table of contents, clear section headers, consistent formatting

- **Reproducibility (5 points):** Code runs without errors, random seeds set for reproducible results, file paths work for reviewers, all required outputs generated

**Scoring:** Exceptional (9-10) demonstrates polished professional presentation. Proficient (8) demonstrates organized clear presentation. Developing (7) demonstrates adequate presentation with minor issues. Unsatisfactory (<7) demonstrates disorganized or incomplete presentation.

---

## Submission Guidelines

### Format Requirements

**Main Document:** Single R Markdown file (`homework2_lastname_firstname.Rmd`) knitting to HTML containing all analyses with embedded code chunks and narrative text. Final HTML should be self-contained (all plots embedded, no external file dependencies).

**Executive Summary:** Separate Word document or PDF (`executive_summary_lastname_firstname.docx/pdf`) formatted professionally suitable for C-level distribution. Use business language without R code or statistical jargon.

**GitHub Repository (Optional but Recommended):** Organize all homework materials in GitHub repository demonstrating professional workflow. Structure: `/data` (raw data), `/scripts` (feature engineering pipeline), `/models` (saved model objects), `/results` (performance tables, diagnostic plots), `/docs` (documentation). Include README describing repository organization.

### Submission Process

**Checkpoint Submission (Day 7):** Submit Sections 1-2 (EDA and Feature Engineering) for feedback. This checkpoint ungraded but enables course correction before investing time in modeling. Teaching staff provides feedback within 48 hours highlighting any issues requiring attention.

**Final Submission (Day 14):** Upload to course submission system: R Markdown file, knitted HTML, executive summary, any supplementary materials. Ensure all code runs without errors and produces expected outputs. Late submissions penalized 10% per day up to 3 days, after which submissions not accepted without prior arrangement.

### Collaboration Policy

Individual assignment - all submitted work must be independently written. You may discuss general approaches and technical questions with classmates but must write your own code and analyses. Copying code from classmates, GitHub, or online sources without attribution constitutes academic integrity violation resulting in zero grade and referral to academic integrity office.

You may consult:
- Course materials (slides, classworks, readings)
- R documentation and package vignettes
- Stack Overflow for specific technical questions (cite any code adapted from online sources)
- Teaching staff office hours and Q&A forum

You may NOT:
- Share code with classmates or use code shared by classmates
- Submit work written by generative AI (ChatGPT, GitHub Copilot, etc.) as your own
- Use solutions from previous course iterations or online sources without attribution

### Technical Requirements

**Software:** R version ≥4.0, RStudio recommended. Required packages: tidyverse, glmnet, caret, pROC, knitr, rmarkdown. Install packages before starting work to avoid last-minute technical issues.

**Computational Resources:** Analyses should complete in reasonable time (<30 minutes) on standard laptop. If you need to use high-performance computing resources, document this in your submission.

**Reproducibility:** Set random seed at beginning of script (set.seed(123)) ensuring identical results across runs. Document R session information (sessionInfo()) at end of document enabling debugging if issues arise during grading.

---

## Tips for Success

### Start Early

Homework requires 15-20 hours of work distributed across two weeks. Students starting in final days rarely produce high-quality work. Budget time for: EDA and feature engineering (5-6 hours), model development and comparison (4-5 hours), evaluation and diagnostics (3-4 hours), deployment planning and documentation (3-4 hours), writing and polishing (2-3 hours).

### Use Checkpoint Feedback

The Day 7 checkpoint exists to help you succeed. Students submitting checkpoints and incorporating feedback typically score 10-15 points higher than those skipping this opportunity. Common checkpoint issues: insufficient EDA depth, missing feature engineering rationale, overly complex pipelines lacking documentation. Addressing these early prevents larger problems later.

### Focus on Interpretation

This homework tests understanding and application, not just technical execution. Strong submissions explain WHY choices were made, not just WHAT was done. "I used LASSO because it provides automatic feature selection" is weak. "I used LASSO because the business requires interpretable models with <30 features, and domain experts confirmed that many of the 150 available features likely provide minimal signal" is strong.

### Balance Technical Depth and Communication

You're writing for data science manager who will evaluate technical rigor AND for business stakeholders who need to understand results. Include sufficient technical detail demonstrating competence (cross-validation approach, hyperparameter search strategy) while maintaining narrative clarity for less technical readers (executive summary, business implications sections).

### Treat as Portfolio Piece

High-quality submissions belong in professional portfolios shown to prospective employers. Approach homework as consulting engagement where you're hired to solve real business problem and must deliver professional work product. Polish visualizations, proofread text, organize code, ensure reproducibility. The extra effort differentiates exceptional submissions from adequate ones.

---

## Common Pitfalls to Avoid

**Insufficient EDA:** Diving into modeling without thorough data understanding leads to avoidable problems. Invest time understanding your data before engineering features or fitting models. Students who rush EDA typically struggle later when encountering data quality issues.

**Overfitting to Validation Set:** Using validation set for iterative model refinement biases performance estimates. Validation set should guide major decisions (model selection, hyperparameter choice) but shouldn't be used repeatedly for minor adjustments. Reserve test set for final evaluation.

**Ignoring Business Context:** Treating homework as pure technical exercise ignores half the evaluation criteria. Every technical decision should connect to business considerations. Feature engineering should reflect customer behavior logic. Model selection should balance performance with interpretability requirements. Deployment planning should address realistic operational constraints.

**Poor Documentation:** Code without comments, analyses without interpretation, results without discussion score poorly regardless of technical correctness. Each code chunk should include comments explaining purpose. Each result should include text interpreting findings and implications.

**Last-Minute Submission:** Technical issues inevitably arise (package installation problems, data loading errors, rendering failures). Last-minute submissions allow no time for troubleshooting. Complete draft 24-48 hours before deadline allowing time to address unexpected issues.

---

## Learning Beyond the Grade

This homework develops professional data science capabilities applicable across domains. The workflow (EDA → feature engineering → model development → evaluation → deployment planning) represents industry standard practice. The documentation and communication requirements mirror real workplace expectations where technical work must be explained to diverse stakeholders.

Use this assignment to:
- Build portfolio-quality work showcasing your capabilities to employers
- Develop systematic approach to regression problems transferable to future projects
- Practice integrating technical analysis with business reasoning
- Learn from mistakes through checkpoint feedback before final submission
- Demonstrate professional communication skills technical and non-technical audiences

Students treating homework as learning opportunity rather than grade chase typically produce stronger work and develop deeper understanding. Embrace the challenge, seek feedback actively, and take pride in producing professional-quality deliverable.

## Questions and Support

**Office Hours:** Teaching staff available for questions during scheduled office hours [insert schedule]. Come prepared with specific questions and work-in-progress to maximize benefit.

**Q&A Forum:** Online forum enables asynchronous questions and peer learning. Post technical questions, interpretation questions, and clarification requests. Teaching staff monitors forum daily responding within 24 hours.

**Email:** For personal issues or questions inappropriate for public forum, email teaching staff. Response within 48 hours (excludes weekends).

**What We Help With:** Clarifying requirements, debugging technical issues, discussing interpretation challenges, providing feedback on approach.

**What We Don't Help With:** Writing your code, making modeling decisions for you, debugging code shared by classmates, extending deadlines without documented extenuating circumstances.

---

## Academic Integrity Reminder

Maintaining academic integrity ensures fair evaluation and protects educational value of assignments. Violations include:

- Submitting work written by others (classmates, online sources, AI assistants)
- Sharing solutions with classmates or posting solutions online
- Using solutions from previous course iterations
- Falsifying data or results

Suspected violations investigated per university policy. Confirmed violations result in zero assignment grade and referral to academic integrity office potentially leading to course failure or disciplinary action.

When uncertain whether something constitutes violation, ask before submitting. Teaching staff clarifies policy questions without penalty.

---

**Good luck! This homework represents opportunity to demonstrate your regularization mastery while building portfolio-quality work. Approach it professionally, seek feedback actively, and take pride in producing excellent deliverable.**
