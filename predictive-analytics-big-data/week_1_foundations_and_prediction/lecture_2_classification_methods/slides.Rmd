---
title: "Advanced Regression & Regularization"
subtitle: "Predictive Analytics & Big Data | Lecture 2"
author: "MSc in Data Science and Business Analytics"
date: "November 7, 2025"
output:
  xaringan::moon_reader:
    css: [default, default-fonts, custom.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
      slideNumberFormat: 'Slide %current%'
    seal: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10,
  fig.height = 4.5,
  fig.align = 'center',
  dpi = 300,
  out.width = "100%"
)

library(tidyverse)
library(glmnet)
library(knitr)
library(kableExtra)

# Define course colors
course_colors <- c('#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51')

# Set theme with smaller base size for more content
theme_set(theme_minimal(base_size = 11))

# Create dummy objects if needed
if (!exists("ridge_cv")) {
  set.seed(42)
  n <- 100
  X_demo <- matrix(rnorm(n * 10), n, 10)
  y_demo <- rnorm(n)
  
  ridge_cv <- cv.glmnet(X_demo, y_demo, alpha = 0)
  lasso_cv <- cv.glmnet(X_demo, y_demo, alpha = 1)
  final_elastic <- cv.glmnet(X_demo, y_demo, alpha = 0.5)
  elastic_coefs <- coef(final_elastic, s = "lambda.min")
}

performance <- tibble(
  Model = c("Ridge", "LASSO", "Elastic Net"),
  `Train RMSE` = c(42.3, 40.1, 41.2),
  `Test RMSE` = c(47.5, 46.2, 45.8),
  `CV RMSE` = c(46.8, 45.9, 45.3),
  `# Features` = c(45, 18, 24)
)
```
```{css, echo=FALSE}
/* Custom CSS for better content fit */
.remark-slide-content {
  padding: 1em 2em 1em 2em;
  font-size: 20px;
}

.remark-code, .remark-inline-code {
  font-size: 14px;
}

.small {
  font-size: 16px;
}

.smaller {
  font-size: 14px;
}

.tiny {
  font-size: 12px;
}

/* Adjust spacing */
.remark-slide-content h1 {
  margin-top: 0.2em;
  margin-bottom: 0.3em;
}

.remark-slide-content h2 {
  margin-top: 0.3em;
  margin-bottom: 0.2em;
}

/* Make tables smaller */
table {
  font-size: 85%;
}

/* Reduce list spacing */
.remark-slide-content ul {
  margin: 0.3em 0;
}

.remark-slide-content li {
  margin: 0.2em 0;
}
```
```{r generate-data, include=FALSE, cache=TRUE}
# Generate realistic product pricing dataset for the lecture
n <- 500  # Products for training
p <- 50   # Features

# Create feature names
feature_names <- c(
  "avg_sales_30d", "avg_sales_90d", "competitor_price_1", "competitor_price_2",
  "inventory_level", "days_in_stock", "product_age_days", "rating_avg",
  "num_reviews", "discount_history", "seasonality_index", "brand_strength",
  "category_popularity", "price_elasticity", "customer_segment_1",
  paste0("feature_", 16:50)
)

# Generate correlated features (realistic business scenario)
correlation_structure <- matrix(0.3, p, p)
diag(correlation_structure) <- 1

# Make some features highly correlated (multicollinearity)
correlation_structure[1:4, 1:4] <- 0.85
correlation_structure[5:8, 5:8] <- 0.80

# Generate features from multivariate normal
X <- mvrnorm(n, mu = rep(0, p), Sigma = correlation_structure)
colnames(X) <- feature_names

# True model: only 10 features actually matter
true_coefficients <- rep(0, p)
true_coefficients[c(1, 3, 5, 8, 11, 14, 17, 22, 28, 35)] <- 
  c(2.5, 1.8, -1.2, 3.1, 1.5, -0.8, 2.2, 1.1, -1.5, 1.9)

# Generate price with noise
intercept <- 50
noise <- rnorm(n, 0, 5)
price <- intercept + X %*% true_coefficients + noise

# Create dataframe
pricing_data <- as.data.frame(cbind(price, X))
colnames(pricing_data)[1] <- "optimal_price"

# Split data
train_idx <- sample(1:nrow(pricing_data), 0.7 * nrow(pricing_data))
train_data <- pricing_data[train_idx, ]
test_data <- pricing_data[-train_idx, ]

# Fit OLS model for comparisons
model_full <- lm(optimal_price ~ ., data = train_data)

# Calculate OLS performance
train_pred <- predict(model_full, train_data)
test_pred <- predict(model_full, test_data)

train_rmse <- sqrt(mean((train_data$optimal_price - train_pred)^2))
test_rmse <- sqrt(mean((test_data$optimal_price - test_pred)^2))

train_r2 <- summary(model_full)$r.squared
test_r2 <- 1 - sum((test_data$optimal_price - test_pred)^2) / 
               sum((test_data$optimal_price - mean(test_data$optimal_price))^2)

train_mae <- mean(abs(train_data$optimal_price - train_pred))
test_mae <- mean(abs(test_data$optimal_price - test_pred))
```

---
class: inverse, center, middle

# Lecture 2
# Advanced Regression & Regularization

### "In God we trust. All others must bring data‚Äîand check their assumptions."
‚Äî W. Edwards Deming (adapted)

---

# Course Roadmap

.pull-left[
**Week 1: Foundations**
- Lecture 1: Regression Fundamentals
- **Lecture 2: Advanced Regression** ‚Üê *You are here*
- Lecture 3: Classification

**Week 2: Advanced Models**
- Lecture 4: Clustering
- Lecture 5: Dimensionality Reduction
- Lecture 6: Neural Networks
]

.pull-right[
**Week 3: Text & Integration**
- Lecture 7: Text Analytics
- Lecture 8: Advanced Text Methods
- Lecture 9: Integration & Deployment

**Three Portfolio Projects**
1. Predictive Pricing Intelligence
2. Customer Intelligence Platform
3. Integrated Analytics Platform
]

---

# Where We Left Off

**Lecture 1 Achievement:** Built marketing mix models with gradient descent

- Implemented regression from first principles
- Diagnosed channel effectiveness (Google, Facebook, Email)
- Made confident investment recommendations

**Today's Challenge:** Scale from 3 predictors to 50+

**Real Business Context:** Dynamic pricing for 10,000 products

---

# Today's Learning Objectives

By the end of this lecture, you will:

1. Diagnose overfitting and multicollinearity in real datasets

2. Explain bias-variance tradeoff with business implications

3. Implement Ridge, LASSO, and Elastic Net regression in R

4. Select optimal regularization parameters via cross-validation

5. Interpret regularized coefficients for stakeholder communication

6. Conduct comprehensive regression diagnostics

7. Document models for production deployment

---
class: center, middle

# Part 1: When Simple Regression Fails

---

# The New Business Challenge

**Your VP of Pricing:**

*"Last quarter's marketing model was great. Now we need something bigger."*

*"Build a dynamic pricing engine for our entire catalog: 10,000 products."*

*"Each product has 50+ features: sales history, competitor prices, seasonality, inventory, ratings, category, brand strength..."*

**She needs it deployed in 4 weeks.**

---

# The Dataset: Product Pricing
```{r display-data, echo=FALSE}
# Display sample
head(pricing_data[, 1:6], 6) %>%
  kable(digits = 2, format = "html", 
        caption = "Sample Product Features (showing 5 of 50 features)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                font_size = 12, full_width = FALSE)
```

**50 features, 500 products. What could go wrong?**

---

# Naive Approach: Use All Features
```{r show-ols-results, echo=TRUE}
# Model already fit in setup: model_full <- lm(optimal_price ~ ., data = train_data)

# Display key metrics
cat("Number of predictors:", length(coef(model_full)) - 1, "\n")
cat("Number of training observations:", nrow(train_data), "\n")
cat("Observations per predictor:", round(nrow(train_data) / (length(coef(model_full)) - 1), 1), "\n")
```

**Training Performance:** R¬≤ = `r round(train_r2, 3)`, RMSE = $`r round(train_rmse, 2)`K

**Testing Performance:** R¬≤ = `r round(test_r2, 3)`, RMSE = $`r round(test_rmse, 2)`K

---

# The Performance Gap
```{r performance-comparison, echo=FALSE, fig.height=5}
# Create performance comparison visualization
performance_df <- data.frame(
  Dataset = rep(c("Training", "Testing"), each = 2),
  Metric = rep(c("R¬≤", "RMSE"), 2),
  Value = c(train_r2, train_rmse/10, test_r2, test_rmse/10)
)

ggplot(performance_df, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_text(aes(label = round(Value, 3)), 
            position = position_dodge(width = 0.7), 
            vjust = -0.5, size = 5) +
  scale_fill_manual(values = c("Training" = "#2E7D32", "Testing" = "#C62828")) +
  labs(
    title = "Training vs Testing Performance",
    subtitle = "The model memorized training data but fails on new products",
    x = "", y = "Value"
  ) +
  theme(legend.position = "top", legend.title = element_blank()) +
  ylim(0, 1.1)
```

**This is overfitting. The model deployed to production would fail catastrophically.**

---

# Visualizing Overfitting
```{r overfitting-visualization, echo=FALSE, fig.height=5.5}
# Create visualization showing actual vs predicted
plot_data <- data.frame(
  Actual = c(train_data$optimal_price, test_data$optimal_price),
  Predicted = c(train_pred, test_pred),
  Dataset = c(rep("Training", length(train_pred)), 
              rep("Testing", length(test_pred)))
)

ggplot(plot_data, aes(x = Actual, y = Predicted, color = Dataset)) +
  geom_point(alpha = 0.6, size = 2.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", 
              color = "black", size = 1) +
  scale_color_manual(values = c("Training" = "#2E7D32", "Testing" = "#C62828")) +
  labs(
    title = "Actual vs Predicted Prices",
    subtitle = "Training points near the line, testing points scattered",
    x = "Actual Optimal Price ($K)",
    y = "Predicted Optimal Price ($K)"
  ) +
  theme(legend.position = "top") +
  facet_wrap(~ Dataset, ncol = 2)
```

---

# What Went Wrong?

**The Problem:** 50 predictors, `r nrow(train_data)` training observations

**Ratio:** `r round(nrow(train_data) / 50, 1)` observations per predictor

**Industry Rule of Thumb:** Need 10-20 observations per predictor for stable estimates

**Consequence:** Model has enough flexibility to memorize training noise

---

# The Second Problem: Multicollinearity
```{r check-coefficients, echo=TRUE}
# Extract coefficients
coefs <- coef(model_full)

# Show first 10 coefficients (excluding intercept)
head(coefs[-1], 10) %>%
  round(2) %>%
  as.data.frame() %>%
  setNames("Coefficient") %>%
  kable(format = "html", caption = "First 10 Feature Coefficients") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                font_size = 14, full_width = FALSE)
```

**Notice:** Coefficients are enormous and unstable

**Why?** Highly correlated features create numerical instability

---

# Multicollinearity Visualization
```{r correlation-heatmap, echo=FALSE, fig.height=6, fig.width=8}
# Compute correlation matrix for first 20 features
cor_matrix <- cor(pricing_data[, 2:21])

# Create heatmap
library(reshape2)
melted_cor <- melt(cor_matrix)

ggplot(melted_cor, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "#2166AC", mid = "white", high = "#B2182B",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Correlation"
  ) +
  labs(
    title = "Feature Correlation Matrix (First 20 Features)",
    subtitle = "Dark red = strong positive correlation, Dark blue = strong negative correlation",
    x = "", y = ""
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    legend.position = "right"
  ) +
  coord_fixed()
```

**Features 1-4 are highly correlated (sales metrics), causing unstable estimates**

---



# The Business Impact

**Scenario:** Model goes to production

**Week 1:** Pricing recommendations seem reasonable

**Week 2:** New competitor enters market, changes feature distribution slightly

**Week 3:** Model recommends $5 for a $50 product, $500 for a $30 product

**Week 4:** CFO demands explanation. You're in the hot seat.

**Root Cause:** Overfit model + multicollinearity = unstable predictions

---

# üéØ CLASSWORK TIME

### Navigate to: `week_1_foundations_and_prediction/lecture_3_classification_methods/classwork_1/`

---

# Classwork 1: Diagnosing Overfitting

**Time:** 30-40 minutes

**Dataset:** `product_features_extended.csv` (75 features, 400 products)

**Your Mission:**

1. Split data 70/30 train/test, fit linear regression with all 75 features

2. Calculate training and testing RMSE and R¬≤

3. Create actual vs predicted plots for both sets

4. Compute correlation matrix and identify highly correlated feature pairs (|r| > 0.85)

5. Calculate VIF for first 15 features

6. Write a professional memo (150-200 words) to VP of Pricing diagnosing the problem

**Deliverable:** Completed `student_workspace.Rmd` with analysis and memo

**Files provided:** `instructions.md`, `student_template.Rmd`, `data/product_features_extended.csv`

---

class: center, middle

# Take a 15-minute break

## When you return, we'll implement Ridge Regression to solve these problems

---

---
class: inverse, center, middle

# Welcome Back!

**What We Diagnosed:**

‚úÖ Severe overfitting (RMSE increased 800%+ from training to testing)

‚úÖ Multicollinearity (20+ feature pairs with |r| > 0.85)

‚úÖ Unstable coefficients (VIF values > 10)

‚úÖ Model unsuitable for deployment

**The Solution:** Ridge Regression (L2 Regularization)

---
class: center, middle

# Part 2: Ridge Regression (L2 Regularization)

---

# The Core Idea

**Standard Linear Regression:**
$$\min_{\beta} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

Find coefficients that minimize prediction error. Period.

**Ridge Regression:**
$$\min_{\beta} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda\sum_{j=1}^{p}\beta_j^2$$

Find coefficients that minimize prediction error **AND** keep coefficients small.

**The $\lambda$ Parameter:** Controls the penalty strength
- $\lambda = 0$: Standard regression (no penalty)
- $\lambda = \infty$: All coefficients forced to zero
- $\lambda$ in between: Balance between fit and simplicity

---

# Why Does This Help?

**Problem:** Too many features cause overfitting and instability

**Ridge Solution:** Penalize large coefficients
- Forces model to use all features moderately
- Prevents any single coefficient from becoming enormous
- Stabilizes estimates when features are correlated

**Key Insight:** Trading slight bias for massive variance reduction

**Business Translation:** Slightly less perfect fit on training data, but much better performance on new data

---

# Geometric Intuition: Constraint Region
```{r ridge-geometry, echo=FALSE, fig.height=5.5}
# Create visualization of Ridge constraint region
library(plotly)

# Generate elliptical contours for RSS (residual sum of squares)
theta <- seq(0, 2*pi, length.out = 100)
ellipse_rss <- function(a, b, center_x, center_y) {
  data.frame(
    beta1 = center_x + a * cos(theta),
    beta2 = center_y + b * sin(theta)
  )
}

# Multiple RSS contours (like topographic map)
rss_contours <- bind_rows(
  ellipse_rss(0.5, 0.3, 1.2, 0.8) %>% mutate(RSS = "RSS = 10"),
  ellipse_rss(1.0, 0.6, 1.2, 0.8) %>% mutate(RSS = "RSS = 20"),
  ellipse_rss(1.5, 0.9, 1.2, 0.8) %>% mutate(RSS = "RSS = 30"),
  ellipse_rss(2.0, 1.2, 1.2, 0.8) %>% mutate(RSS = "RSS = 40")
)

# Ridge constraint: circle
ridge_radius <- 1.0
ridge_constraint <- data.frame(
  beta1 = ridge_radius * cos(theta),
  beta2 = ridge_radius * sin(theta)
)

# OLS solution (center of ellipses)
ols_solution <- data.frame(beta1 = 1.2, beta2 = 0.8)

# Ridge solution (tangent point)
ridge_solution <- data.frame(beta1 = 0.72, beta2 = 0.48)

# Create plot
ggplot() +
  # RSS contours
  geom_path(data = rss_contours, aes(x = beta1, y = beta2, color = RSS, group = RSS),
            size = 1) +
  # Ridge constraint circle
  geom_path(data = ridge_constraint, aes(x = beta1, y = beta2),
            color = "#1976D2", size = 1.5, linetype = "solid") +
  # OLS solution
  geom_point(data = ols_solution, aes(x = beta1, y = beta2),
             color = "#C62828", size = 5, shape = 16) +
  geom_text(data = ols_solution, aes(x = beta1, y = beta2 + 0.15, label = "OLS\nSolution"),
            color = "#C62828", fontface = "bold", size = 4) +
  # Ridge solution
  geom_point(data = ridge_solution, aes(x = beta1, y = beta2),
             color = "#2E7D32", size = 5, shape = 17) +
  geom_text(data = ridge_solution, aes(x = beta1, y = beta2 - 0.2, label = "Ridge\nSolution"),
            color = "#2E7D32", fontface = "bold", size = 4) +
  # Annotations
  annotate("text", x = 0, y = 1.3, label = "Constraint: Œ≤‚ÇÅ¬≤ + Œ≤‚ÇÇ¬≤ ‚â§ t",
           color = "#1976D2", fontface = "bold", size = 4.5) +
  annotate("text", x = -1.8, y = 1.5, label = "Lower RSS\n(better fit)",
           color = "gray30", fontface = "italic", size = 3.5) +
  scale_color_manual(values = c("RSS = 10" = "gray20", "RSS = 20" = "gray40",
                                 "RSS = 30" = "gray60", "RSS = 40" = "gray80")) +
  coord_fixed(xlim = c(-2, 2.5), ylim = c(-1.5, 2)) +
  labs(
    title = "Ridge Regression: Geometric Interpretation",
    subtitle = "Solution found where RSS contour touches constraint region",
    x = expression(beta[1]),
    y = expression(beta[2])
  ) +
  theme(legend.position = "none")
```

**Ridge finds the smallest RSS achievable within the circular constraint region**

---

# Visual Comparison: OLS vs Ridge
```{r ols-vs-ridge, echo=FALSE, fig.height=5}
# Create visual comparison of coefficient magnitudes
set.seed(42)

# Simulate coefficient paths
p_features <- 20
ols_coefs <- rnorm(p_features, mean = 0, sd = 25)  # Highly variable
ridge_coefs <- ols_coefs * 0.35  # Shrunk toward zero

coef_comparison <- data.frame(
  Feature = rep(paste0("Feature ", 1:p_features), 2),
  Coefficient = c(ols_coefs, ridge_coefs),
  Method = rep(c("OLS (No Penalty)", "Ridge (Œª=0.5)"), each = p_features)
)

ggplot(coef_comparison, aes(x = Feature, y = Coefficient, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_hline(yintercept = 0, linetype = "solid", color = "black") +
  scale_fill_manual(values = c("OLS (No Penalty)" = "#C62828", 
                                "Ridge (Œª=0.5)" = "#2E7D32")) +
  labs(
    title = "OLS vs Ridge Coefficients",
    subtitle = "Ridge shrinks all coefficients toward zero, stabilizing estimates",
    x = "", y = "Coefficient Value"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    legend.position = "top",
    legend.title = element_blank()
  ) +
  coord_cartesian(ylim = c(-60, 60))
```

**Notice: Ridge coefficients are smaller but all non-zero**

---

# Implementing Ridge in R

**Package:** `glmnet` (gold standard for regularized regression)

**Key Functions:**
- `glmnet()`: Fit Ridge/LASSO/Elastic Net models
- `cv.glmnet()`: Cross-validation to select optimal $\lambda$
- `predict()`: Generate predictions from fitted model

**Important Parameters:**
- `alpha = 0`: Ridge regression (L2 penalty)
- `alpha = 1`: LASSO regression (L1 penalty)
- `alpha ‚àà (0,1)`: Elastic Net (combination)

---

# Ridge Regression: Step-by-Step Code
```{r ridge-implementation, echo=TRUE}
# Load required library
library(glmnet)

# Prepare data (glmnet requires matrix format, not dataframe)
X_train <- model.matrix(optimal_price ~ . - 1, data = train_data)  # -1 removes intercept
y_train <- train_data$optimal_price

X_test <- model.matrix(optimal_price ~ . - 1, data = test_data)
y_test <- test_data$optimal_price

# Fit Ridge regression across a range of lambda values
# glmnet automatically tries 100 different lambda values
ridge_model <- glmnet(
  x = X_train,
  y = y_train,
  alpha = 0,              # 0 = Ridge, 1 = LASSO
  standardize = TRUE      # Standardize features (important!)
)

# Examine the model
print(ridge_model)
```

**Output shows:** Number of features, number of lambda values tried, percentage deviance explained

---

# Understanding the Ridge Model Object
```{r ridge-model-structure, echo=TRUE}
# What lambda values were tried?
cat("Number of lambda values:", length(ridge_model$lambda), "\n")
cat("Lambda range: ", round(min(ridge_model$lambda), 4), 
    "to", round(max(ridge_model$lambda), 2), "\n\n")

# Coefficients for specific lambda value (e.g., lambda = 1.0)
lambda_index <- which.min(abs(ridge_model$lambda - 1.0))
selected_lambda <- ridge_model$lambda[lambda_index]

cat("Selected lambda:", round(selected_lambda, 4), "\n")
cat("Number of non-zero coefficients:", 
    sum(coef(ridge_model, s = selected_lambda) != 0), "\n")
```

**Key Insight:** Ridge NEVER sets coefficients exactly to zero‚Äîall features remain in the model with reduced magnitude

---

# Visualizing the Regularization Path
```{r regularization-path, echo=TRUE, fig.height=5}
# Plot coefficient paths as lambda increases
plot(ridge_model, xvar = "lambda", label = TRUE, lwd = 2)
title("Ridge Regression: Coefficient Paths", line = 2.5)
```

**Interpretation:**
- X-axis: log(Œª) ‚Äî penalty strength increases left to right
- Y-axis: Coefficient values
- Each line: One predictor's coefficient trajectory
- **As Œª increases, all coefficients shrink toward zero (but never reach exactly zero)**

---

# Choosing Optimal Lambda: Cross-Validation

**The Problem:** Which lambda value gives best predictive performance?

**The Solution:** k-fold cross-validation

**Process:**
1. Split training data into k folds (typically k=10)
2. For each lambda value:
   - Train on k-1 folds
   - Validate on remaining fold
   - Repeat k times
3. Average validation error across folds
4. Select lambda with minimum average error

**R Implementation:** `cv.glmnet()` does this automatically

---

# Cross-Validation in Action
```{r ridge-cv, echo=TRUE, fig.height=4.5}
# Perform 10-fold cross-validation to find optimal lambda
set.seed(42)
ridge_cv <- cv.glmnet(
  x = X_train,
  y = y_train,
  alpha = 0,
  nfolds = 10,
  type.measure = "mse"  # Use mean squared error
)

# Plot cross-validation results
plot(ridge_cv)
title("10-Fold Cross-Validation for Ridge Regression", line = 2.5)
```

**Vertical lines:** 
- Left: `lambda.min` (lowest CV error)
- Right: `lambda.1se` (simplest model within 1 SE of minimum)

---

# Extracting Optimal Lambda Values
```{r optimal-lambda, echo=TRUE}
# Extract optimal lambda values
lambda_min <- ridge_cv$lambda.min      # Lambda with minimum CV error
lambda_1se <- ridge_cv$lambda.1se      # Lambda within 1 SE (more conservative)

cat("Lambda min (best CV performance):", round(lambda_min, 4), "\n")
cat("Lambda 1se (simplest within 1 SE):", round(lambda_1se, 4), "\n")

# Get CV error at optimal lambda
min_mse <- min(ridge_cv$cvm)
cat("\nMinimum CV MSE:", round(min_mse, 2), "\n")
cat("Corresponding RMSE:", round(sqrt(min_mse), 2), "\n")
```

**Decision:** Most practitioners use `lambda.min` for best predictive performance, or `lambda.1se` for simpler, more interpretable models

---

# Making Predictions with Ridge
```{r ridge-predictions, echo=TRUE}
# Generate predictions using optimal lambda
ridge_pred_train <- predict(ridge_cv, newx = X_train, s = "lambda.min")
ridge_pred_test <- predict(ridge_cv, newx = X_test, s = "lambda.min")

# Calculate performance metrics
ridge_train_rmse <- sqrt(mean((y_train - ridge_pred_train)^2))
ridge_test_rmse <- sqrt(mean((y_test - ridge_pred_test)^2))

ridge_train_r2 <- 1 - sum((y_train - ridge_pred_train)^2) / 
                       sum((y_train - mean(y_train))^2)
ridge_test_r2 <- 1 - sum((y_test - ridge_pred_test)^2) / 
                      sum((y_test - mean(y_test))^2)

cat("Ridge Training RMSE:", round(ridge_train_rmse, 2), "\n")
cat("Ridge Testing RMSE:", round(ridge_test_rmse, 2), "\n\n")

cat("Ridge Training R¬≤:", round(ridge_train_r2, 3), "\n")
cat("Ridge Testing R¬≤:", round(ridge_test_r2, 3), "\n")
```

---

# Comparing OLS vs Ridge Performance
```{r compare-ols-ridge, echo=FALSE, fig.height=5}
# Create comparison dataframe
performance_comp <- data.frame(
  Model = rep(c("OLS", "Ridge"), each = 4),
  Metric = rep(c("Train RMSE", "Test RMSE", "Train R¬≤", "Test R¬≤"), 2),
  Value = c(
    train_rmse, test_rmse, train_r2, test_r2,  # OLS from earlier
    ridge_train_rmse, ridge_test_rmse, ridge_train_r2, ridge_test_r2  # Ridge
  )
)

# Separate plot for RMSE and R¬≤
performance_comp %>%
  mutate(
    Dataset = ifelse(grepl("Train", Metric), "Training", "Testing"),
    Metric_Type = ifelse(grepl("RMSE", Metric), "RMSE ($K)", "R¬≤")
  ) %>%
  ggplot(aes(x = Model, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_text(aes(label = round(Value, 2)), 
            position = position_dodge(width = 0.7), 
            vjust = -0.5, size = 4, fontface = "bold") +
  facet_wrap(~ Metric_Type, scales = "free_y", ncol = 2) +
  scale_fill_manual(values = c("Training" = "#2E7D32", "Testing" = "#1976D2")) +
  labs(
    title = "OLS vs Ridge: Performance Comparison",
    subtitle = "Ridge reduces training-testing gap (generalization improvement)",
    x = "", y = "Value"
  ) +
  theme(legend.position = "top")
```

**Key Result:** Ridge sacrifices some training performance for better testing performance

---

# Quantifying the Improvement
```{r improvement-metrics, echo=TRUE}
# Calculate improvements
rmse_gap_ols <- test_rmse - train_rmse
rmse_gap_ridge <- ridge_test_rmse - ridge_train_rmse
gap_reduction <- ((rmse_gap_ols - rmse_gap_ridge) / rmse_gap_ols) * 100

r2_drop_ols <- train_r2 - test_r2
r2_drop_ridge <- ridge_train_r2 - ridge_test_r2
r2_improvement <- ((r2_drop_ols - r2_drop_ridge) / r2_drop_ols) * 100

cat("RMSE Train-Test Gap:\n")
cat("  OLS:", round(rmse_gap_ols, 2), "  |  Ridge:", 
    round(rmse_gap_ridge, 2), "\n")
cat("  Gap Reduction:", round(gap_reduction, 1), "%\n\n")

cat("R¬≤ Train-Test Drop:\n")
cat("  OLS:", round(r2_drop_ols, 3), "  |  Ridge:", 
    round(r2_drop_ridge, 3), "\n")
cat("  Improvement:", round(r2_improvement, 1), "%\n")
```

**Business Impact:** Ridge model generalizes `r round(gap_reduction, 0)`% better to new products

---

# Examining Ridge Coefficients
```{r ridge-coefficients, echo=TRUE}
# Extract coefficients at optimal lambda
ridge_coefs <- coef(ridge_cv, s = "lambda.min")

# Compare top 10 coefficients: OLS vs Ridge
ols_coefs <- coef(model_full)

coef_comparison <- data.frame(
  Feature = names(ols_coefs)[-1][1:10],  # First 10 features
  OLS = as.vector(ols_coefs[-1][1:10]),
  Ridge = as.vector(ridge_coefs[-1][1:10])
) %>%
  mutate(
    Shrinkage = (abs(OLS) - abs(Ridge)) / abs(OLS) * 100
  )

coef_comparison %>%
  kable(digits = 2, caption = "Top 10 Features: OLS vs Ridge Coefficients") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), font_size = 12)
```

**Observation:** Ridge shrinks large coefficients more aggressively than small ones

---

# Ridge Coefficient Visualization
```{r ridge-coef-viz, echo=FALSE, fig.height=5.5}
# Visualize coefficient shrinkage for first 20 features
coef_viz <- data.frame(
  Feature = names(ols_coefs)[-1][1:20],
  OLS = as.vector(ols_coefs[-1][1:20]),
  Ridge = as.vector(ridge_coefs[-1][1:20])
) %>%
  pivot_longer(cols = c(OLS, Ridge), names_to = "Method", values_to = "Coefficient")

ggplot(coef_viz, aes(x = Feature, y = Coefficient, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_hline(yintercept = 0, linetype = "solid", color = "black") +
  scale_fill_manual(values = c("OLS" = "#C62828", "Ridge" = "#2E7D32")) +
  labs(
    title = "Coefficient Shrinkage: OLS vs Ridge (First 20 Features)",
    subtitle = "Ridge systematically reduces coefficient magnitudes",
    x = "", y = "Coefficient Value"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    legend.position = "top"
  )
```

---

# Interpreting Ridge Coefficients

**Challenge:** With regularization, coefficients aren't directly interpretable as "unit change in X ‚Üí change in Y"

**Why?** Penalty distorts natural scale relationships

**Practical Approach:**

1. **Relative Importance:** Larger |coefficient| = more important feature
2. **Direction:** Sign indicates positive/negative relationship
3. **Magnitude Comparisons:** Compare across features within same model

**For Stakeholders:**
*"Feature X has a coefficient of 2.3 in our Ridge model, meaning it's among the top 5 drivers of price. A positive coefficient indicates higher X values lead to higher optimal prices, though the exact unit interpretation is affected by the regularization we applied to stabilize the model."*

---

# When to Use Ridge Regression

**Use Ridge When:**

‚úÖ You have many correlated predictors (multicollinearity)

‚úÖ You want to keep all features in the model

‚úÖ Features have small-to-moderate effects (no true zeros)

‚úÖ Model stability is critical for deployment

‚úÖ You're working with wide data (many features, fewer observations)

**Don't Use Ridge When:**

‚ùå You need automatic feature selection (use LASSO instead)

‚ùå You know many features are truly irrelevant (use LASSO instead)

‚ùå You have far more observations than features and no multicollinearity (OLS is fine)

---

# Ridge for Multicollinearity: A Case Study
```{r multicollinearity-demo, echo=FALSE, fig.height=5}
# Demonstrate how Ridge handles correlated features
# Create highly correlated features
set.seed(42)
n_obs <- 200
x1 <- rnorm(n_obs)
x2 <- x1 + rnorm(n_obs, 0, 0.1)  # Highly correlated with x1
x3 <- rnorm(n_obs)
y_sim <- 2*x1 + 3*x3 + rnorm(n_obs, 0, 0.5)

# Fit OLS
data_sim <- data.frame(y = y_sim, x1 = x1, x2 = x2, x3 = x3)
ols_sim <- lm(y ~ x1 + x2 + x3, data = data_sim)

# Fit Ridge
X_sim <- as.matrix(data_sim[, c("x1", "x2", "x3")])
ridge_sim <- cv.glmnet(X_sim, y_sim, alpha = 0)

# Compare coefficients
coef_sim <- data.frame(
  Feature = c("x1", "x2", "x3"),
  True = c(2, 0, 3),
  OLS = coef(ols_sim)[-1],
  Ridge = as.vector(coef(ridge_sim, s = "lambda.min")[-1])
)

coef_sim %>%
  pivot_longer(cols = c(True, OLS, Ridge), names_to = "Method", values_to = "Coefficient") %>%
  ggplot(aes(x = Feature, y = Coefficient, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_hline(yintercept = 0, linetype = "solid") +
  scale_fill_manual(values = c("True" = "black", "OLS" = "#C62828", "Ridge" = "#2E7D32")) +
  labs(
    title = "Handling Multicollinearity: True vs OLS vs Ridge",
    subtitle = "x1 and x2 are highly correlated (r=0.995), x3 is independent",
    x = "", y = "Coefficient Value"
  ) +
  theme(legend.position = "top")
```

**Result:** Ridge distributes weight sensibly; OLS is unstable when x1 and x2 are correlated

---

# Business Decision: Deploy the Model?

**Scenario:** VP asks if the Ridge model is ready for production

**Your Analysis:**

| Criterion | OLS Model | Ridge Model | Threshold |
|-----------|-----------|-------------|-----------|
| Testing RMSE | $19.8K | $8.2K | < $10K ‚úÖ |
| Train-Test Gap | 847% | 215% | < 300% ‚úÖ |
| Coefficient Stability | High VIF (>10) | Stable | VIF < 5 ‚úÖ |
| R¬≤ on Test | 0.62 | 0.81 | > 0.75 ‚úÖ |

**Recommendation:** Ridge model meets all deployment criteria. Proceed with limited rollout and monitoring.

---

# Deployment Checklist

**Before deploying any model:**

- [ ] Testing RMSE acceptable for business use case
- [ ] Train-test gap < 300% (reasonable generalization)
- [ ] Coefficients make business sense (directionally correct)
- [ ] No extreme coefficient values (magnitude < 100)
- [ ] Cross-validation performed with k‚â•5 folds
- [ ] Performance documented with confidence intervals
- [ ] Stakeholders understand model limitations
- [ ] Monitoring plan in place for production performance
- [ ] Retraining schedule defined (e.g., monthly)

---

# From Laptop to Big Data: Ridge at Scale

**Current:** 500 products, 75 features on laptop with `glmnet`

**Big Data Equivalent:** 100M products, 10K features on Spark cluster

**Conceptual Mapping:**

| Laptop (R) | Big Data (Spark MLlib) |
|------------|------------------------|
| `glmnet()` | `LinearRegressionWithSGD` + L2 penalty |
| `cv.glmnet()` | `CrossValidator` with distributed folds |
| In-memory matrix | Distributed RDD/DataFrame |
| Single machine | Partitioned across nodes |

**Key Concept:** Ridge's stability prevents numerical errors when gradients computed across distributed systems

**Scalability Benefit:** Regularization reduces coefficient matrix size, enabling efficient distributed storage

---

# Ridge Summary: Key Takeaways

‚úÖ **What:** Adds L2 penalty ($\lambda \sum \beta^2$) to ordinary least squares

‚úÖ **Why:** Reduces overfitting and handles multicollinearity

‚úÖ **How:** Shrinks all coefficients toward zero (but never to exactly zero)

‚úÖ **When:** Many correlated features, need stability, want to keep all features

‚úÖ **Trade-off:** Slight bias for massive variance reduction

‚úÖ **Implementation:** `glmnet(alpha=0)` with `cv.glmnet()` for lambda selection

**Next:** LASSO regression‚Äîwhat if we want automatic feature selection?

---

# üéØ CLASSWORK TIME

### Navigate to: `week_1_foundations_and_prediction/lecture_3_classification_methods/classwork_2/`

---

# Classwork 2: Building Ridge Models

**Time:** 30-40 minutes

**Dataset:** Same `product_features_extended.csv` from Classwork 1

**Your Mission:**

1. Implement Ridge regression with `glmnet` (alpha=0)

2. Use 10-fold cross-validation to find optimal lambda

3. Compare Ridge vs OLS performance (RMSE, R¬≤, train-test gap)

4. Visualize regularization path and coefficient shrinkage

5. Extract and interpret the top 10 most important features

6. Make a deployment recommendation based on your analysis

**Deliverable:** Completed `student_workspace.Rmd` with Ridge implementation and analysis

**Files provided:** `instructions.md`, `student_template.Rmd`, data already in folder

---

class: center, middle

# Take a 15-minute break

## When you return, we'll explore LASSO for automatic feature selection

---

---

# Welcome Back!

**What We Accomplished with Ridge:**

‚úÖ Reduced overfitting through L2 penalty

‚úÖ Stabilized coefficients despite multicollinearity

‚úÖ Improved generalization (train-test gap reduced by ~60%)

‚úÖ Model ready for deployment

**The Limitation:** Ridge keeps ALL 50 features in the model

**The Question:** What if only 10-15 features truly matter?

---
class: center, middle

# Part 3: LASSO Regression (L1 Regularization)

---

# The Feature Selection Problem

**Current Situation:**

- 50 features in our pricing model
- Ridge shrinks all coefficients but keeps all features
- Stakeholders ask: "Which features actually matter?"
- Deployment team asks: "Do we really need to collect all 50 features?"

**Business Reality:**

- Each feature costs money to collect and maintain
- Simpler models are easier to explain to executives
- Fewer features mean faster predictions in production

**Solution Needed:** Automatic feature selection during model fitting

---

# LASSO: The Core Idea

**Ridge Regression (L2):**
$$\min_{\beta} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda\sum_{j=1}^{p}\beta_j^2$$

Penalizes the **square** of coefficients

**LASSO Regression (L1):**
$$\min_{\beta} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda\sum_{j=1}^{p}|\beta_j|$$

Penalizes the **absolute value** of coefficients

**Key Difference:** Absolute value penalty forces many coefficients to **exactly zero**

**LASSO = Least Absolute Shrinkage and Selection Operator**

---

# Why Does L1 Create Sparsity?

**Geometric Intuition: Ridge vs LASSO Constraint Regions**
```{r lasso-geometry, echo=FALSE, fig.height=5.5}
# Create visualization comparing Ridge and LASSO constraint regions
theta <- seq(0, 2*pi, length.out = 100)

# Ridge constraint: circle
ridge_constraint <- data.frame(
  beta1 = cos(theta),
  beta2 = sin(theta),
  Type = "Ridge (L2)"
)

# LASSO constraint: diamond
lasso_constraint <- data.frame(
  beta1 = c(seq(1, 0, length.out = 25), seq(0, -1, length.out = 25),
            seq(-1, 0, length.out = 25), seq(0, 1, length.out = 25)),
  beta2 = c(seq(0, 1, length.out = 25), seq(1, 0, length.out = 25),
            seq(0, -1, length.out = 25), seq(-1, 0, length.out = 25)),
  Type = "LASSO (L1)"
)

# Combine constraints
constraints <- bind_rows(ridge_constraint, lasso_constraint)

# RSS contours (ellipses)
ellipse_rss <- function(a, b, center_x, center_y) {
  data.frame(
    beta1 = center_x + a * cos(theta),
    beta2 = center_y + b * sin(theta)
  )
}

rss_contours <- bind_rows(
  ellipse_rss(0.5, 0.3, 1.2, 0.8) %>% mutate(RSS = "RSS = 10"),
  ellipse_rss(1.0, 0.6, 1.2, 0.8) %>% mutate(RSS = "RSS = 20"),
  ellipse_rss(1.5, 0.9, 1.2, 0.8) %>% mutate(RSS = "RSS = 30")
)

# OLS solution
ols_solution <- data.frame(beta1 = 1.2, beta2 = 0.8)

# Ridge solution (on circle)
ridge_solution <- data.frame(beta1 = 0.72, beta2 = 0.48, Type = "Ridge")

# LASSO solution (on corner - one coefficient zero)
lasso_solution <- data.frame(beta1 = 1.0, beta2 = 0, Type = "LASSO")

ggplot() +
  # RSS contours
  geom_path(data = rss_contours, aes(x = beta1, y = beta2, color = RSS, group = RSS),
            size = 1, alpha = 0.6) +
  # Constraint regions
  geom_path(data = constraints, aes(x = beta1, y = beta2, linetype = Type),
            size = 1.5, color = "#1976D2") +
  # OLS solution
  geom_point(data = ols_solution, aes(x = beta1, y = beta2),
             color = "#C62828", size = 5, shape = 16) +
  geom_text(data = ols_solution, aes(x = beta1, y = beta2 + 0.15, label = "OLS"),
            color = "#C62828", fontface = "bold", size = 4) +
  # Ridge solution
  geom_point(data = ridge_solution, aes(x = beta1, y = beta2),
             color = "#2E7D32", size = 5, shape = 17) +
  # LASSO solution
  geom_point(data = lasso_solution, aes(x = beta1, y = beta2),
             color = "#FF6F00", size = 5, shape = 18) +
  geom_text(data = lasso_solution, aes(x = beta1 + 0.2, y = beta2 - 0.15, label = "LASSO\n(Œ≤‚ÇÇ=0)"),
            color = "#FF6F00", fontface = "bold", size = 4) +
  scale_color_manual(values = c("RSS = 10" = "gray20", "RSS = 20" = "gray40", "RSS = 30" = "gray60")) +
  scale_linetype_manual(values = c("Ridge (L2)" = "solid", "LASSO (L1)" = "solid")) +
  coord_fixed(xlim = c(-1.5, 2), ylim = c(-1.5, 1.5)) +
  labs(
    title = "Ridge vs LASSO: Why L1 Creates Sparsity",
    subtitle = "LASSO's diamond corners cause coefficients to hit exactly zero",
    x = expression(beta[1]),
    y = expression(beta[2])
  ) +
  theme(legend.position = "bottom")
```

**Key Insight:** LASSO's diamond-shaped constraint has corners on the axes‚Äîcontours naturally touch at these corners, setting coefficients to zero

---

# The Sparsity Phenomenon

**Ridge (L2 Penalty):**
- Circular constraint region
- Solution can occur anywhere on circle
- All coefficients non-zero (just small)

**LASSO (L1 Penalty):**
- Diamond-shaped constraint region
- Corners align with axes (Œ≤‚ÇÅ=0 or Œ≤‚ÇÇ=0)
- RSS contours likely to touch corners
- Result: Exact zeros, automatic feature selection

**Higher Dimensions:**
- With 50 features, LASSO has 50 corners
- Might select only 10-15 features
- Others set exactly to zero

---

# Implementing LASSO in R

**Same Package, Different Alpha:**
```{r lasso-setup, echo=TRUE}
# Prepare data (same as Ridge - already have X_train, y_train, X_test, y_test)
# These were created in Ridge section

# Fit LASSO regression (alpha = 1 for L1 penalty)
lasso_model <- glmnet(
  x = X_train,
  y = y_train,
  alpha = 1,              # 1 = LASSO (L1), 0 = Ridge (L2)
  standardize = TRUE
)

# Examine model
print(lasso_model)
```

**Output shows:** Number of features, lambda values, and **Df** (degrees of freedom = number of non-zero coefficients)

---

# LASSO Model Structure
```{r lasso-structure, echo=TRUE}
# Lambda range
cat("Number of lambda values:", length(lasso_model$lambda), "\n")
cat("Lambda range:", round(min(lasso_model$lambda), 4), 
    "to", round(max(lasso_model$lambda), 2), "\n\n")

# For a specific lambda, count non-zero coefficients
lambda_idx <- 50  # Example: 50th lambda value
selected_lambda <- lasso_model$lambda[lambda_idx]
coef_at_lambda <- coef(lasso_model, s = selected_lambda)
n_nonzero <- sum(coef_at_lambda != 0) - 1  # Subtract intercept

cat("At lambda =", round(selected_lambda, 4), "\n")
cat("Non-zero coefficients:", n_nonzero, "out of", ncol(X_train), "\n")
cat("Features eliminated:", ncol(X_train) - n_nonzero, "\n")
```

**This is automatic feature selection in action!**

---

# LASSO Regularization Path
```{r lasso-path, echo=TRUE, fig.height=5.5}
# Plot coefficient paths
plot(lasso_model, xvar = "lambda", label = TRUE, lwd = 2)
title("LASSO Regularization Path", line = 2.5)
```

**Key Observations:**
- As Œª increases (right side), coefficients hit exactly zero
- Features drop out one at a time
- Most aggressive shrinkage sends weak predictors to zero first
- Strong predictors remain longer

---

# Comparing Ridge vs LASSO Paths
```{r ridge-lasso-path-comparison, echo=FALSE, fig.height=6}
# Create side-by-side comparison
par(mfrow = c(1, 2))

# Ridge path
plot(ridge_model, xvar = "lambda", label = FALSE, lwd = 2, 
     main = "Ridge: All Coefficients Remain")

# LASSO path
plot(lasso_model, xvar = "lambda", label = FALSE, lwd = 2,
     main = "LASSO: Coefficients Reach Zero")

par(mfrow = c(1, 1))
```

**Visual Difference:**
- Ridge: Lines approach zero but never touch
- LASSO: Lines hit zero axis and stay there

---

# Cross-Validation for LASSO
```{r lasso-cv, echo=TRUE, fig.height=4.5}
# Perform 10-fold cross-validation
set.seed(42)
lasso_cv <- cv.glmnet(
  x = X_train,
  y = y_train,
  alpha = 1,           # LASSO
  nfolds = 10,
  type.measure = "mse"
)

# Plot CV results
plot(lasso_cv)
title("Cross-Validation for LASSO", line = 2.5)
```

**Top axis shows:** Number of non-zero coefficients at each lambda

---

# Optimal Lambda for LASSO
```{r lasso-optimal-lambda, echo=TRUE}
# Extract optimal lambdas
lasso_lambda_min <- lasso_cv$lambda.min
lasso_lambda_1se <- lasso_cv$lambda.1se

# Number of features selected at each lambda
coef_min <- coef(lasso_cv, s = "lambda.min")
coef_1se <- coef(lasso_cv, s = "lambda.1se")

n_features_min <- sum(coef_min != 0) - 1  # Exclude intercept
n_features_1se <- sum(coef_1se != 0) - 1

cat("Lambda min:", round(lasso_lambda_min, 4), "\n")
cat("  Features selected:", n_features_min, "out of", ncol(X_train), "\n\n")

cat("Lambda 1se:", round(lasso_lambda_1se, 4), "\n")
cat("  Features selected:", n_features_1se, "out of", ncol(X_train), "\n")
```

**Decision:** `lambda.min` maximizes performance, `lambda.1se` creates simplest model within 1 SE

---

# LASSO Performance Evaluation
```{r lasso-performance, echo=TRUE}
# Generate predictions using lambda.min
lasso_pred_train <- predict(lasso_cv, newx = X_train, s = "lambda.min")
lasso_pred_test <- predict(lasso_cv, newx = X_test, s = "lambda.min")

# Calculate metrics
lasso_train_rmse <- sqrt(mean((y_train - lasso_pred_train)^2))
lasso_test_rmse <- sqrt(mean((y_test - lasso_pred_test)^2))

lasso_train_r2 <- 1 - sum((y_train - lasso_pred_train)^2) / 
                       sum((y_train - mean(y_train))^2)
lasso_test_r2 <- 1 - sum((y_test - lasso_pred_test)^2) / 
                      sum((y_test - mean(y_test))^2)

cat("LASSO Training RMSE:", round(lasso_train_rmse, 2), "\n")
cat("LASSO Testing RMSE:", round(lasso_test_rmse, 2), "\n\n")

cat("LASSO Training R¬≤:", round(lasso_train_r2, 3), "\n")
cat("LASSO Testing R¬≤:", round(lasso_test_r2, 3), "\n")
```

---

# Three-Way Comparison: OLS vs Ridge vs LASSO
```{r three-way-comparison, echo=FALSE, fig.height=5.5}
# Create comprehensive comparison
comparison_data <- data.frame(
  Model = rep(c("OLS", "Ridge", "LASSO"), each = 4),
  Metric = rep(c("Train RMSE", "Test RMSE", "Train R¬≤", "Test R¬≤"), 3),
  Value = c(
    # OLS
    train_rmse, test_rmse, train_r2, test_r2,
    # Ridge (from earlier)
    ridge_train_rmse, ridge_test_rmse, ridge_train_r2, ridge_test_r2,
    # LASSO
    lasso_train_rmse, lasso_test_rmse, lasso_train_r2, lasso_test_r2
  )
) %>%
  mutate(
    Dataset = ifelse(grepl("Train", Metric), "Training", "Testing"),
    Metric_Type = ifelse(grepl("RMSE", Metric), "RMSE ($K)", "R¬≤")
  )

ggplot(comparison_data, aes(x = Model, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_text(aes(label = round(Value, 2)), 
            position = position_dodge(width = 0.7), 
            vjust = -0.5, size = 3.5, fontface = "bold") +
  facet_wrap(~ Metric_Type, scales = "free_y", ncol = 2) +
  scale_fill_manual(values = c("Training" = "#2E7D32", "Testing" = "#1976D2")) +
  labs(
    title = "Model Comparison: OLS vs Ridge vs LASSO",
    subtitle = "LASSO achieves comparable performance with automatic feature selection",
    x = "", y = "Value"
  ) +
  theme(legend.position = "top", axis.text.x = element_text(angle = 15, hjust = 1))
```

---

# Feature Selection: Which Features Did LASSO Keep?
```{r lasso-selected-features, echo=TRUE}
# Extract non-zero coefficients at lambda.min
lasso_coefs <- coef(lasso_cv, s = "lambda.min")
selected_features <- which(lasso_coefs != 0)
selected_names <- rownames(lasso_coefs)[selected_features]
selected_values <- as.vector(lasso_coefs[selected_features])

# Create dataframe (exclude intercept)
selected_df <- data.frame(
  Feature = selected_names[-1],
  Coefficient = selected_values[-1]
) %>%
  arrange(desc(abs(Coefficient)))

# Display top 15
head(selected_df, 15) %>%
  kable(digits = 3, caption = "LASSO Selected Features (Top 15)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), font_size = 12)
```

---

# Visualizing Feature Selection
```{r feature-selection-viz, echo=FALSE, fig.height=6}
# Create visualization showing which features were kept vs eliminated
all_features <- colnames(X_train)
feature_status <- data.frame(
  Feature = all_features,
  Selected = all_features %in% selected_df$Feature,
  Coefficient = sapply(all_features, function(f) {
    idx <- which(selected_df$Feature == f)
    if (length(idx) > 0) selected_df$Coefficient[idx] else 0
  })
) %>%
  mutate(
    Feature_Num = 1:n(),
    Status = ifelse(Selected, "Selected", "Eliminated")
  )

ggplot(feature_status, aes(x = Feature_Num, y = abs(Coefficient), fill = Status)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(values = c("Selected" = "#2E7D32", "Eliminated" = "#E0E0E0")) +
  labs(
    title = "LASSO Feature Selection Results",
    subtitle = paste0("Selected ", sum(feature_status$Selected), " out of ", 
                     nrow(feature_status), " features"),
    x = "Feature Index",
    y = "Absolute Coefficient Value"
  ) +
  theme(legend.position = "top")
```

**LASSO automatically identified the most important `r sum(feature_status$Selected)` features**

---

# Business Value of Feature Selection

**Cost Savings:**

- **Data Collection:** Don't need to collect `r sum(!feature_status$Selected)` features
- **Storage:** Reduced database requirements
- **Processing:** Faster predictions (fewer computations)
- **Maintenance:** Fewer data pipelines to monitor

**Model Interpretability:**

- Easier to explain `r sum(feature_status$Selected)` features than 50
- Stakeholders can focus on what truly matters
- Reduces "black box" perception

**Deployment Efficiency:**

- Scoring new products is faster
- Lower latency in production systems
- Simpler error handling

---

# Comparing Selected Features Across Models
```{r coefficient-comparison-all, echo=FALSE, fig.height=6}
# Compare coefficients for top 10 LASSO-selected features
top_features <- head(selected_df$Feature, 10)

# Get coefficients from all three models for these features
coef_comparison_df <- data.frame(
  Feature = top_features,
  OLS = sapply(top_features, function(f) coef(model_full)[f]),
  Ridge = sapply(top_features, function(f) {
    idx <- which(rownames(ridge_coefs) == f)
    if (length(idx) > 0) as.vector(ridge_coefs)[idx] else 0
  }),
  LASSO = selected_df$Coefficient[1:10]
) %>%
  pivot_longer(cols = c(OLS, Ridge, LASSO), names_to = "Model", values_to = "Coefficient")

ggplot(coef_comparison_df, aes(x = Feature, y = Coefficient, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.8) +
  geom_hline(yintercept = 0, linetype = "solid", color = "black") +
  scale_fill_manual(values = c("OLS" = "#C62828", "Ridge" = "#1976D2", "LASSO" = "#FF6F00")) +
  labs(
    title = "Top 10 Features: Coefficient Comparison",
    subtitle = "LASSO provides sparse solution with strong coefficients",
    x = "", y = "Coefficient Value"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    legend.position = "top"
  )
```

---

# When to Use LASSO

**Use LASSO When:**

‚úÖ You have many features but suspect only a few are truly important

‚úÖ You need automatic feature selection

‚úÖ Model interpretability is critical (fewer features = clearer story)

‚úÖ Deployment efficiency matters (faster scoring, less data collection)

‚úÖ You want to identify the "signal" features among "noise"

**Don't Use LASSO When:**

‚ùå Features are highly correlated (LASSO arbitrarily picks one, drops others)

‚ùå You know many features have small but real effects (Ridge is better)

‚ùå You have very few features relative to observations (OLS is fine)

---

# LASSO Limitation: Handling Correlated Features

**Problem:** When features are highly correlated, LASSO:
- Picks one arbitrarily
- Sets others to zero
- May change selection with slight data changes

**Example:**
```{r lasso-correlation-issue, echo=FALSE}
# Demonstrate instability with correlated features
cat("Features 1-4 are highly correlated (r > 0.85)\n\n")

# Check which of the first 4 features LASSO selected
selected_from_corr <- intersect(colnames(X_train)[1:4], selected_df$Feature)

cat("LASSO selected from this correlated group:", 
    paste(selected_from_corr, collapse = ", "), "\n")
cat("Others set to zero despite high correlation\n")
```

**Solution:** Elastic Net (coming next) handles this better

---

# LASSO for High-Dimensional Problems

**Scenario:** More features than observations (p > n)

**Example:** Genomics with 20,000 genes but 100 patients

**Why LASSO Excels:**

- OLS is impossible (underdetermined system)
- Ridge gives solutions but keeps all 20,000 features
- LASSO finds sparse solution (maybe 50-100 genes)

**Real Application:** Cancer classification from gene expression
- Start with 20,000 genes
- LASSO selects 30-50 biomarkers
- Clinically interpretable and actionable

---

# From Laptop to Big Data: LASSO at Scale

**Current:** 500 products, 50 features on laptop

**Big Data Reality:** 100M products, 100K features on Spark cluster

**Why LASSO Matters at Scale:**

| Aspect | Impact |
|--------|--------|
| **Storage** | Sparse coefficient vector: store only non-zero values |
| **Computation** | Scoring requires fewer multiply-add operations |
| **Communication** | Less data transfer between cluster nodes |
| **Memory** | Reduced memory footprint per partition |

**Conceptual Mapping:**
- Laptop: `glmnet(alpha=1)`
- Spark: `LinearRegressionWithSGD` + L1 penalty
- Both create sparse solutions for efficiency

---

# LASSO Summary: Key Takeaways

‚úÖ **What:** L1 penalty ($\lambda \sum |\beta|$) that forces exact zeros

‚úÖ **Why:** Automatic feature selection, model simplicity, deployment efficiency

‚úÖ **How:** Diamond-shaped constraint creates sparsity at corners

‚úÖ **When:** Many features, suspect most are noise, need interpretability

‚úÖ **Trade-off:** May be unstable with highly correlated features

‚úÖ **Implementation:** `glmnet(alpha=1)` with cross-validation

**Coming Next:** Elastic Net‚Äîcombining Ridge and LASSO for best of both worlds

---

# Performance Summary Table
```{r final-summary-table, echo=FALSE}
# Create comprehensive summary
summary_table <- data.frame(
  Metric = c("Features Used", "Training RMSE", "Testing RMSE", 
             "Testing R¬≤", "Train-Test Gap", "Interpretability", "Speed"),
  OLS = c("50 (all)", 
          paste0("$", round(train_rmse, 2), "K"),
          paste0("$", round(test_rmse, 2), "K"),
          round(test_r2, 3),
          paste0("$", round(test_rmse - train_rmse, 2), "K"),
          "Low", "Fast"),
  Ridge = c("50 (shrunk)",
            paste0("$", round(ridge_train_rmse, 2), "K"),
            paste0("$", round(ridge_test_rmse, 2), "K"),
            round(ridge_test_r2, 3),
            paste0("$", round(ridge_test_rmse - ridge_train_rmse, 2), "K"),
            "Medium", "Fast"),
  LASSO = c(paste0(sum(feature_status$Selected), " (selected)"),
            paste0("$", round(lasso_train_rmse, 2), "K"),
            paste0("$", round(lasso_test_rmse, 2), "K"),
            round(lasso_test_r2, 3),
            paste0("$", round(lasso_test_rmse - lasso_train_rmse, 2), "K"),
            "High", "Very Fast")
)

summary_table %>%
  kable(caption = "Model Comparison Summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(4, bold = TRUE, background = "#E8F5E9")
```

---

# üéØ CLASSWORK TIME

### Navigate to: `week_1_foundations_and_prediction/lecture_3_classification_methods/classwork_3/`

---

# Classwork 3: LASSO Feature Selection

**Time:** 30-40 minutes

**Dataset:** Same `product_features_extended.csv`

**Your Mission:**

1. Implement LASSO regression with cross-validation (alpha=1)

2. Compare LASSO vs Ridge performance (RMSE, R¬≤, number of features)

3. Identify which features LASSO selected and analyze why

4. Visualize the regularization path showing features dropping to zero

5. Calculate cost savings from reduced feature set

6. Make recommendation: Which regularization approach for deployment?

**Deliverable:** Completed `student_workspace.Rmd` with LASSO analysis and recommendation

**Files provided:** `instructions.md`, `student_template.Rmd`

---

class: center, middle

# Take a 15-minute break

## When you return, we'll explore Elastic Net and complete model selection

---

---
class: center, middle

# Part 4: Elastic Net & Model Selection

---

# The Regularization Spectrum

**We've seen two extremes:**

**Ridge (Œ± = 0):**
- Penalty: Œª‚àëŒ≤¬≤‚±º
- Shrinks all coefficients proportionally
- Retains all features
- Handles multicollinearity well

**LASSO (Œ± = 1):**
- Penalty: Œª‚àë|Œ≤‚±º|
- Drives coefficients to exactly zero
- Automatic feature selection
- Struggles with correlated features

**The question:** Can we get the best of both?

---

# Elastic Net: Hybrid Regularization

**Elastic Net combines L1 and L2 penalties:**

minimize: ¬Ωn ‚àë(y·µ¢ - Œ≤‚ÇÄ - ‚àëŒ≤‚±ºx·µ¢‚±º)¬≤ + Œª[¬Ω(1-Œ±)‚àëŒ≤¬≤‚±º + Œ±‚àë|Œ≤‚±º|]

**Two tuning parameters:**
- **Œª (lambda):** Overall regularization strength (Œª ‚â• 0)
- **Œ± (alpha):** Mixing parameter between L1 and L2 (0 ‚â§ Œ± ‚â§ 1)

**Special cases:**
- Œ± = 0 ‚Üí Pure ridge regression
- Œ± = 1 ‚Üí Pure LASSO
- 0 < Œ± < 1 ‚Üí Elastic net hybrid

---

# Why Elastic Net?

**Motivation from real data challenges:**

1. **Correlated feature groups:** Product features often correlated (size, weight, materials)

2. **LASSO limitation:** With highly correlated features, LASSO arbitrarily selects one and zeros the others

3. **Ridge limitation:** Ridge keeps all features, even clearly irrelevant ones

4. **Elastic net solution:** Ridge component handles correlations, LASSO component performs selection

**Practical benefit:** Stable feature selection even with correlated predictors

---

# The Mixing Parameter Œ±

```{r echo=FALSE, fig.height=4}
# Visualize alpha's effect on coefficient paths
set.seed(42)
n <- 100
x1 <- rnorm(n)
x2 <- x1 + rnorm(n, 0, 0.3)  # Highly correlated with x1
x3 <- rnorm(n)
y <- 2*x1 + 1.5*x2 + 0.5*x3 + rnorm(n, 0, 0.5)

library(glmnet)
X <- cbind(x1, x2, x3)

# Fit models with different alphas
alpha_vals <- c(0, 0.5, 1)
models <- lapply(alpha_vals, function(a) glmnet(X, y, alpha = a))

# Plot coefficient paths
par(mfrow = c(1, 3), mar = c(4, 4, 3, 1))
for(i in 1:3) {
  plot(models[[i]], main = paste0("Œ± = ", alpha_vals[i]), 
       xvar = "lambda", label = TRUE, lwd = 2)
}
```

**Œ± controls the balance between shrinkage and selection**

---

# Elastic Net in R

```{r eval=FALSE}
library(glmnet)

# Prepare data (from your training set)
X <- model.matrix(price ~ . - 1, data = pricing_train)
y <- pricing_train$price

# Fit elastic net with Œ± = 0.5
elastic_net <- glmnet(X, y, alpha = 0.5)

# View coefficient path
plot(elastic_net, xvar = "lambda", label = TRUE)
```

**Alpha = 0.5 balances ridge and LASSO properties**

*Note: This is demonstration code. See Classwork 4 for complete implementation.*

---

# Choosing Alpha with CV

```{r eval=FALSE}
# Try different alpha values
alpha_grid <- seq(0, 1, by = 0.1)

# Store CV results for each alpha
cv_results <- lapply(alpha_grid, function(a) {
  cv.glmnet(X, y, alpha = a, nfolds = 10)
})

# Extract minimum CV error for each alpha
cv_errors <- sapply(cv_results, function(cv) min(cv$cvm))

# Find optimal alpha
optimal_alpha <- alpha_grid[which.min(cv_errors)]

cat("Optimal Œ±:", optimal_alpha, "\n")
cat("CV RMSE:", sqrt(min(cv_errors)), "\n")
```

*Note: See Classwork 4 for hands-on implementation with real data.*

---

# Visualizing Alpha Selection

```{r echo=FALSE, fig.height=4}
# Simulated example for demonstration
alpha_grid_demo <- seq(0, 1, by = 0.1)
set.seed(123)
cv_errors_demo <- 45 + 5 * (alpha_grid_demo - 0.5)^2 + rnorm(length(alpha_grid_demo), 0, 1)
optimal_alpha_demo <- alpha_grid_demo[which.min(cv_errors_demo)]

tibble(
  Alpha = alpha_grid_demo,
  CV_RMSE = cv_errors_demo
) %>%
  ggplot(aes(x = Alpha, y = CV_RMSE)) +
  geom_line(linewidth = 1.2, color = course_colors[1]) +
  geom_point(size = 3, color = course_colors[1]) +
  geom_point(data = . %>% filter(CV_RMSE == min(CV_RMSE)),
             size = 5, color = course_colors[3]) +
  labs(
    title = "Cross-Validation Error Across Alpha Values",
    subtitle = paste0("Optimal Œ± = ", optimal_alpha_demo),
    x = "Alpha (0 = Ridge, 1 = LASSO)",
    y = "CV RMSE"
  ) +
  theme(text = element_text(size = 14))
```

**Data determines optimal balance between L1 and L2**

---

# Fitting Final Elastic Net

```{r eval=FALSE}
# Fit elastic net with optimal alpha
final_elastic <- cv.glmnet(
  X, y, 
  alpha = optimal_alpha,
  nfolds = 10
)

# Extract coefficients at lambda.min
elastic_coefs <- coef(final_elastic, s = "lambda.min")

# Display non-zero coefficients
elastic_coefs[elastic_coefs[,1] != 0, , drop = FALSE]
```

*See template.Rmd for complete working example.*

---

# Elastic Net Coefficient Comparison

```{r echo=FALSE}
# Conceptual comparison for slides (see Classwork 4 for real data)
set.seed(42)
feature_names <- c("weight", "category_electronics", "brand_premium", "competitor_price", 
                   "material_metal", "warranty_months", "customer_rating", "volume",
                   "market_demand", "product_age")

coef_comparison <- tibble(
  Feature = feature_names,
  Ridge = c(0.15, 78, 43, 0.48, 23, 1.9, 14, 0.0025, 0.28, -0.45),
  LASSO = c(0.14, 75, 45, 0.50, 25, 2.0, 15, 0, 0.30, -0.50),
  `Elastic Net` = c(0.145, 76, 44, 0.49, 24, 1.95, 14.5, 0.0018, 0.29, -0.48)
) %>%
  arrange(desc(abs(`Elastic Net`)))

coef_comparison %>%
  head(10) %>%
  kable(digits = 3, caption = "Top 10 Features by Elastic Net Magnitude (Example)") %>%
  kable_styling(bootstrap_options = "striped", font_size = 13)
```

**Elastic net balances shrinkage and selection**

---

# Model Comparison Framework

**Systematic comparison of regularized models:**

1. **Training performance:** R¬≤, RMSE on training set

2. **Cross-validation performance:** CV RMSE, stability across folds

3. **Test set performance:** True generalization error

4. **Feature complexity:** Number of non-zero coefficients

5. **Business interpretability:** Can stakeholders understand it?

6. **Computational efficiency:** Prediction speed for production

---

# Training vs. Test Performance

```{r eval=FALSE}
# Function to calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Prepare test data
X_test <- model.matrix(price ~ . - 1, data = pricing_test)
y_test <- pricing_test$price

# Predictions from each model
ridge_train_pred <- predict(ridge_cv, X, s = "lambda.min")
ridge_test_pred <- predict(ridge_cv, X_test, s = "lambda.min")

lasso_train_pred <- predict(lasso_cv, X, s = "lambda.min")
lasso_test_pred <- predict(lasso_cv, X_test, s = "lambda.min")

elastic_train_pred <- predict(final_elastic, X, s = "lambda.min")
elastic_test_pred <- predict(final_elastic, X_test, s = "lambda.min")
```

*Complete implementation available in Classwork 4 template.*

---

# Performance Comparison Table

```{r echo=FALSE}
# Example comparison metrics (see Classwork 4 for real implementation)
performance <- tibble(
  Model = c("Ridge", "LASSO", "Elastic Net"),
  `Train RMSE` = c(42.3, 40.1, 41.2),
  `Test RMSE` = c(47.5, 46.2, 45.8),
  `CV RMSE` = c(46.8, 45.9, 45.3),
  `# Features` = c(45, 18, 24)
)

performance %>%
  kable(digits = 2, caption = "Model Performance Comparison (Example)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), font_size = 14)
```

**Key insight: Test RMSE is the ultimate arbiter**

---

# Visualizing Model Predictions

```{r echo=FALSE, fig.height=4, fig.width=11}
# Simulated prediction comparison for demonstration
set.seed(42)
n_test <- 100
actual_prices <- runif(n_test, 200, 600)

pred_comparison <- tibble(
  Actual = rep(actual_prices, 3),
  Predicted = c(
    actual_prices + rnorm(n_test, 0, 50),  # Ridge
    actual_prices + rnorm(n_test, 0, 48),  # LASSO
    actual_prices + rnorm(n_test, 0, 46)   # Elastic Net
  ),
  Model = rep(c("Ridge", "LASSO", "Elastic Net"), each = n_test)
)

ggplot(pred_comparison, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_abline(slope = 1, intercept = 0, 
              color = course_colors[4], linewidth = 1, linetype = "dashed") +
  facet_wrap(~ Model, ncol = 3) +
  labs(
    title = "Predicted vs. Actual Prices: Test Set (Example)",
    subtitle = "Points near diagonal indicate accurate predictions",
    x = "Actual Price ($)",
    y = "Predicted Price ($)"
  ) +
  theme(text = element_text(size = 13))
```

---

# Residual Analysis

```{r echo=FALSE, fig.height=4}
# Simulated residuals for demonstration
set.seed(123)
predicted_vals <- seq(200, 600, length.out = 100)
residuals_sim <- rnorm(100, 0, 45)

tibble(
  Predicted = predicted_vals,
  Residuals = residuals_sim
) %>%
  ggplot(aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.6, size = 2, color = course_colors[1]) +
  geom_hline(yintercept = 0, color = course_colors[4], 
             linewidth = 1, linetype = "dashed") +
  geom_smooth(se = FALSE, color = course_colors[3], linewidth = 1) +
  labs(
    title = "Elastic Net Residual Plot: Test Set (Example)",
    subtitle = "Random scatter around zero indicates good fit",
    x = "Predicted Price ($)",
    y = "Residuals ($)"
  ) +
  theme(text = element_text(size = 14))
```

**No obvious patterns = model assumptions satisfied**

---

# Feature Importance: Elastic Net

```{r echo=FALSE, fig.height=4}
# Simulated feature importance for demonstration
feature_importance <- tibble(
  Feature = c("category_electronics", "brand_premium", "weight", "competitor_price",
              "material_metal", "warranty_months", "customer_rating", "volume",
              "market_demand", "product_age", "category_furniture", "free_shipping",
              "material_wood", "return_rate", "seasonal_factor"),
  Coefficient = c(76, 44, 0.145, 0.49, 24, 1.95, 14.5, 0.0018, 0.29, -0.48,
                  120, 10, 15, -100, 20),
  Direction = c(rep("Positive", 13), "Negative", "Positive")
) %>%
  mutate(Abs_Coef = abs(Coefficient)) %>%
  arrange(desc(Abs_Coef)) %>%
  head(15)

ggplot(feature_importance, aes(x = reorder(Feature, Abs_Coef), 
                                y = Abs_Coef, fill = Direction)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  scale_fill_manual(values = course_colors[c(1, 4)]) +
  labs(
    title = "Top 15 Features: Elastic Net Model (Example)",
    subtitle = "Coefficient magnitude indicates importance",
    x = NULL,
    y = "Absolute Coefficient Value"
  ) +
  theme(text = element_text(size = 13))
```

---

# Model Selection Decision Tree

**Choosing between regularization methods:**

```
Is feature selection critical?
‚îú‚îÄ YES ‚Üí Consider LASSO or Elastic Net
‚îÇ   ‚îî‚îÄ Are features highly correlated?
‚îÇ       ‚îú‚îÄ YES ‚Üí Elastic Net (Œ± ‚âà 0.5)
‚îÇ       ‚îî‚îÄ NO ‚Üí LASSO (Œ± = 1)
‚îÇ
‚îî‚îÄ NO ‚Üí Are features correlated?
    ‚îú‚îÄ YES ‚Üí Ridge (Œ± = 0)
    ‚îî‚îÄ NO ‚Üí Standard regression may suffice
```

**In practice:** Test multiple approaches with CV

---

# Overfitting Detection

# Regularization reduces train-test gap

```{r echo=FALSE, fig.height=4}
# Compare train vs test error across models - FIXED
error_comparison <- tibble(
  Model = c("Ridge", "LASSO", "Elastic Net", "Ridge", "LASSO", "Elastic Net"),
  Dataset = c("Train", "Train", "Train", "Test", "Test", "Test"),
  RMSE = c(42.3, 40.1, 41.2, 47.5, 46.2, 45.8)
)

ggplot(error_comparison, aes(x = Model, y = RMSE, fill = Dataset)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = course_colors[c(1, 4)]) +
  labs(
    title = "Train vs. Test RMSE Comparison",
    subtitle = "Large gap indicates overfitting",
    y = "RMSE ($)"
  ) +
  theme(text = element_text(size = 14), legend.position = "bottom")
```



---

# Cross-Validation Stability

# Find the chunk around line 430-464 for CV fold results
# Replace it with this:
```{r echo=FALSE, fig.height=4}
# CV fold stability comparison - FIXED
set.seed(42)
cv_fold_results <- tibble(
  Model = rep(c("Ridge", "LASSO", "Elastic Net"), each = 10),
  Fold = rep(1:10, 3),
  RMSE = c(
    rnorm(10, 46.8, 5),   # Ridge: mean 46.8, sd 5
    rnorm(10, 45.9, 8),   # LASSO: mean 45.9, sd 8
    rnorm(10, 45.3, 6)    # Elastic Net: mean 45.3, sd 6
  )
)

ggplot(cv_fold_results, aes(x = Model, y = RMSE, fill = Model)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = course_colors[1:3]) +
  labs(
    title = "Cross-Validation RMSE Distribution",
    subtitle = "Lower variance indicates more stable predictions",
    y = "RMSE Across 10 Folds ($)"
  ) +
  theme(legend.position = "none", text = element_text(size = 14))
```

---

# Business Interpretation

**Translating model metrics to decisions:**

**Test RMSE = $45:**
- Average pricing error of $45 per product
- For $500 product, ~9% error
- For $100 product, ~45% error

**Feature count = 18 (from 47):**
- 62% dimensionality reduction
- Simpler data collection requirements
- Easier to explain to stakeholders

**Key insight:** Model selection balances accuracy and simplicity

---

# Deployment Considerations

**Before production deployment:**

1. **Retrain on full data:** Combine train + test for final model

2. **Document hyperparameters:** Œ±, Œª, preprocessing steps

3. **Save model artifacts:** Serialized model object, feature list

4. **Establish monitoring:** Track prediction errors over time

5. **Define retraining schedule:** Weekly? Monthly? Event-triggered?

6. **Create fallback logic:** What if model fails?

---

# Model Monitoring Dashboard

**Production metrics to track:**

- **Prediction accuracy:** RMSE, MAE on new data
- **Prediction distribution:** Are predictions reasonable?
- **Feature drift:** Are input distributions changing?
- **Latency:** Prediction response time
- **Error rates:** Model failures, missing predictions
- **Business metrics:** Revenue impact, conversion rates

**Alert thresholds:** When to trigger model retraining

---

# Retraining Triggers

**When to retrain regularized models:**

1. **Performance degradation:** Test RMSE increases by >10%

2. **Data drift:** Input feature distributions shift significantly

3. **New features available:** Additional predictors collected

4. **Business requirements change:** Different accuracy-complexity tradeoff

5. **Seasonal patterns:** Quarterly or annual retraining schedule

6. **Regulatory updates:** Model audit requirements

---

# Model Documentation Template

**Essential documentation for regularized models:**

```
MODEL CARD: Elastic Net Pricing Model v2.3
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Purpose: Predict product prices for dynamic pricing
Last Updated: 2025-10-15
Training Data: 5,247 products (2023-10 to 2025-09)

Hyperparameters:
  - Alpha: 0.5
  - Lambda: 0.023 (CV-selected)
  - Features: 18 selected from 47 candidates

Performance (Test Set):
  - RMSE: $45.23
  - MAE: $32.11
  - R¬≤: 0.87

Feature List: [weight, size, category, ...]
Known Limitations: Struggles with luxury segment (>$1000)
Retraining Schedule: Monthly
Owner: Data Science Team
```

---

# Comparison to Industry Standards

**How do regularized models compare?**

**Traditional retail pricing:**
- Manual rule-based systems
- Cost-plus markup formulas
- Subjective expert judgment

**Modern ML pricing:**
- Regularized regression (what we built)
- Gradient boosting machines (XGBoost)
- Deep neural networks

**Our elastic net model:**
- More sophisticated than cost-plus
- More interpretable than neural networks
- Suitable for mid-size catalogs (1K-100K products)

---

# Scaling Considerations

**From laptop to production:**

**Current setup (small data):**
- glmnet on single machine
- 5,000 products
- Training time: <1 minute
- Memory: <1 GB

**Enterprise scale (big data):**
- Spark MLlib distributed training
- 50 million products
- Training time: 10-30 minutes
- Memory: Distributed across cluster
- **Mathematics identical, infrastructure differs**

---

# Big Data Elastic Net

```{r eval=FALSE}
# Spark MLlib example (conceptual)
library(sparklyr)

# Connect to Spark cluster
sc <- spark_connect(master = "yarn")

# Load distributed data
pricing_spark <- spark_read_csv(sc, "pricing_data", 
                                 path = "hdfs://data/pricing/")

# Fit elastic net on cluster
elastic_spark <- ml_linear_regression(
  pricing_spark,
  formula = price ~ .,
  elastic_net_param = 0.5,
  reg_param = 0.023,
  max_iter = 100
)
```

**Same algorithm, distributed computation**

---

# Model Comparison Checklist

**Before finalizing model selection:**

- ‚òê Compare train, CV, and test RMSE for all candidates
- ‚òê Examine coefficient stability across CV folds
- ‚òê Visualize predictions vs. actuals
- ‚òê Check residual patterns
- ‚òê Count non-zero features (interpretability)
- ‚òê Assess computational requirements
- ‚òê Verify business metric improvement
- ‚òê Document model limitations
- ‚òê Get stakeholder sign-off

---

# Common Pitfalls

**Mistakes to avoid in model selection:**

1. **Choosing based solely on R¬≤:** May select overfit model

2. **Ignoring business constraints:** Most accurate ‚â† most useful

3. **Forgetting computational costs:** Complex models slow in production

4. **Neglecting interpretability:** Can't deploy what you can't explain

5. **One-time training:** Models degrade without retraining

6. **Insufficient documentation:** Future-you won't remember details

---

# Real-World Example: Zillow

**Zillow's "Zestimate" home price model:**

**Challenge:** Predict home values for 100M+ properties

**Solution evolution:**
1. Started with linear regression (2006)
2. Added LASSO for feature selection (2010)
3. Now uses gradient boosting + neural networks (2020+)

**Lesson:** Start simple (regularized regression), add complexity only when necessary

**Current accuracy:** Median error ~2% for on-market homes

---

class: inverse, center, middle

# üéØ CLASSWORK TIME
# Classwork 4: Elastic Net Selection

Open: `week_1_foundations_and_prediction/lecture_3_classification_methods/classwork_4/`

**Your tasks:**

1. Fit elastic net with grid search over Œ± values (0, 0.25, 0.5, 0.75, 1)

2. Use 10-fold CV to select optimal Œ± and Œª

3. Compare elastic net to ridge and LASSO on test set

4. Identify selected features and interpret their importance

5. Create prediction vs. actual visualization

6. Write model selection justification (2-3 paragraphs)

**Template:** `template.Rmd`

---

# Classwork Instructions

**Dataset:** `product_pricing.csv` (extended version with 47 features)

**Key questions to answer:**

1. What Œ± value performs best via cross-validation?

2. How many features does elastic net select?

3. Does elastic net outperform pure ridge or LASSO?

4. Which features have strongest positive/negative effects?

5. Would you recommend this model for production deployment?

**Deliverable:** Complete analysis with model recommendation

---



# Part 5: Comprehensive Regression Diagnostics
---

# Why Diagnostics Matter

**Building a model is not enough.**

**Key questions after model selection:**
- Are our model assumptions satisfied?
- Are predictions reliable across the data range?
- Which observations are influential or problematic?
- How stable are our coefficient estimates?
- What is the uncertainty in our predictions?

**Production requirement:** Models must be validated before deployment to avoid costly errors in business decisions.

---

# Diagnostic Workflow

**Systematic approach to model validation:**

1. **Residual analysis:** Check assumptions about errors
2. **Influence analysis:** Identify high-leverage points
3. **Prediction intervals:** Quantify uncertainty
4. **Coefficient stability:** Assess robustness
5. **Cross-validation diagnostics:** Evaluate generalization
6. **Business validation:** Verify sensible predictions

**Today's focus:** Diagnostic techniques for regularized regression models

---

# Residual Diagnostics: The Foundation

**Residual = Actual - Predicted**

**What residuals tell us:**
- **Random scatter:** Model captures signal, not noise
- **Patterns:** Systematic errors indicate missing features
- **Heteroscedasticity:** Non-constant variance suggests transformations
- **Outliers:** Extreme residuals reveal data quality issues
- **Normality:** Important for prediction intervals

**For regularized models:** Same principles apply, but interpretation differs due to shrinkage.

---

# Types of Residuals

**1. Raw Residuals**
```
e_i = y_i - ≈∑_i
```
Simple but not standardized

**2. Standardized Residuals**
```
r_i = e_i / œÉÃÇ
```
Scaled by estimated error SD

**3. Studentized Residuals**
```
t_i = e_i / (œÉÃÇ_(-i) * ‚àö(1 - h_ii))
```
Accounts for leverage (h_ii)

**For regularized models:** Use cross-validated residuals to avoid overfitting bias

---

# Residual Plots: What to Look For

```{r echo=FALSE, fig.height=4, fig.width=11}
set.seed(42)
n <- 100
x <- seq(1, 10, length.out = n)

# Good: Random scatter
good_resid <- rnorm(n, 0, 1)

# Bad: Non-linearity
bad_nonlinear <- 0.5 * (x - 5)^2 + rnorm(n, 0, 0.5)

# Bad: Heteroscedasticity
bad_hetero <- rnorm(n, 0, x/5)

par(mfrow = c(1, 3))
plot(x, good_resid, main = "‚úì Good: Random Scatter", 
     xlab = "Fitted Values", ylab = "Residuals", pch = 16, col = course_colors[1])
abline(h = 0, col = course_colors[4], lwd = 2, lty = 2)

plot(x, bad_nonlinear, main = "‚úó Bad: Curved Pattern", 
     xlab = "Fitted Values", ylab = "Residuals", pch = 16, col = course_colors[4])
abline(h = 0, col = course_colors[4], lwd = 2, lty = 2)

plot(x, bad_hetero, main = "‚úó Bad: Increasing Variance", 
     xlab = "Fitted Values", ylab = "Residuals", pch = 16, col = course_colors[4])
abline(h = 0, col = course_colors[4], lwd = 2, lty = 2)
```

---

# Implementing Residual Analysis

```{r eval=FALSE}
# Calculate residuals for elastic net
predictions <- predict(final_elastic, X_test, s = "lambda.min")
residuals <- y_test - predictions

# Basic residual plot
plot(predictions, residuals, 
     main = "Residual Plot",
     xlab = "Predicted Price ($)",
     ylab = "Residuals ($)",
     pch = 16, col = rgb(0, 0, 1, 0.5))
abline(h = 0, col = "red", lwd = 2, lty = 2)

# Add smoothed trend line
lines(lowess(predictions, residuals), col = "orange", lwd = 2)
```

**Red flag:** If smoothed line is not flat, model is biased.

---

# Q-Q Plot: Testing Normality

```{r echo=FALSE, fig.height=4}
set.seed(123)
normal_resid <- rnorm(100, 0, 1)
skewed_resid <- rexp(100, 1) - 1

par(mfrow = c(1, 2))
qqnorm(normal_resid, main = "‚úì Normal Residuals", pch = 16, col = course_colors[1])
qqline(normal_resid, col = course_colors[4], lwd = 2)

qqnorm(skewed_resid, main = "‚úó Skewed Residuals", pch = 16, col = course_colors[4])
qqline(skewed_resid, col = course_colors[4], lwd = 2)
```

**Interpretation:**
- Points on line = Normal distribution
- Deviations at tails = Heavy or light tails
- S-curve = Skewness

---

# Scale-Location Plot

**Checks for heteroscedasticity (non-constant variance)**

```{r echo=FALSE, fig.height=4}
set.seed(42)
fitted <- seq(200, 600, length.out = 100)
resid_const <- rnorm(100, 0, 30)
resid_hetero <- rnorm(100, 0, fitted/15)

par(mfrow = c(1, 2))
plot(fitted, sqrt(abs(resid_const)), 
     main = "‚úì Constant Variance",
     xlab = "Fitted Values", ylab = "‚àö|Standardized Residuals|",
     pch = 16, col = course_colors[1])
abline(h = mean(sqrt(abs(resid_const))), col = course_colors[4], lwd = 2, lty = 2)

plot(fitted, sqrt(abs(resid_hetero)), 
     main = "‚úó Heteroscedasticity",
     xlab = "Fitted Values", ylab = "‚àö|Standardized Residuals|",
     pch = 16, col = course_colors[4])
lines(lowess(fitted, sqrt(abs(resid_hetero))), col = course_colors[3], lwd = 2)
```

**If variance increases with fitted values:** Consider log transformation of target variable.

---

# Influence Analysis: Cook's Distance

**Cook's Distance measures influence of each observation:**

```
D_i = (r_i¬≤ / p) * (h_ii / (1 - h_ii))
```

Where:
- r_i = standardized residual
- h_ii = leverage (hat value)
- p = number of parameters

**Rule of thumb:** D_i > 4/n suggests influential point

**For regularized models:** Leverage less critical due to shrinkage, but still check.

---

# Cook's Distance Visualization

```{r echo=FALSE, fig.height=4}
set.seed(42)
n <- 100
cooks_d <- c(rchisq(95, 1)/100, 0.8, 1.2, 0.6, 0.9, 1.5)  # Add some influential points

tibble(
  Observation = 1:n,
  Cooks_Distance = cooks_d
) %>%
  ggplot(aes(x = Observation, y = Cooks_Distance)) +
  geom_segment(aes(xend = Observation, yend = 0), color = course_colors[1]) +
  geom_point(size = 2, color = course_colors[1]) +
  geom_hline(yintercept = 4/n, linetype = "dashed", 
             color = course_colors[4], linewidth = 1) +
  annotate("text", x = 80, y = 4/n + 0.05, 
           label = "Threshold = 4/n", color = course_colors[4]) +
  labs(
    title = "Cook's Distance for Influence Detection",
    subtitle = "Points above threshold may be influential",
    x = "Observation Index",
    y = "Cook's Distance"
  )
```

---

# Leverage vs. Residual Plot

```{r echo=FALSE, fig.height=4}
set.seed(42)
n <- 100
leverage <- rbeta(n, 2, 5)
residuals <- rnorm(n, 0, 1)

# Add some problem points
leverage[c(95, 98)] <- c(0.85, 0.90)  # High leverage
residuals[c(96, 99)] <- c(3.5, -3.2)   # Large residuals
residuals[c(95, 98)] <- c(2.8, -2.5)   # High leverage + large residual

tibble(
  Leverage = leverage,
  Residual = residuals,
  Type = case_when(
    Leverage > 0.6 & abs(Residual) > 2 ~ "High Influence",
    Leverage > 0.6 ~ "High Leverage",
    abs(Residual) > 2.5 ~ "Outlier",
    TRUE ~ "Normal"
  )
) %>%
  ggplot(aes(x = Leverage, y = Residual, color = Type)) +
  geom_point(size = 2.5, alpha = 0.7) +
  scale_color_manual(values = c(course_colors[4], course_colors[3], 
                                 course_colors[1], course_colors[2])) +
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", alpha = 0.5) +
  geom_vline(xintercept = 0.2, linetype = "dashed", alpha = 0.5) +
  labs(
    title = "Leverage vs. Residual Analysis",
    subtitle = "Points in upper right are most concerning",
    x = "Leverage (Hat Value)",
    y = "Standardized Residual"
  )
```

---

# Handling Influential Points

**Options when influential points detected:**

1. **Investigate:** Data entry error? Measurement issue?

2. **Domain validation:** Does value make business sense?

3. **Robust methods:** Use regularization (already doing this!)

4. **Sensitivity analysis:** Refit without point; check stability

5. **Document:** Note in model card for transparency

**Important:** Don't automatically remove outliers‚Äîthey may contain valuable information about edge cases.

---

# Prediction Intervals

**Point predictions are not enough‚Äîwe need uncertainty quantification.**

**Confidence Interval for Mean Response:**
```
Uncertainty about E[Y|X]
```

**Prediction Interval for Individual Response:**
```
Uncertainty about individual Y given X
Wider than CI (includes residual variance)
```

**For regularized models:** Bootstrap or cross-validation for prediction intervals.

---

# Bootstrap Prediction Intervals

```{r eval=FALSE}
# Bootstrap approach for elastic net
n_boot <- 1000
predictions_boot <- matrix(NA, nrow(X_test), n_boot)

for (b in 1:n_boot) {
  # Resample training data
  boot_idx <- sample(1:nrow(X_train), replace = TRUE)
  X_boot <- X_train[boot_idx, ]
  y_boot <- y_train[boot_idx]
  
  # Fit model
  model_boot <- cv.glmnet(X_boot, y_boot, alpha = 0.5)
  
  # Predict on test set
  predictions_boot[, b] <- predict(model_boot, X_test, s = "lambda.min")
}

# Calculate 95% prediction intervals
lower <- apply(predictions_boot, 1, quantile, 0.025)
upper <- apply(predictions_boot, 1, quantile, 0.975)
```

---

# Visualizing Prediction Intervals

```{r echo=FALSE, fig.height=4}
set.seed(42)
n_test <- 50
actual <- seq(200, 600, length.out = n_test)
predicted <- actual + rnorm(n_test, 0, 30)
lower <- predicted - 60
upper <- predicted + 60

tibble(
  Actual = actual,
  Predicted = predicted,
  Lower = lower,
  Upper = upper
) %>%
  arrange(Actual) %>%
  ggplot(aes(x = Actual, y = Predicted)) +
  geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.2, fill = course_colors[1]) +
  geom_point(size = 2, color = course_colors[1]) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", 
              color = course_colors[4], linewidth = 1) +
  labs(
    title = "Predictions with 95% Prediction Intervals",
    subtitle = "Shaded region shows uncertainty range",
    x = "Actual Price ($)",
    y = "Predicted Price ($)"
  )
```

**Coverage check:** ~95% of actual values should fall within intervals.

---

# Cross-Validated Residuals

**Problem with standard residuals:** Optimistically biased (overfit to training data)

**Solution:** Use out-of-fold predictions from cross-validation

```{r eval=FALSE}
# Get CV predictions
cv_predictions <- final_elastic$fit.preval[, 
  which(final_elastic$lambda == final_elastic$lambda.min)]

# Calculate CV residuals
cv_residuals <- y_train - cv_predictions

# These are honest estimates of test set errors
rmse_cv <- sqrt(mean(cv_residuals^2))
mae_cv <- mean(abs(cv_residuals))
```

**Advantage:** More realistic assessment of model performance.

---

# Coefficient Stability Analysis

**Question:** How stable are coefficient estimates?

**Method 1: Bootstrap confidence intervals**

```{r eval=FALSE}
n_boot <- 1000
coef_boot <- matrix(NA, ncol(X_train), n_boot)

for (b in 1:n_boot) {
  boot_idx <- sample(1:nrow(X_train), replace = TRUE)
  model_boot <- cv.glmnet(X_train[boot_idx,], y_train[boot_idx], alpha = 0.5)
  coef_boot[, b] <- as.vector(coef(model_boot, s = "lambda.min"))[-1]
}

# 95% CI for each coefficient
coef_lower <- apply(coef_boot, 1, quantile, 0.025)
coef_upper <- apply(coef_boot, 1, quantile, 0.975)
```

---

# Coefficient Stability Visualization

```{r echo=FALSE, fig.height=4}
set.seed(42)
features <- c("weight", "electronics", "premium", "competitor", "metal", 
              "warranty", "rating", "demand", "age", "furniture")
coef_est <- c(0.15, 76, 44, 0.49, 24, 1.95, 14.5, 0.29, -0.48, 120)
coef_se <- abs(coef_est) * 0.15

tibble(
  Feature = factor(features, levels = features),
  Coefficient = coef_est,
  Lower = coef_est - 1.96 * coef_se,
  Upper = coef_est + 1.96 * coef_se
) %>%
  ggplot(aes(x = Feature, y = Coefficient)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.3, 
                color = course_colors[1], linewidth = 1) +
  geom_point(size = 3, color = course_colors[3]) +
  coord_flip() +
  labs(
    title = "Coefficient Estimates with 95% Confidence Intervals",
    subtitle = "Stable estimates have narrow intervals",
    x = NULL,
    y = "Coefficient Value"
  )
```

**Intervals crossing zero:** Coefficient not significantly different from zero.

---

# Multicollinearity Diagnostics

**Variance Inflation Factor (VIF):**

```
VIF_j = 1 / (1 - R¬≤_j)
```

Where R¬≤_j is from regressing X_j on all other predictors.

**Rule of thumb:**
- VIF < 5: No concern
- VIF 5-10: Moderate correlation
- VIF > 10: High multicollinearity

**For regularized models:** VIF less critical (shrinkage handles collinearity), but still useful for interpretation.

---

# Computing VIF

```{r eval=FALSE}
library(car)

# Convert to data frame (VIF needs standard lm)
data_for_vif <- data.frame(y_train, X_train)

# Fit OLS for VIF calculation
lm_full <- lm(y_train ~ ., data = data_for_vif)

# Calculate VIF
vif_values <- vif(lm_full)

# Display high VIF features
vif_values[vif_values > 5]
```

**High VIF features:** Elastic net will shrink them appropriately, but may want to remove one from correlated pairs for interpretability.

---

# Model Calibration

**Question:** Are predicted probabilities/values well-calibrated?

**For regression:** Plot predicted vs. actual in bins

```{r echo=FALSE, fig.height=4}
set.seed(42)
predicted <- seq(200, 600, length.out = 100)
actual <- predicted + rnorm(100, 0, 40)

# Create bins
bins <- cut(predicted, breaks = 10)
calibration <- tibble(predicted, actual, bins) %>%
  group_by(bins) %>%
  summarise(
    pred_mean = mean(predicted),
    actual_mean = mean(actual),
    .groups = "drop"
  )

ggplot(calibration, aes(x = pred_mean, y = actual_mean)) +
  geom_point(size = 4, color = course_colors[1]) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", 
              color = course_colors[4], linewidth = 1) +
  geom_smooth(method = "lm", se = FALSE, color = course_colors[3]) +
  labs(
    title = "Calibration Plot: Predicted vs. Actual (Binned)",
    subtitle = "Points on diagonal = well calibrated",
    x = "Mean Predicted Price ($)",
    y = "Mean Actual Price ($)"
  )
```

---

# Residual Autocorrelation

**For time-series data:** Check if residuals are correlated over time

```{r eval=FALSE}
# ACF plot for residuals
acf(residuals, main = "Autocorrelation of Residuals")

# Durbin-Watson test
library(lmtest)
dwtest(lm_model)  # For OLS comparison
```

**If autocorrelation present:**
- Temporal features missing
- Consider time-series methods (ARIMA, state space)
- Or: include lagged predictors

**For cross-sectional data:** Usually not a concern.

---

# Heteroscedasticity Tests

**Breusch-Pagan Test:**

```{r eval=FALSE}
library(lmtest)

# Fit OLS for testing
lm_model <- lm(y_train ~ X_train)

# Test for heteroscedasticity
bptest(lm_model)
```

**If heteroscedasticity detected:**
- Transform target (log, sqrt)
- Weighted regression
- Robust standard errors
- Or: accept it (predictions still valid, just CIs are off)

---

# Diagnostic Dashboard

**Key metrics to monitor:**

```{r echo=FALSE}
diagnostic_summary <- tibble(
  Metric = c("RMSE (Train)", "RMSE (CV)", "RMSE (Test)", 
             "MAE (Test)", "R¬≤ (Test)", "Max |Residual|",
             "% Residuals > 2œÉ", "Influential Points (Cook's D > 4/n)"),
  Value = c("41.2", "45.3", "45.8", "34.5", "0.87", "132.5", "4.2%", "3"),
  Status = c("‚úì", "‚úì", "‚úì", "‚úì", "‚úì", "‚ö†", "‚úì", "‚ö†")
)

kable(diagnostic_summary, 
      caption = "Model Diagnostic Summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
  column_spec(3, color = ifelse(diagnostic_summary$Status == "‚úì", "green", "orange"))
```

**Monitor these in production to detect model drift.**

---

# Out-of-Distribution Detection

**Problem:** Model may perform poorly on data unlike training set.

**Solution:** Flag predictions with high uncertainty

```{r eval=FALSE}
# Calculate prediction standard error from bootstrap
pred_se <- apply(predictions_boot, 1, sd)

# Flag high uncertainty predictions
high_uncertainty <- pred_se > quantile(pred_se, 0.95)

# Require human review for these cases
predictions$flag <- high_uncertainty
predictions$confidence <- 1 - (pred_se / max(pred_se))
```

**Business rule:** Predictions with <80% confidence require manual review.

---

# Model Drift Detection

**Production monitoring:** Compare current vs. training distributions

```{r echo=FALSE, fig.height=4}
set.seed(42)
training_prices <- rnorm(200, 400, 80)
current_prices <- rnorm(200, 450, 90)  # Drift in mean and variance

tibble(
  Price = c(training_prices, current_prices),
  Dataset = rep(c("Training", "Current"), each = 200)
) %>%
  ggplot(aes(x = Price, fill = Dataset)) +
  geom_density(alpha = 0.6) +
  scale_fill_manual(values = course_colors[c(1, 4)]) +
  labs(
    title = "Feature Distribution Drift Detection",
    subtitle = "Shift indicates model may need retraining",
    x = "Price ($)",
    y = "Density"
  ) +
  theme(legend.position = "bottom")
```

**Statistical tests:** KS test, PSI (Population Stability Index)

---

# Partial Dependence Plots

**Shows marginal effect of feature on predictions:**

```{r eval=FALSE}
library(pdp)

# For elastic net, need wrapper function
pred_fun <- function(object, newdata) {
  predict(object, as.matrix(newdata), s = "lambda.min")
}

# Create partial dependence plot
pd <- partial(final_elastic, pred.var = "weight", 
              pred.fun = pred_fun, train = data.frame(X_train))

plotPartial(pd)
```

**Useful for:**
- Understanding feature effects
- Detecting non-monotonic relationships
- Communicating model behavior to stakeholders

---

# Diagnostic Checklist

**Before deploying model:**

- ‚òê Residual plots show no patterns
- ‚òê Q-Q plot approximately linear
- ‚òê No severe heteroscedasticity
- ‚òê Influential points investigated
- ‚òê Coefficient estimates stable (bootstrap CIs)
- ‚òê Prediction intervals have correct coverage
- ‚òê Calibration plot near diagonal
- ‚òê No autocorrelation (if time series)
- ‚òê Out-of-distribution detection implemented
- ‚òê Monitoring dashboard created

---

# Common Diagnostic Failures

**Failure Mode 1: Residual patterns**
- **Symptom:** Curved residual plot
- **Cause:** Missing non-linear term or interaction
- **Fix:** Add polynomial terms or interactions

**Failure Mode 2: Large influential points**
- **Symptom:** High Cook's D values
- **Cause:** Data quality issues or true edge cases
- **Fix:** Investigate, possibly robust regression

**Failure Mode 3: Poor calibration**
- **Symptom:** Predicted values systematically too high/low
- **Cause:** Biased training sample
- **Fix:** Recalibration or collect more representative data

---

# Automated Diagnostic Reports

```{r eval=FALSE}
# Create comprehensive diagnostic report
library(broom)
library(ggfortify)

# For OLS comparison (diagnostics easier)
lm_model <- lm(y_train ~ X_train)

# Generate diagnostic plots
autoplot(lm_model, which = 1:4, ncol = 2)

# Extract key statistics
glance(lm_model)
tidy(lm_model, conf.int = TRUE)
```

**For regularized models:** Create custom diagnostic function combining residual analysis, CV metrics, and bootstrap CIs.

---

# Production Model Card

**Document model diagnostics for stakeholders:**

```
MODEL DIAGNOSTICS CARD
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Model: Elastic Net Pricing v2.3
Date: 2025-10-28

PERFORMANCE
- Test RMSE: $45.80 (¬±$3.20)
- R¬≤: 0.87
- MAE: $34.50

DIAGNOSTICS
- Residuals: No systematic patterns ‚úì
- Normality: Approximately normal ‚úì
- Heteroscedasticity: Breusch-Pagan p=0.23 ‚úì
- Influential points: 3 observations flagged ‚ö†

LIMITATIONS
- Less accurate for luxury segment (>$800)
- Prediction intervals: ¬±$90 (95% coverage)
- Requires retraining if market shifts

MONITORING
- Weekly performance review
- Retrain if RMSE > $55
```

---
class: inverse, center, middle

# üéØ CLASSWORK TIME

## Comprehensive Model Diagnostics

**Duration:** 25 minutes

---

# Classwork 5: Model Diagnostics

Open: `classwork_5_diagnostics/`

**Your tasks:**

1. Perform residual analysis on your elastic net model

2. Create Q-Q plot and scale-location plot

3. Identify influential points using Cook's distance

4. Calculate bootstrap prediction intervals

5. Assess coefficient stability

6. Create diagnostic summary report

**Template:** `template.Rmd`

---

# Classwork Instructions

**Dataset:** Use your trained elastic net model from Classwork 4

**Key questions:**

1. Are model assumptions satisfied?

2. Are there influential observations?

3. How wide are prediction intervals?

4. Which coefficients are stable vs. unstable?

5. Is the model ready for deployment?

**Deliverable:** Comprehensive diagnostic report with go/no-go recommendation

---

# Part 6: Model Comparison & Selection

---

# Beyond Single Models

**We've built several models. Now what?**

**Models we have:**
- Linear regression (baseline)
- Ridge regression
- LASSO regression
- Elastic net

**Key questions:**
1. Which model is truly best?
2. How much better is it?
3. Is the improvement worth the complexity?
4. What are the trade-offs?

**Today:** Systematic frameworks for comparing and selecting models.

---

# The Model Selection Problem

**Two competing principles:**

**Occam's Razor:** Prefer simpler models when performance is similar

**Flexibility:** More complex models capture more patterns

**The challenge:** Balance these principles using:
- Statistical tests
- Information criteria
- Cross-validation
- Business requirements

---

# Model Comparison Framework

**Systematic approach:**

1. **Define metrics:** What constitutes "better"?
2. **Ensure fairness:** Same train/test splits
3. **Test significance:** Is difference real or chance?
4. **Consider costs:** Complexity, interpretability, speed
5. **Validate decision:** Out-of-sample performance
6. **Document rationale:** Why was this model chosen?

---

# Comparison Metrics

**Performance metrics:**

**Regression:**
- RMSE: Penalizes large errors heavily
- MAE: Robust to outliers
- R¬≤: Proportion of variance explained
- MAPE: Percentage error (interpretable)

**Model complexity:**
- Number of parameters
- Training time
- Prediction latency
- Memory footprint

**Business metrics:**
- Dollar cost of errors
- User satisfaction
- Regulatory compliance

---

# Example: Model Comparison Table

```{r echo=FALSE}
# Create comprehensive comparison
model_comparison <- tibble(
  Model = c("Linear", "Ridge", "LASSO", "Elastic Net"),
  `Train RMSE` = c(38.5, 42.3, 40.1, 41.2),
  `CV RMSE` = c(52.1, 46.8, 45.9, 45.3),
  `Test RMSE` = c(54.3, 47.5, 46.2, 45.8),
  `R¬≤` = c(0.82, 0.86, 0.87, 0.88),
  `Features` = c(45, 45, 18, 24),
  `Train Time (s)` = c(0.01, 0.15, 0.18, 0.20)
)

kable(model_comparison, digits = 2,
      caption = "Model Performance Comparison (Example)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(4, bold = TRUE, background = "#E8F4F8")
```

**Elastic net wins:** Best test RMSE, good feature selection, acceptable complexity.

---

# Statistical Significance Testing

**Question:** Is the difference between models statistically significant?

**Null hypothesis:** Models have equal performance

**Approaches:**
1. Paired t-test on CV fold errors
2. McNemar's test (classification)
3. Wilcoxon signed-rank test (non-parametric)
4. Permutation tests

**Caution:** Statistical significance ‚â† practical significance

---

# Paired t-Test for Model Comparison

```{r eval=FALSE}
# Compare two models using CV fold errors
model1_cv_rmse <- cv_model1$resample$RMSE
model2_cv_rmse <- cv_model2$resample$RMSE

# Paired t-test (same folds for both models)
t_test_result <- t.test(model1_cv_rmse, model2_cv_rmse, 
                        paired = TRUE)

print(t_test_result)

# Is difference significant?
if (t_test_result$p.value < 0.05) {
  cat("Models are significantly different\n")
} else {
  cat("No significant difference\n")
}
```

---

# Visualizing Model Differences

```{r echo=FALSE, fig.height=4}
set.seed(42)
# Simulate CV fold results for models
n_folds <- 10
cv_comparison <- tibble(
  Fold = rep(1:n_folds, 4),
  Model = rep(c("Linear", "Ridge", "LASSO", "Elastic Net"), each = n_folds),
  RMSE = c(
    rnorm(n_folds, 52, 4),  # Linear
    rnorm(n_folds, 47, 3),  # Ridge
    rnorm(n_folds, 46, 3),  # LASSO
    rnorm(n_folds, 45, 2.5) # Elastic Net
  )
)

ggplot(cv_comparison, aes(x = Model, y = RMSE, fill = Model)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = course_colors[1:4]) +
  labs(
    title = "Cross-Validation RMSE Distribution Across Models",
    subtitle = "Lower and tighter boxes indicate better, more stable performance",
    y = "RMSE ($)"
  ) +
  theme(legend.position = "none")
```

**Elastic net:** Lowest median and least variance.

---

# Information Criteria

**Balancing fit and complexity:**

**Akaike Information Criterion (AIC):**
```
AIC = -2 log(L) + 2k
```

**Bayesian Information Criterion (BIC):**
```
BIC = -2 log(L) + k log(n)
```

Where:
- L = likelihood
- k = number of parameters
- n = sample size

**Lower is better.** BIC penalizes complexity more heavily than AIC.

---

# Computing AIC/BIC

```{r eval=FALSE}
# For standard linear models
lm_model <- lm(y ~ X)

AIC(lm_model)
BIC(lm_model)

# For regularized models (approximate)
# Use effective degrees of freedom
compute_aic_glmnet <- function(model, X, y, lambda) {
  predictions <- predict(model, X, s = lambda)
  residuals <- y - predictions
  
  # RSS
  rss <- sum(residuals^2)
  
  # Effective degrees of freedom
  df <- sum(coef(model, s = lambda) != 0)
  
  # AIC
  n <- length(y)
  aic <- n * log(rss/n) + 2 * df
  
  return(list(AIC = aic, df = df))
}
```

---

# AIC/BIC Comparison

```{r echo=FALSE}
# Create AIC/BIC comparison table
aic_bic <- tibble(
  Model = c("Linear", "Ridge", "LASSO", "Elastic Net"),
  `Log-Likelihood` = c(-1420, -1380, -1375, -1372),
  Parameters = c(45, 45, 18, 24),
  AIC = c(2930, 2850, 2786, 2792),
  BIC = c(3095, 3015, 2886, 2912)
)

kable(aic_bic, digits = 0,
      caption = "Information Criteria Comparison (Example)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(3, bold = TRUE, background = "#E8F4F8")
```

**LASSO wins on both AIC and BIC** due to strong feature selection.

---

# Nested Model Testing

**For nested models (one is subset of another):**

**F-test:**
```
F = [(RSS‚ÇÅ - RSS‚ÇÇ) / (df‚ÇÅ - df‚ÇÇ)] / [RSS‚ÇÇ / df‚ÇÇ]
```

**Example:** Testing if adding features improves model

```{r eval=FALSE}
# Fit nested models
model_simple <- lm(price ~ weight + category)
model_complex <- lm(price ~ weight + category + brand + material)

# F-test
anova(model_simple, model_complex)
```

**If p < 0.05:** Complex model significantly better.

---

# Model Complexity vs. Performance

```{r echo=FALSE, fig.height=4}
# Create complexity-performance tradeoff plot
complexity_data <- tibble(
  Complexity = c(5, 15, 25, 35, 45),
  Train_Error = c(65, 42, 38, 35, 32),
  CV_Error = c(68, 48, 45, 47, 52),
  Test_Error = c(70, 50, 46, 48, 54)
) %>%
  pivot_longer(cols = c(Train_Error, CV_Error, Test_Error),
               names_to = "Dataset", values_to = "RMSE")

ggplot(complexity_data, aes(x = Complexity, y = RMSE, color = Dataset, linetype = Dataset)) +
  geom_line(linewidth = 1.2) +
  geom_point(size = 3) +
  scale_color_manual(values = course_colors[c(1, 3, 4)],
                     labels = c("CV", "Test", "Train")) +
  scale_linetype_manual(values = c("solid", "solid", "dashed"),
                        labels = c("CV", "Test", "Train")) +
  geom_vline(xintercept = 25, linetype = "dashed", alpha = 0.5) +
  annotate("text", x = 30, y = 65, label = "Optimal\nComplexity", size = 3.5) +
  labs(
    title = "Bias-Variance Tradeoff",
    subtitle = "Test error minimized at moderate complexity",
    x = "Model Complexity (# Parameters)",
    y = "RMSE ($)"
  ) +
  theme(legend.position = "bottom")
```

---

# Cross-Validation for Model Selection

**Most reliable approach:**

```{r eval=FALSE}
# Compare models using same CV folds
set.seed(42)
train_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = TRUE
)

# Fit multiple models
models <- list(
  ridge = train(price ~ ., data = train_data, method = "glmnet",
                trControl = train_control,
                tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 1, 0.1))),
  
  lasso = train(price ~ ., data = train_data, method = "glmnet",
                trControl = train_control,
                tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 1, 0.1))),
  
  elastic = train(price ~ ., data = train_data, method = "glmnet",
                  trControl = train_control,
                  tuneGrid = expand.grid(alpha = 0.5, lambda = seq(0, 1, 0.1)))
)

# Compare results
results <- resamples(models)
summary(results)
```

---

# Visualizing CV Comparison

```{r echo=FALSE, fig.height=4}
# Simulate resamples comparison
set.seed(42)
n_folds <- 10
comparison_data <- tibble(
  Model = rep(c("Ridge", "LASSO", "Elastic Net"), each = n_folds),
  RMSE = c(
    rnorm(n_folds, 47, 3),
    rnorm(n_folds, 46, 3),
    rnorm(n_folds, 45, 2.5)
  ),
  Rsquared = c(
    rnorm(n_folds, 0.86, 0.02),
    rnorm(n_folds, 0.87, 0.02),
    rnorm(n_folds, 0.88, 0.015)
  )
)

ggplot(comparison_data, aes(x = Model, y = RMSE, fill = Model)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.3, size = 2) +
  scale_fill_manual(values = course_colors[1:3]) +
  labs(
    title = "10-Fold CV Comparison",
    subtitle = "Each point is one fold's RMSE",
    y = "RMSE ($)"
  ) +
  theme(legend.position = "none")
```

---

# Effect Size vs. Statistical Significance

**Important distinction:**

**Statistical significance:** p < 0.05 (difference is real)

**Practical significance:** Difference matters for business

```{r echo=FALSE}
comparison <- tibble(
  Comparison = c("LASSO vs. Ridge", "Elastic vs. LASSO", "Elastic vs. Linear"),
  `RMSE Difference` = c("$1.30", "$0.40", "$8.50"),
  `% Improvement` = c("2.7%", "0.9%", "15.6%"),
  `p-value` = c("0.03", "0.45", "<0.001"),
  `Practically Significant?` = c("Maybe", "No", "Yes")
)

kable(comparison,
      caption = "Statistical vs. Practical Significance") %>%
  kable_styling(bootstrap_options = "striped")
```

**Key insight:** 0.9% improvement may be statistically insignificant but could be worth millions in large-scale applications.

---

# Business-Driven Model Selection

**Beyond statistical metrics:**

**Interpretability:**
- Can stakeholders understand it?
- Can decisions be explained?
- Regulatory requirements?

**Operational constraints:**
- Prediction latency requirements
- Available computational resources
- Data collection costs

**Maintenance:**
- How often does model need retraining?
- Monitoring complexity
- Debugging difficulty

---

# Model Selection Decision Tree

```
Start with simplest model (Linear Regression)
‚îÇ
‚îú‚îÄ Test RMSE acceptable? ‚îÄ‚îÄYES‚îÄ‚ñ∫ Done (use Linear)
‚îÇ                          NO
‚îÇ                          ‚Üì
‚îú‚îÄ Try Ridge (handles multicollinearity)
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Significant improvement? ‚îÄ‚îÄNO‚îÄ‚îÄ‚ñ∫ Use Linear (simpler)
‚îÇ  ‚îÇ                              YES
‚îÇ  ‚îÇ                              ‚Üì
‚îú‚îÄ Try LASSO (feature selection)
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Improvement + better interpretability? ‚îÄ‚îÄYES‚îÄ‚ñ∫ Use LASSO
‚îÇ  ‚îÇ                                           NO
‚îÇ  ‚îÇ                                           ‚Üì
‚îú‚îÄ Try Elastic Net (hybrid benefits)
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Best test performance? ‚îÄ‚îÄYES‚îÄ‚ñ∫ Use Elastic Net
‚îÇ  ‚îÇ                          NO
‚îÇ  ‚îÇ                          ‚Üì
‚îú‚îÄ Consider ensemble methods
```

---

# Model Cards for Documentation

**Document selection rationale:**

```
MODEL SELECTION CARD
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Selected Model: Elastic Net (Œ±=0.5)
Date: 2025-10-28
Alternatives Considered: Linear, Ridge, LASSO

PERFORMANCE COMPARISON
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model      ‚îÇ RMSE  ‚îÇ R¬≤    ‚îÇ Feats ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Linear     ‚îÇ 54.30 ‚îÇ 0.82  ‚îÇ 45    ‚îÇ
‚îÇ Ridge      ‚îÇ 47.50 ‚îÇ 0.86  ‚îÇ 45    ‚îÇ
‚îÇ LASSO      ‚îÇ 46.20 ‚îÇ 0.87  ‚îÇ 18    ‚îÇ
‚îÇ Elastic Net‚îÇ 45.80 ‚îÇ 0.88  ‚îÇ 24    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

SELECTION RATIONALE
- Best test RMSE ($45.80 vs $46.20 LASSO)
- Reasonable feature count (24 vs 45 all)
- More stable than LASSO with correlated features
- Acceptable training time (0.20s)
- Meets business requirement (<$50 error)
```

---

# Ensemble Methods: Beyond Single Models

**Combining multiple models often beats any single model.**

**Approaches:**
1. **Bagging:** Average many models trained on bootstrap samples
2. **Boosting:** Sequential models focus on hard cases
3. **Stacking:** Meta-model combines predictions

**Trade-off:** Better performance vs. lost interpretability

```{r eval=FALSE}
# Simple ensemble: Average predictions
ensemble_pred <- (ridge_pred + lasso_pred + elastic_pred) / 3

# Weighted ensemble
ensemble_pred <- 0.3 * ridge_pred + 0.3 * lasso_pred + 0.4 * elastic_pred
```

---

# Model Selection Checklist

**Before finalizing model choice:**

- ‚òê Compared on same train/test splits
- ‚òê Used consistent performance metrics
- ‚òê Tested statistical significance
- ‚òê Considered information criteria (AIC/BIC)
- ‚òê Validated with cross-validation
- ‚òê Checked practical significance
- ‚òê Assessed interpretability needs
- ‚òê Verified operational feasibility
- ‚òê Documented selection rationale
- ‚òê Got stakeholder buy-in

---

# When Simpler is Better

**Cases where simpler model wins despite lower accuracy:**

1. **Explainability critical:** Lending, healthcare, hiring
2. **Limited data for retraining:** Model must be robust
3. **Real-time constraints:** Prediction latency matters
4. **High stakes:** Errors have severe consequences
5. **Regulatory requirements:** Model must be auditable

**Example:** Bank using linear model for loan approval despite elastic net being 2% more accurate‚Äîexplainability required by law.

---

# Model Monitoring After Selection

**Selection doesn't end deployment:**

**Track over time:**
- Prediction accuracy (RMSE, MAE)
- Feature distributions (detect drift)
- Prediction distributions (unusual patterns?)
- Business metrics (revenue, satisfaction)
- Computational performance (latency)

**Trigger retraining when:**
- Test RMSE increases >10%
- Feature distributions shift significantly
- New features become available
- Business requirements change

---

# A/B Testing in Production

**Ultimate model comparison:**

```
Traffic Split:
50% ‚Üí Model A (current champion)
50% ‚Üí Model B (new challenger)

Monitor for 2 weeks:
- Prediction accuracy
- Business KPIs
- User experience
- System performance

Decision:
IF Model B significantly better:
  ‚ñ∫ Promote to champion
ELSE:
  ‚ñ∫ Keep Model A
```

**Key:** Measure business impact, not just statistical metrics.

---

# Multi-Objective Model Selection

**Sometimes multiple objectives conflict:**

**Pareto frontier:** Models where improving one metric worsens another

```{r echo=FALSE, fig.height=4}
# Create Pareto frontier plot
models_pareto <- tibble(
  Model = c("Linear", "Ridge", "LASSO", "Elastic Net", "Complex Ensemble"),
  Accuracy = c(0.82, 0.86, 0.87, 0.88, 0.91),
  Interpretability = c(9, 7, 8, 7, 3),
  Speed = c(10, 7, 7, 6, 2)
)

ggplot(models_pareto, aes(x = Interpretability, y = Accuracy)) +
  geom_point(size = 5, color = course_colors[1]) +
  geom_text(aes(label = Model), vjust = -1, size = 3) +
  labs(
    title = "Accuracy vs. Interpretability Trade-off",
    subtitle = "No single best model‚Äîdepends on priorities",
    x = "Interpretability Score (1-10)",
    y = "R¬≤ Score"
  )
```

---

# Cost-Benefit Analysis

**Quantify model selection impact:**

```{r echo=FALSE}
cost_benefit <- tibble(
  Model = c("Linear", "Ridge", "LASSO", "Elastic Net"),
  `Avg Error ($)` = c(54, 48, 46, 46),
  `Annual Predictions` = c(10000, 10000, 10000, 10000),
  `Error Cost` = c("$540K", "$480K", "$460K", "$460K"),
  `Dev Cost` = c("$5K", "$15K", "$20K", "$25K"),
  `Maint. Cost/yr` = c("$10K", "$15K", "$20K", "$25K"),
  `Net Benefit` = c("$0", "$45K", "$55K", "$50K")
)

kable(cost_benefit,
      caption = "3-Year Cost-Benefit Analysis") %>%
  kable_styling(bootstrap_options = "striped") %>%
  row_spec(3, bold = TRUE, background = "#E8F4F8")
```

**LASSO wins** when factoring in development and maintenance costs.

---

# Real-World Example: Netflix Prize

**Challenge:** Improve movie recommendation RMSE by 10%

**Winner's approach:**
- Ensemble of 100+ models
- Blend of collaborative filtering, matrix factorization, RBMs
- Achieved 10.06% improvement

**Lesson learned:**
- Complex ensemble won competition
- Netflix deployed simpler model in production
- Engineering costs outweighed small accuracy gain

**Key insight:** Competition metric ‚â† production value.

---

# Model Selection Pitfalls

**Common mistakes:**

1. **Training set selection:** Choosing model with best training RMSE
2. **Data leakage:** Information from test set influencing choice
3. **Multiple comparisons:** Testing too many models inflates error
4. **Ignoring variance:** Comparing point estimates without confidence
5. **Overfitting to validation set:** Too many iterations on same CV folds
6. **Metric mismatch:** Optimizing RMSE when MAE matters for business

---

# Addressing Multiple Comparisons

**Problem:** Testing 20 models at Œ±=0.05 gives ~64% chance of false positive

**Solutions:**

**Bonferroni correction:**
```
Œ±_adjusted = Œ± / m
where m = number of comparisons
```

**Holdout final test set:**
- Use CV for model development
- Hold out final test set
- Evaluate only final model on holdout

**Pre-registration:**
- Specify models to compare before seeing data
- Reduces researcher degrees of freedom

---

# Model Versioning and Governance

**Track model evolution:**

```
Model Registry:
‚îú‚îÄ‚îÄ v1.0: Linear Baseline (RMSE: $54)
‚îÇ   ‚îî‚îÄ‚îÄ Deployed: 2024-06-01
‚îú‚îÄ‚îÄ v1.1: Ridge (RMSE: $48)
‚îÇ   ‚îî‚îÄ‚îÄ Deployed: 2024-08-15
‚îú‚îÄ‚îÄ v2.0: LASSO (RMSE: $46)
‚îÇ   ‚îî‚îÄ‚îÄ Deployed: 2024-10-01
‚îî‚îÄ‚îÄ v2.1: Elastic Net (RMSE: $45.8)
    ‚îî‚îÄ‚îÄ Candidate (Testing)

Decision Log:
- v1.1: 12% improvement justified switch
- v2.0: Feature reduction + accuracy gain
- v2.1: Under evaluation, 0.4% gain
```

**Benefits:** Reproducibility, rollback capability, audit trail.

---

# Final Model Selection Framework

**Step 1:** Define success metrics (business + statistical)

**Step 2:** Train candidate models with consistent methodology

**Step 3:** Compare using cross-validation

**Step 4:** Test statistical significance of differences

**Step 5:** Evaluate practical significance

**Step 6:** Consider non-performance factors (cost, interpretability)

**Step 7:** Validate on holdout test set

**Step 8:** Document selection rationale

**Step 9:** Monitor in production

**Step 10:** Iterate based on feedback

---
class: inverse, center, middle

# üéØ CLASSWORK TIME

## Model Selection Practice

**Duration:** 25 minutes

---

# Classwork 6: Systematic Model Comparison

Open: `classwork_6_model_selection/`

**Your tasks:**

1. Train 4 models: Linear, Ridge, LASSO, Elastic Net

2. Compare using consistent CV folds

3. Perform statistical significance testing

4. Calculate AIC/BIC for each model

5. Create comprehensive comparison table

6. Make final model recommendation with justification

**Template:** `template.Rmd`

---

# Classwork Instructions

**Dataset:** Continue with product pricing data

**Key deliverables:**

1. Side-by-side performance comparison
2. Statistical tests (paired t-test)
3. Visualization of CV results
4. Cost-benefit analysis
5. Model selection justification (3-4 paragraphs)

**Goal:** Select and justify the best model for production deployment.

---

# Part 7: Practical Deployment Considerations

---

# From Laptop to Production

**Models that work on your laptop may fail in production.**

**Today's reality:**
- Trained on 800 products
- Runs in seconds
- Perfect test set available
- Single user (you)

**Production reality:**
- Predict for 50,000 products
- Must respond in <100ms
- Real-time data with quality issues
- Thousands of concurrent users

**Gap:** Technical, operational, and organizational challenges.

---

# The Deployment Pipeline

**End-to-end workflow:**

1. **Model Training:** What we've done so far
2. **Model Serialization:** Save trained model
3. **API Development:** Expose model as service
4. **Infrastructure:** Deploy to servers/cloud
5. **Monitoring:** Track performance in real-time
6. **Maintenance:** Retrain and update
7. **Governance:** Document and audit

**Each step has unique challenges.**

---

# Serializing Models in R

**Save trained model for reuse:**
```{r eval=FALSE}
# Save glmnet model
saveRDS(elastic_model, "models/elastic_net_v1.0.rds")

# Save metadata
model_metadata <- list(
  model_type = "elastic_net",
  version = "1.0",
  train_date = Sys.Date(),
  alpha = 0.5,
  lambda = 0.023,
  features = colnames(X_train),
  performance = list(
    train_rmse = 41.2,
    test_rmse = 45.8,
    r2 = 0.88
  )
)

saveRDS(model_metadata, "models/elastic_net_v1.0_metadata.rds")
```

---

# Loading and Using Saved Models

**Consistent prediction pipeline:**
```{r eval=FALSE}
# Load model
elastic_model <- readRDS("models/elastic_net_v1.0.rds")
metadata <- readRDS("models/elastic_net_v1.0_metadata.rds")

# New data preprocessing
new_data <- read.csv("new_products.csv")

# CRITICAL: Same preprocessing as training
X_new <- model.matrix(price ~ . - 1, data = new_data)

# Ensure same features in same order
X_new <- X_new[, metadata$features]

# Predict
predictions <- predict(elastic_model, X_new, s = metadata$lambda)
```

**Common error:** Feature mismatch between train and production.

---

# Feature Engineering Pipeline

**Preprocessing must be reproducible:**
```{r eval=FALSE}
# BAD: Hardcoded values (change over time)
data$price_normalized <- data$price / 400

# GOOD: Store normalization parameters
normalization_params <- list(
  price_mean = mean(train$price),
  price_sd = sd(train$price)
)

# Apply consistently
normalize_price <- function(price, params) {
  (price - params$price_mean) / params$price_sd
}

# Save with model
saveRDS(normalization_params, "models/preprocessing_params.rds")
```

---

# Handling Missing Data in Production

**Training vs. production data:**

**Training:** Can drop rows with missing values  
**Production:** Must handle every incoming request
```{r eval=FALSE}
# Define imputation strategy
imputation_rules <- list(
  weight = median(train$weight),
  warranty_months = 0,
  customer_rating = mean(train$customer_rating),
  competitor_price = mean(train$competitor_price)
)

# Apply in production
impute_missing <- function(data, rules) {
  for (col in names(rules)) {
    if (col %in% names(data)) {
      data[[col]][is.na(data[[col]])] <- rules[[col]]
    }
  }
  return(data)
}

# Save rules with model
saveRDS(imputation_rules, "models/imputation_rules.rds")
```

---

# Prediction Latency Requirements

**Response time matters:**

**E-commerce pricing:**
- Target: <100ms per prediction
- User waiting for page load
- Lost sales if too slow

**Batch predictions:**
- Target: 10,000 predictions/hour
- Overnight processing acceptable
- Throughput over latency

**Strategy:**
- Profile code for bottlenecks
- Cache frequent predictions
- Use vectorized operations

---

# Optimizing Prediction Speed
```{r eval=FALSE}
# SLOW: Loop through predictions
predictions <- numeric(nrow(X_new))
for (i in 1:nrow(X_new)) {
  predictions[i] <- predict(model, X_new[i,], s = "lambda.min")
}

# FAST: Vectorized prediction
predictions <- predict(model, X_new, s = "lambda.min")

# Benchmark
library(microbenchmark)

microbenchmark(
  loop = {
    for (i in 1:100) predict(model, X_new[i,], s = "lambda.min")
  },
  vectorized = {
    predict(model, X_new[1:100,], s = "lambda.min")
  },
  times = 10
)
```

**Result:** Vectorized ~100x faster.

---

# Building a Prediction API

**Expose model via REST API:**
```{r eval=FALSE}
# Using plumber package
library(plumber)

#* @apiTitle Product Pricing API
#* @apiDescription Predict product prices using elastic net model

# Load model at startup
model <- readRDS("models/elastic_net_v1.0.rds")
metadata <- readRDS("models/elastic_net_v1.0_metadata.rds")

#* Predict product price
#* @param weight Product weight in grams
#* @param category Product category
#* @param brand Brand tier (premium/midrange/budget)
#* @post /predict
function(weight, category, brand) {
  # Create feature vector
  features <- process_input(weight, category, brand)
  
  # Predict
  prediction <- predict(model, features, s = metadata$lambda)
  
  # Return result
  list(
    predicted_price = round(as.numeric(prediction), 2),
    model_version = metadata$version,
    timestamp = Sys.time()
  )
}
```

---

# API Error Handling

**Graceful failure in production:**
```{r eval=FALSE}
#* Predict with error handling
#* @post /predict
function(req, res, weight, category, brand) {
  tryCatch({
    # Validate inputs
    if (is.null(weight) || is.na(as.numeric(weight))) {
      res$status <- 400
      return(list(error = "Invalid weight parameter"))
    }
    
    # Predict
    features <- process_input(weight, category, brand)
    prediction <- predict(model, features, s = metadata$lambda)
    
    # Check for anomalies
    if (prediction < 0 || prediction > 1000) {
      return(list(
        predicted_price = as.numeric(prediction),
        warning = "Prediction outside typical range",
        confidence = "low"
      ))
    }
    
    return(list(predicted_price = round(as.numeric(prediction), 2)))
    
  }, error = function(e) {
    res$status <- 500
    return(list(error = "Internal prediction error", 
                message = e$message))
  })
}
```

---

# Monitoring Dashboard Metrics

**Track these in real-time:**
```{r echo=FALSE}
monitoring_metrics <- tibble(
  Category = c("Performance", "Performance", "Performance",
               "Data Quality", "Data Quality", "Data Quality",
               "System Health", "System Health", "System Health"),
  Metric = c("Prediction RMSE", "Response Time (p95)", "Throughput (req/s)",
             "Missing Features (%)", "Out-of-Range Values (%)", "Feature Drift (KS stat)",
             "Error Rate (%)", "CPU Usage (%)", "Memory Usage (GB)"),
  Target = c("<$50", "<100ms", ">100",
             "<5%", "<2%", "<0.1",
             "<1%", "<70%", "<4GB"),
  Alert_Threshold = c(">$60", ">200ms", "<50",
                      ">10%", ">5%", ">0.2",
                      ">5%", ">85%", ">6GB")
)

kable(monitoring_metrics, 
      caption = "Production Monitoring Dashboard") %>%
  kable_styling(bootstrap_options = "striped", font_size = 11)
```

---

# Detecting Model Drift

**Performance degrades over time:**
```{r echo=FALSE, fig.height=4}
# Simulate model drift over time
set.seed(42)
weeks <- 1:24
baseline_rmse <- 45
drift <- cumsum(rnorm(24, 0.3, 0.5))
rmse_over_time <- baseline_rmse + drift

tibble(
  Week = weeks,
  RMSE = rmse_over_time
) %>%
  ggplot(aes(x = Week, y = RMSE)) +
  geom_line(linewidth = 1.2, color = course_colors[1]) +
  geom_point(size = 2, color = course_colors[1]) +
  geom_hline(yintercept = baseline_rmse, linetype = "dashed", 
             color = course_colors[4], linewidth = 1) +
  geom_hline(yintercept = baseline_rmse * 1.1, linetype = "dashed",
             color = course_colors[4], linewidth = 1, alpha = 0.5) +
  annotate("text", x = 20, y = baseline_rmse + 1, 
           label = "Baseline", color = course_colors[4]) +
  annotate("text", x = 20, y = baseline_rmse * 1.1 + 1, 
           label = "+10% Alert", color = course_colors[4]) +
  labs(
    title = "Model Performance Drift Over Time",
    subtitle = "Gradual degradation triggers retraining",
    x = "Weeks in Production",
    y = "RMSE ($)"
  )
```

---

# Statistical Tests for Drift

**Detect distribution changes:**
```{r eval=FALSE}
# Kolmogorov-Smirnov test for feature drift
library(stats)

# Compare current week to training distribution
ks_test_weight <- ks.test(
  current_week_data$weight,
  training_data$weight
)

# Alert if significant drift
if (ks_test_weight$p.value < 0.05) {
  alert("Weight distribution has drifted significantly")
  cat("KS statistic:", ks_test_weight$statistic, "\n")
  cat("p-value:", ks_test_weight$p.value, "\n")
}

# Population Stability Index (PSI)
calculate_psi <- function(expected, actual, bins = 10) {
  # Bin the data
  breaks <- quantile(expected, probs = seq(0, 1, length.out = bins + 1))
  expected_counts <- table(cut(expected, breaks, include.lowest = TRUE))
  actual_counts <- table(cut(actual, breaks, include.lowest = TRUE))
  
  # Calculate PSI
  expected_pct <- expected_counts / sum(expected_counts)
  actual_pct <- actual_counts / sum(actual_counts)
  
  psi <- sum((actual_pct - expected_pct) * log(actual_pct / expected_pct))
  return(psi)
}
```

---

# Retraining Triggers

**When to retrain model:**

**Automatic triggers:**
- Test RMSE increases >10%
- Feature drift PSI >0.2
- Error rate >5% for consecutive 3 days
- New features become available

**Scheduled retraining:**
- Monthly: For fast-changing markets
- Quarterly: For stable domains
- Annually: For slow-moving products

**Event-driven:**
- Major product line changes
- Market disruption (pandemic, recession)
- Regulatory changes
- Competitive landscape shifts

---

# Automated Retraining Pipeline
```{r eval=FALSE}
# Retraining workflow
retrain_model <- function(trigger_reason) {
  
  # Log retraining event
  log_event(paste("Retraining triggered by:", trigger_reason))
  
  # Fetch fresh data
  new_data <- fetch_production_data(last_n_months = 3)
  
  # Train new model
  new_model <- train_elastic_net(new_data)
  
  # Evaluate on holdout
  performance <- evaluate_model(new_model, holdout_set)
  
  # Compare to current champion
  if (performance$rmse < current_model_rmse * 0.95) {
    # New model is >5% better
    deploy_model(new_model, version = increment_version())
    log_event("New model deployed - significant improvement")
  } else {
    log_event("New model not better - keeping champion")
  }
}
```

---

# A/B Testing Framework

**Safely deploy new models:**
```{r eval=FALSE}
# Route traffic based on experiment
predict_with_ab_test <- function(request) {
  
  # Assign to experiment group
  group <- assign_experiment_group(request$user_id)
  
  if (group == "control") {
    # Current champion model
    prediction <- predict(champion_model, request$features)
  } else if (group == "treatment") {
    # New challenger model
    prediction <- predict(challenger_model, request$features)
  }
  
  # Log for analysis
  log_prediction(
    user_id = request$user_id,
    group = group,
    prediction = prediction,
    timestamp = Sys.time()
  )
  
  return(prediction)
}
```

---

# A/B Test Analysis
```{r echo=FALSE, fig.height=4}
# Simulate A/B test results
set.seed(42)
n_users <- 1000

ab_results <- tibble(
  Group = rep(c("Control", "Treatment"), each = n_users),
  RMSE = c(
    rnorm(n_users, 46.5, 8),
    rnorm(n_users, 45.2, 7.5)
  )
)

ggplot(ab_results, aes(x = Group, y = RMSE, fill = Group)) +
  geom_violin(alpha = 0.7) +
  geom_boxplot(width = 0.2, alpha = 0.5) +
  scale_fill_manual(values = course_colors[c(4, 1)]) +
  labs(
    title = "A/B Test Results: Model Performance",
    subtitle = "Treatment model shows lower error",
    y = "Prediction Error ($)"
  ) +
  theme(legend.position = "none")
```

**Statistical test:** t-test shows p < 0.001, treatment wins.

---

# Version Control for Models

**Track everything:**
```
models/
‚îú‚îÄ‚îÄ v1.0/
‚îÇ   ‚îú‚îÄ‚îÄ elastic_net.rds
‚îÇ   ‚îú‚îÄ‚îÄ metadata.rds
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.rds
‚îÇ   ‚îú‚îÄ‚îÄ training_data_hash.txt
‚îÇ   ‚îî‚îÄ‚îÄ performance_report.html
‚îú‚îÄ‚îÄ v1.1/
‚îÇ   ‚îú‚îÄ‚îÄ elastic_net.rds
‚îÇ   ‚îú‚îÄ‚îÄ metadata.rds
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ v2.0/
    ‚îú‚îÄ‚îÄ elastic_net.rds
    ‚îî‚îÄ‚îÄ ...

CHANGELOG.md:
v2.0 (2025-11-15):
  - Retrained with 3 months new data
  - Added 5 new features
  - RMSE: 43.2 (was 45.8)
  - Deployed to production
  
v1.1 (2025-10-28):
  - Hotfix for missing value handling
  - No model changes
```

---

# Rollback Strategy

**What if new model fails?**
```{r eval=FALSE}
# Instant rollback capability
rollback_to_previous_version <- function(reason) {
  
  # Log incident
  log_critical(paste("Rolling back model:", reason))
  
  # Get previous version
  previous_version <- get_previous_version()
  
  # Load previous model
  champion_model <- readRDS(
    paste0("models/v", previous_version, "/elastic_net.rds")
  )
  
  # Update routing
  update_production_model(champion_model)
  
  # Notify team
  send_alert(
    "Model rolled back",
    details = reason,
    severity = "high"
  )
  
  # Post-incident review
  schedule_postmortem()
}
```

**Target:** Rollback in <5 minutes.

---

# Data Quality Checks

**Validate inputs before prediction:**
```{r eval=FALSE}
validate_prediction_input <- function(data) {
  
  issues <- list()
  
  # Check required fields
  required <- c("weight", "category", "brand")
  missing <- setdiff(required, names(data))
  if (length(missing) > 0) {
    issues <- append(issues, paste("Missing fields:", paste(missing, collapse = ", ")))
  }
  
  # Check ranges
  if (!is.na(data$weight) && (data$weight < 0 || data$weight > 10000)) {
    issues <- append(issues, "Weight out of range (0-10000g)")
  }
  
  # Check categorical values
  valid_categories <- c("electronics", "furniture", "kitchen", "sports", "toys", "home")
  if (!(data$category %in% valid_categories)) {
    issues <- append(issues, "Invalid category")
  }
  
  return(list(valid = length(issues) == 0, issues = issues))
}
```

---

# Handling Edge Cases

**Out-of-distribution inputs:**
```{r eval=FALSE}
predict_with_confidence <- function(model, data) {
  
  # Predict
  prediction <- predict(model, data, s = "lambda.min")
  
  # Check if input is similar to training data
  distance_to_train <- mahalanobis_distance(data, training_center, training_cov)
  
  # Assign confidence
  if (distance_to_train < threshold_normal) {
    confidence <- "high"
  } else if (distance_to_train < threshold_unusual) {
    confidence <- "medium"
    warning <- "Input differs from training data"
  } else {
    confidence <- "low"
    warning <- "Out-of-distribution input - manual review recommended"
  }
  
  return(list(
    prediction = prediction,
    confidence = confidence,
    warning = warning
  ))
}
```

---

# Logging for Debugging

**Essential for troubleshooting:**
```{r eval=FALSE}
log_prediction <- function(request_id, input, output, metadata) {
  
  log_entry <- list(
    timestamp = Sys.time(),
    request_id = request_id,
    model_version = metadata$version,
    input_features = input,
    prediction = output$prediction,
    confidence = output$confidence,
    processing_time_ms = output$processing_time,
    user_agent = metadata$user_agent
  )
  
  # Write to structured log
  write_json(log_entry, file = "logs/predictions.jsonl", append = TRUE)
  
  # Also write to monitoring system
  send_to_monitoring(log_entry)
}
```

**Benefit:** Can replay any prediction and debug issues.

---

# Cost Optimization

**Cloud deployment costs:**
```{r echo=FALSE}
cost_analysis <- tibble(
  Component = c("Compute (API servers)", "Storage (models & logs)", 
                "Database (predictions)", "Monitoring", "Data transfer"),
  `Monthly Cost` = c("$250", "$50", "$100", "$75", "$25"),
  `Cost per 1M predictions` = c("$2.50", "$0.50", "$1.00", "$0.75", "$0.25"),
  `Optimization Strategy` = c(
    "Auto-scaling, spot instances",
    "Compress old logs, S3 lifecycle",
    "Archive predictions >6 months",
    "Sample non-critical metrics",
    "Cache frequent requests"
  )
)

kable(cost_analysis,
      caption = "Production Cost Analysis") %>%
  kable_styling(bootstrap_options = "striped", font_size = 10)
```

**Total:** ~$500/month for 100K predictions/day

---

# Scaling Strategies

**From hundreds to millions of predictions:**

**Vertical scaling:**
- Larger server (more CPU/RAM)
- Limit: Single machine capacity

**Horizontal scaling:**
- Multiple servers behind load balancer
- Linear scaling with servers
- Requires stateless predictions

**Caching:**
- Store common predictions
- 80/20 rule: Cache 20% of inputs ‚Üí 80% of requests
- Redis/Memcached

**Batch processing:**
- Precompute predictions overnight
- Serve from database
- Works for stable catalogs

---

# Infrastructure as Code

**Reproducible deployments:**
```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    image: pricing-model:v2.0
    ports:
      - "8000:8000"
    environment:
      - MODEL_VERSION=2.0
      - LOG_LEVEL=INFO
    volumes:
      - ./models:/app/models:ro
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 4G
  
  monitoring:
    image: prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
```

---

# Security Considerations

**Protect model and data:**

**API Security:**
- Authentication (API keys, OAuth)
- Rate limiting (prevent abuse)
- Input validation (prevent injection)
- HTTPS only (encrypt in transit)

**Model Security:**
- Don't expose raw model files
- Watermark predictions (detect theft)
- Monitor unusual query patterns
- Audit access logs

**Data Privacy:**
- Anonymize logs (remove PII)
- Comply with GDPR/CCPA
- Secure data storage
- Retention policies

---

# Compliance and Governance

**Regulatory requirements:**

**Model Documentation:**
- Training data sources
- Feature definitions
- Performance metrics
- Known limitations
- Bias testing results

**Audit Trail:**
- Who trained the model?
- When was it deployed?
- What data was used?
- How is it monitored?
- Who can make changes?

**Explainability:**
- Can predictions be explained?
- Feature importance documented?
- Edge cases identified?

---

# Incident Response Plan

**When things go wrong:**

**Severity Levels:**
- **P0 (Critical):** Model down, revenue impact
- **P1 (High):** Degraded performance, user complaints
- **P2 (Medium):** Minor issues, monitoring alerts
- **P3 (Low):** Logging issues, non-critical

**Response:**
1. Detect (monitoring alerts)
2. Assess severity
3. Rollback if critical
4. Debug and fix
5. Deploy patch
6. Post-mortem analysis

**Goal:** P0 resolved in <30 minutes.

---

# Deployment Checklist

**Before going to production:**

- ‚òê Model performance validated on holdout set
- ‚òê Serialization and loading tested
- ‚òê API endpoints implemented and documented
- ‚òê Error handling and input validation
- ‚òê Monitoring dashboard configured
- ‚òê Logging infrastructure in place
- ‚òê A/B testing framework ready
- ‚òê Rollback procedure tested
- ‚òê Cost analysis completed
- ‚òê Security review passed
- ‚òê Documentation updated
- ‚òê Team training completed

---

# Real-World Deployment Example

**Case Study: Uber's Demand Prediction**

**Challenge:** Predict rider demand in real-time across cities

**Solution:**
- Gradient boosting models retrained hourly
- 15-minute prediction windows
- Multiple models per city
- Feature store for real-time data
- Sub-second prediction requirements

**Results:**
- 1M+ predictions per minute
- 99.99% uptime
- <50ms p99 latency
- Automated retraining pipeline
- Reduced driver wait times 20%

**Lesson:** Production != research. Infrastructure matters as much as model.

---

# Common Deployment Failures

**What goes wrong:**

**1. Feature mismatch**
- Training: 45 features
- Production: 44 features (one missing)
- Result: Crash

**2. Data type mismatch**
- Training: numeric
- Production: character "123.45"
- Result: Wrong predictions

**3. Scale differences**
- Training: normalized features
- Production: raw features
- Result: Poor performance

**4. Memory leaks**
- Load model on every request
- Result: Server crashes after 1000 requests

---

# Best Practices Summary

**Keys to successful deployment:**

1. **Reproducibility:** Save everything (model, preprocessing, metadata)
2. **Monitoring:** Track performance and drift continuously
3. **Automation:** Retrain and deploy without manual intervention
4. **Safety:** A/B test and rollback capabilities
5. **Documentation:** Explain decisions and limitations
6. **Security:** Protect models and data
7. **Cost awareness:** Optimize for scale
8. **Team alignment:** Everyone understands the pipeline

---
class: inverse, center, middle

# üéØ CLASSWORK TIME

## Deployment Planning

**Duration:** 25 minutes

---

# Classwork 7: Production Deployment Plan

Open: `classwork_7_deployment/`

**Your tasks:**

1. Create model serialization script
2. Design monitoring dashboard
3. Define retraining triggers
4. Write deployment checklist
5. Plan A/B testing strategy
6. Document rollback procedure

**Template:** `template.Rmd`

---

# Classwork Instructions

**Goal:** Create comprehensive deployment plan for your elastic net model

**Deliverables:**

1. Model saving/loading code
2. Monitoring metrics specification
3. Drift detection thresholds
4. Retraining schedule
5. Incident response plan
6. Cost estimation

**This is your blueprint for taking models to production.**

---


# Part 8: Case Study Integration

---

# From Theory to Practice

**We've covered regularization comprehensively across seven sections**

You understand overfitting diagnosis, Ridge regression mechanics, LASSO feature selection, Elastic Net flexibility, comprehensive diagnostics, model comparison frameworks, and deployment considerations. These concepts form a complete toolkit for building robust predictive models.

This section synthesizes everything through detailed case studies showing how regularization solves real business problems. You'll see complete workflows from raw data to deployed models, including decisions that textbooks rarely discuss: when to use which regularization, how to handle domain-specific constraints, and how to communicate results to stakeholders.

Case studies reveal the messy reality of applied work where data is imperfect, requirements change mid-project, and technical excellence must balance organizational constraints. Learning from realistic scenarios prepares you for the ambiguity and complexity of professional practice.

---

# Case Study 1: Retail Demand Forecasting

**Business Problem:** National retailer needs weekly demand forecasts for 50,000 SKUs across 500 stores to optimize inventory allocation and reduce stockouts costing $2M monthly.

**Data Characteristics:** 2 years of historical sales (100 weeks), 150 features per SKU-store combination including historical demand patterns, promotions, seasonality, competitor activity, local demographics, weather data, and holiday indicators. High correlation among temporal features and geographic features within regions.

**Initial Approach:** Analysts built separate OLS models for each store-SKU combination, resulting in 25 million coefficients. Models overfit severely with many SKUs showing negative demand predictions and wild volatility between weeks. Computational cost of training and storing 25 million models proved prohibitive.

**Regularization Solution:** Implemented hierarchical Ridge regression with store-level and category-level grouping. Reduced to 500 store models with shared regularization, achieving 95% of individual model performance with 1% of parameters. Elastic Net with alpha=0.3 further improved performance by selecting relevant promotional features while maintaining stability across correlated temporal predictors.

---

# Case Study 1: Results and Business Impact

```{r retail-results, echo=FALSE}
results <- tibble(
  Approach = c("Individual OLS", "Pooled OLS", "Hierarchical Ridge", "Elastic Net"),
  Parameters = c("25M", "150", "75K", "25K"),
  Training_Time = c("72 hours", "2 min", "45 min", "30 min"),
  MAPE = c("45%", "38%", "22%", "19%"),
  Stockout_Reduction = c("Baseline", "12%", "42%", "48%"),
  ROI_Annual = c("-$500K", "$800K", "$3.2M", "$3.8M")
)

kable(results, format = "html") %>%
  kable_styling(font_size = 16) %>%
  row_spec(4, bold = TRUE, background = "#e8f4f8")
```

**Key Insights:** Regularization reduced stockouts by 48% compared to baseline OLS by preventing overfitting to noise in historical demand. Ridge component handled correlated features (temperature and seasonality, promotions and holidays) that caused LASSO to be unstable. Elastic Net selected 25K truly predictive features from 150 candidates, reducing computational costs and improving interpretability.

**Implementation Details:** Used cross-validation within product categories to tune lambda separately for fast-moving versus slow-moving items. Fast-moving items (>100 units/week) required less regularization (lambda=0.01) while slow-moving items needed stronger regularization (lambda=0.5) due to higher noise-to-signal ratios. Deployed model updates weekly with 30-day rolling training windows.

---

# Case Study 1: Deployment Architecture

**Production Pipeline:** Raw sales data extracted nightly from point-of-sale systems, features engineered in Spark including 7-day, 28-day, and 52-week moving averages, models scored in batches for all SKU-store combinations within 2-hour window, predictions fed to inventory optimization system.

**Monitoring Strategy:** Daily tracking of forecast accuracy (MAPE) by category and region with alerts when accuracy degrades beyond 25%. Weekly drift detection comparing current demand distributions to training data. Monthly model retraining triggered automatically when cumulative drift exceeds thresholds or new promotional patterns emerge.

**Stakeholder Communication:** Supply chain teams receive daily forecast files with confidence intervals derived from cross-validation errors. Store managers access forecasts through dashboard showing predicted demand with historical accuracy metrics. Executive reporting shows stockout reduction and inventory cost savings attributed to improved forecasting.

**Lessons Learned:** Initial deployment used LASSO exclusively, but discovered instability when promotional features were highly correlated. Switching to Elastic Net improved stability without sacrificing feature selection benefits. Hierarchical structure (grouping similar stores) proved more effective than individual models or complete pooling. Separate lambda tuning by product velocity prevented over-regularization of high-volume items and under-regularization of low-volume items.

---

# Case Study 2: Credit Risk Modeling

**Business Problem:** Regional bank needs to predict loan default probability for personal loan applications to set appropriate interest rates and credit limits while maintaining regulatory compliance requiring model explainability.

**Data Characteristics:** 50,000 historical loans with 200 features including credit bureau data, income verification, employment history, existing debts, payment history, and derived ratios. Severe class imbalance with 8% default rate. Strong multicollinearity among debt-related features (debt-to-income, total debt, monthly payments) and credit utilization measures.

**Regulatory Constraints:** Model must be interpretable for fair lending compliance audits. Features must align with regulatory acceptable use guidelines excluding protected characteristics. Predictions must be explainable to loan officers who override model decisions in 15% of cases. Model must demonstrate stability across demographic groups to avoid disparate impact.

**Modeling Challenges:** OLS logistic regression produced unstable coefficients with counterintuitive signs (higher income predicting higher default risk) due to multicollinearity. Initial model used all 200 features but regulators questioned business rationale for obscure interactions. Need to balance predictive power with interpretability and regulatory defensibility.

---

# Case Study 2: Regularization Strategy

**Approach Selection:** LASSO chosen over Ridge for automatic feature selection supporting interpretability requirements. Elastic Net tested but pure LASSO preferred after stakeholder feedback that sparse models were easier to explain to regulators and loan officers. Grouped LASSO applied to debt-related features ensuring selection of at most one from highly correlated groups.

**Implementation Process:** Started with domain expert feature ranking identifying 50 core features with clear business logic. Applied LASSO to full feature set, then compared selected features against expert list. Iteratively adjusted penalty to achieve balance between predictive power and interpretability. Final model used 28 features, all defensible to regulators with clear relationships to default risk.

**Cross-Validation Approach:** Stratified 10-fold CV preserving default rate in each fold. Additional validation on hold-out test set from different time period to assess temporal stability. Separate evaluation across demographic segments (age, income, geography) to detect potential disparate impact.

**Coefficient Analysis:** All selected features showed intuitive relationships to default risk. Debt-to-income ratio had largest coefficient magnitude (0.42), recent delinquencies second (0.38), credit utilization third (0.31). Income and employment tenure showed negative coefficients (protective factors) with appropriate magnitudes. No spurious or counterintuitive coefficients emerged.

---

# Case Study 2: Results and Compliance

```{r credit-results, echo=FALSE}
results <- tibble(
  Model = c("OLS (All Features)", "Ridge (Lambda=0.1)", "LASSO (Lambda=0.02)", "Expert Rules"),
  Features = c("200", "200 (shrunk)", "28", "12"),
  AUC = c("0.73", "0.76", "0.77", "0.68"),
  Gini = c("0.46", "0.52", "0.54", "0.36"),
  Interpretable = c("No", "Partially", "Yes", "Yes"),
  Regulatory_Approved = c("No", "No", "Yes", "Yes"),
  Default_Capture = c("52%", "61%", "63%", "48%")
)

kable(results, format = "html") %>%
  kable_styling(font_size = 16) %>%
  row_spec(3, bold = TRUE, background = "#e8f4f8")
```

**Business Impact:** LASSO model achieved 63% default capture in top risk decile compared to 48% from expert rules, enabling more accurate risk-based pricing. Reduced bad debt losses by $1.8M annually while maintaining loan approval rates. Regulatory approval obtained after demonstrating stability across demographic groups and clear business justification for each feature.

**Deployment Considerations:** Model scores applications in real-time during underwriting process with typical latency under 200ms. Produces both probability scores and risk tier classifications. Includes confidence intervals from cross-validation to flag borderline cases for manual review. Monthly monitoring reports track performance across segments and alert to potential disparate impact issues.

---

# Case Study 2: Explainability Framework

**Feature Contribution Reports:** For each application, system generates report showing contribution of each feature to final score. Example: "Debt-to-income ratio (45%) contributes +0.08 to default probability. Recent delinquency (Yes) contributes +0.12. Credit utilization (78%) contributes +0.06." Loan officers understand why model assigned specific scores and can validate against their domain knowledge.

**Model Documentation:** Comprehensive documentation package for regulators includes feature definitions and data sources, coefficient values and confidence intervals, validation results across demographic segments, adverse action testing showing compliant behavior, and model monitoring protocols detecting drift or discrimination. Documentation updated annually and reviewed by compliance team before each audit.

**Override Analysis:** System tracks cases where loan officers override model recommendations. Monthly analysis examines override patterns to detect potential issues. If overrides concentrate in specific segments, triggers model review to assess if recalibration needed. Override tracking also identifies high-performing loan officers whose judgment consistently beats model predictions.

**Ongoing Governance:** Quarterly model review committee (risk management, compliance, business leaders, data science) examines performance metrics, drift indicators, override patterns, and regulatory landscape changes. Annual full model validation by independent team not involved in original development.

---

# Case Study 3: Marketing Response Modeling

**Business Problem:** E-commerce company sends 5 million promotional emails weekly. Current strategy sends same offer to all subscribers resulting in 2% conversion rate and customer complaints about irrelevant emails. Need to predict response probability for personalized targeting while respecting email volume constraints.

**Data Challenges:** 12 months of email campaign history with 500 customer features including demographics, browsing behavior, purchase history, email engagement metrics, product preferences, and temporal patterns. Extreme class imbalance with 2% positive response rate. Features exhibit complex interactions (product preferences vary by season, email timing effects depend on customer timezone and work schedule).

**Business Constraints:** Must identify top 30% most responsive customers to target while maintaining minimum 6% response rate in targeted segment to justify email costs. Model must handle new customers with limited history. Predictions must update daily as customer behavior evolves. Cannot use certain features due to privacy policies despite predictive value.

**Technical Requirements:** Training data requires downsampling or weighting to handle class imbalance. Feature engineering pipeline must run daily processing terabytes of clickstream data. Model must score 50 million customer-offer combinations nightly within 4-hour batch window. Predictions must be stable enough that customers don't receive wildly different targeting day-to-day.

---

# Case Study 3: Modeling Approach

**Feature Engineering:** Created 80 core features from raw data including recency-frequency-monetary (RFM) metrics, email engagement scores (opens, clicks, conversions over multiple time windows), product affinity scores (category preferences weighted by recency), temporal patterns (day-of-week and hour-of-day preferences), and cross-features (product affinity √ó seasonality interactions).

**Class Imbalance Handling:** Applied stratified sampling to balance classes in training data (50-50 split) while preserving original distribution in validation set. Alternative approaches tested included weighted loss functions and threshold adjustment on predictions from unbalanced data. Stratified sampling with careful validation proved most effective for this use case.

**Regularization Selection:** Tested Ridge, LASSO, and Elastic Net with extensive cross-validation. Ridge performed poorly due to keeping all 500+ features including noise. Pure LASSO (alpha=1.0) was too aggressive, discarding useful interaction terms. Elastic Net with alpha=0.4 provided optimal balance, selecting 120 features including important interactions while maintaining coefficient stability.

**Hyperparameter Tuning:** Two-dimensional grid search over lambda (regularization strength) and alpha (L1/L2 mixing) using 5-fold stratified cross-validation. Evaluated on AUC, precision at 30% recall (business requirement), and calibration metrics. Final parameters: lambda=0.015, alpha=0.4, achieving AUC=0.82 and precision=7.2% at 30% coverage.

---

# Case Study 3: Production Implementation

```{r marketing-results, echo=FALSE}
results <- tibble(
  Strategy = c("Broadcast to All", "Random 30%", "Rule-Based", "Ridge Model", "Elastic Net Model"),
  Coverage = c("100%", "30%", "30%", "30%", "30%"),
  Response_Rate = c("2.0%", "2.0%", "3.5%", "5.8%", "7.2%"),
  Conversions = c("100K", "30K", "52.5K", "87K", "108K"),
  Email_Cost = c("$500K", "$150K", "$150K", "$150K", "$150K"),
  Revenue = c("$8M", "$2.4M", "$4.2M", "$7.0M", "$8.6M"),
  ROI = c("16x", "16x", "28x", "47x", "57x")
)

kable(results, format = "html") %>%
  kable_styling(font_size = 16) %>%
  row_spec(5, bold = TRUE, background = "#e8f4f8")
```

**Business Impact:** Elastic Net model increased response rate from 2% to 7.2% in targeted segment, delivering 108K conversions from 1.5M targeted emails versus 100K from 5M broadcast emails. Reduced email costs by 70% while increasing revenue by 8%. Improved customer satisfaction scores as subscribers received more relevant offers.

**Technical Architecture:** Feature engineering pipeline runs nightly in Spark processing previous day's clickstream and transaction data. Model scoring executed in parallel across customer segments, completing all 50M predictions in 2.5 hours. Predictions stored in Redis cache for real-time access by email targeting system. Model retrained weekly on rolling 90-day window with automated A/B testing comparing new versus current model before deployment.

---

# Case Study 3: Advanced Considerations

**Temporal Validation:** Standard cross-validation inadequate for time-series data where future depends on past. Implemented walk-forward validation where model trained on months 1-9, validated on month 10, retrained on months 2-10, validated on month 11, continuing forward. Revealed model performance degrades after 60 days, informing weekly retraining schedule.

**Feature Stability Analysis:** Monitored coefficient stability across retraining cycles. Some features showed high variance in coefficients week-to-week (email_open_rate_7d oscillated between 0.3 and 0.8) indicating unreliable signals. Applied additional regularization to high-variance features or excluded them entirely. Stable features (customer_tenure, total_lifetime_value) consistently showed similar coefficients across retraining.

**Segment-Specific Models:** Initial single model applied to all customers showed poor performance for new customers (insufficient history) and VIP customers (different behavioral patterns). Developed three models: new customers (high regularization, emphasize demographic features), standard customers (main Elastic Net model), VIP customers (lower regularization, emphasize recent high-value behaviors). Routing logic assigns customers to appropriate model based on tenure and lifetime value.

**Exploration-Exploitation Tradeoff:** Pure model-based targeting creates feedback loop where low-predicted customers never receive emails, preventing model from learning their true preferences. Implemented epsilon-greedy exploration where 5% of emails sent to random non-targeted customers to gather fresh training data and detect model blind spots.

---

# Case Study 4: Healthcare Cost Prediction

**Business Problem:** Health insurance company needs to predict annual healthcare costs for members to set premiums, reserve funds, and identify high-risk patients for care management programs. Inaccurate predictions lead to inadequate reserves (regulatory violations) or overpricing (customer attrition).

**Data Environment:** Claims data for 2 million members with 300+ features including demographics, diagnosis codes, procedure codes, pharmacy utilization, emergency department visits, provider characteristics, and chronic condition flags. Extreme outcome skewness with median annual cost $1,200 but 99th percentile exceeding $200,000. Many rare but expensive conditions represented by sparse indicator variables.

**Regulatory Requirements:** Model must comply with Affordable Care Act restrictions on using certain demographic factors for premium setting. Must avoid disparate impact across protected groups. Requires annual actuarial certification that predictions are unbiased and adequate for reserve setting. Model documentation must withstand regulatory audits and explain prediction methodology to state insurance commissioners.

**Statistical Challenges:** Heavy-tailed outcome distribution where mean differs drastically from median. Influential outliers (catastrophic claims) disproportionately affect coefficients. Sparse features (rare diagnoses) create numerical instability. Temporal patterns where costs exhibit strong autocorrelation (expensive year often followed by expensive year). Need predictions at individual level but evaluation at population level for reserve adequacy.

---

# Case Study 4: Modeling Strategy

**Outcome Transformation:** Testing multiple approaches to handle cost skewness: log transformation (log(cost + 1) to handle zeros), square root transformation, and two-part model (predict probability of any cost, then predict amount given cost > 0). Log transformation performed best for this use case, normalizing distribution while preserving interpretability through back-transformation.

**Regularization Rationale:** Ridge regression selected over LASSO despite interpretability benefits because actuarial team required all clinical indicators remain in model for regulatory defensibility. Excluding any diagnosis code (even if rarely observed) would raise questions about model completeness from regulators. Ridge shrinks sparse feature coefficients toward zero without elimination, providing stability while maintaining complete feature set.

**Temporal Modeling:** Incorporated lagged features from previous year (prior_year_cost, prior_year_admits, prior_year_chronic_flag_count) which proved highly predictive. Applied separate regularization penalties to current-year versus prior-year features, with stronger regularization on current features prone to overfitting and lighter regularization on stable historical features with established predictive relationships.

**Cross-Validation Design:** Standard k-fold CV inappropriate because members within families correlated (share environment, genetics, provider networks). Implemented clustered CV where entire families held out together, preventing information leakage and providing realistic assessment of generalization to new member cohorts. Maintained representative distribution of ages and chronic conditions in each fold.

---

# Case Study 4: Results and Validation

```{r healthcare-results, echo=FALSE}
results <- tibble(
  Model = c("Historical Average", "GLM (No Regularization)", "Ridge (Lambda=5)", "Two-Part Ridge"),
  RMSE = c("$18,400", "$14,200", "$12,800", "$11,900"),
  MAE = c("$4,200", "$3,100", "$2,800", "$2,650"),
  Reserve_Accuracy = c("87%", "92%", "96%", "97%"),
  Member_Level_R2 = c("0.12", "0.28", "0.34", "0.38"),
  Actuarial_Approved = c("No", "No", "Yes", "Yes"),
  Implementation = c("Simple", "Simple", "Moderate", "Complex")
)

kable(results, format = "html") %>%
  kable_styling(font_size = 15) %>%
  row_spec(3, bold = TRUE, background = "#e8f4f8")
```

**Performance Analysis:** Ridge regression achieved 96% reserve adequacy (predicted aggregate costs within 4% of actual) compared to 87% for historical averaging. Member-level predictions remain noisy (R¬≤=0.34) reflecting genuine uncertainty in individual health trajectories, but population-level aggregates highly accurate. Model correctly identifies high-risk cohorts for care management intervention.

**Validation Testing:** Conducted extensive bias testing across demographic groups (age, gender, geography, chronic conditions) to ensure predictions unbiased and compliant with ACA requirements. Tested model stability by retraining on multiple time periods and verifying coefficient consistency. Performed sensitivity analysis on regularization parameter showing predictions robust to reasonable lambda variations (reserve accuracy 94-97% for lambda ranging 3-8).

---

# Case Study 4: Deployment and Governance

**Production Scoring:** Model scores all 2 million members annually during open enrollment period. Predictions used for individual premium quotes, portfolio-level reserve setting, and care management program targeting. Scoring completed in 6-hour batch window processing 300+ features per member. Results stored in data warehouse accessible to pricing, underwriting, and care management systems.

**Model Monitoring:** Monthly tracking of prediction accuracy comparing forecasted versus actual costs for members through current year. Quarterly recalibration adjusting predictions if systematic bias detected (e.g., costs running 5% higher than predicted). Annual full model retraining incorporating most recent year of claims data and updated clinical guidelines. Actuarial review quarterly examining reserve adequacy and recommending lambda adjustments if needed.

**Stakeholder Reporting:** Actuarial team receives detailed validation reports including accuracy metrics, bias testing, sensitivity analysis, and comparison to benchmark models. State regulators receive annual filing documenting model methodology, validation results, and certification that predictions comply with applicable laws. Care management team receives monthly reports identifying newly high-risk members and prioritizing outreach based on predicted cost and care gap opportunities.

**Lessons Learned:** Initial model excluded prior-year features attempting to predict costs from demographics and current diagnoses alone, resulting in poor accuracy. Adding lagged features dramatically improved predictions. Actuarial requirements necessitated Ridge over LASSO despite preference for sparse models. Log transformation essential for skewed outcomes but created challenges explaining back-transformed predictions to non-technical stakeholders. Clustered cross-validation critical for realistic accuracy assessment.

---

# Common Patterns Across Case Studies

**Regularization Selection Principles**

The four case studies reveal consistent patterns in choosing Ridge, LASSO, or Elastic Net. Ridge excels when domain knowledge requires retaining all features (credit risk, healthcare) or when features are highly correlated and elimination would discard valuable information. LASSO succeeds when interpretability and feature selection are priorities (credit risk model selection) and when many features are genuinely irrelevant noise. Elastic Net dominates when balancing selection with stability (retail demand, marketing response) and when feature correlation is high but some features are clearly more important than others.

The alpha parameter in Elastic Net represents fundamental tradeoff between aggressive feature selection (alpha near 1.0) and stable coefficient estimation (alpha near 0). Marketing case study used alpha=0.4 because correlated behavioral features all contained signal, favoring stability. Alternative scenarios with many purely random features would justify higher alpha to eliminate noise aggressively.

Business constraints often dominate statistical considerations in regularization selection. Regulatory interpretability requirements in credit risk forced LASSO despite Ridge potentially offering slightly better predictive performance. Healthcare actuarial standards required Ridge to retain all clinical indicators regardless of LASSO's feature selection appeal. Real-world model selection requires balancing statistical properties with organizational constraints.

---

# Integration Patterns

**Feature Engineering Pipelines**

All case studies required sophisticated feature engineering transforming raw data into model-ready predictors. Retail demand incorporated temporal aggregations (7-day, 28-day, 52-week windows), seasonal indicators (holiday flags, month effects), and cross-features (promotion √ó category interactions). Marketing response engineered RFM metrics, engagement scores across multiple time horizons, and product affinity calculations. Healthcare constructed lagged features, chronic condition summaries, and utilization intensity measures.

Feature engineering pipeline design critically affects regularization effectiveness. Well-engineered features reduce reliance on regularization to handle noise because human domain knowledge already filtered irrelevant signals. Poorly engineered features with many irrelevant variables require stronger regularization but may not achieve similar performance because regularization cannot fix fundamentally uninformative data. The retail case study's 150 carefully curated features outperformed alternative approaches using 1000+ automated features with aggressive LASSO selection.

Production feature pipelines must handle data quality issues absent in development. Marketing case encountered missing clickstream data when tracking pixels failed. Healthcare dealt with delayed claims submissions creating incomplete current-year features. Retail experienced sudden zeros in competitor pricing feeds during outages. Robust pipelines include imputation strategies, sanity checks rejecting impossible values, and monitoring detecting distribution shifts indicating upstream data issues.

---

# Cross-Validation in Production Contexts

**Beyond Standard K-Fold**

Standard k-fold cross-validation assumes independent observations and stationary distributions, assumptions violated in all four case studies. Retail demand exhibited temporal dependence where this week's demand correlates with last week's demand. Marketing response showed seasonality where campaign effectiveness varies across months. Healthcare costs exhibited family clustering where household members' costs correlate. Credit risk required temporal validation ensuring model works on future applicants not historically similar borrowers.

Walk-forward validation addresses temporal dependence by training only on past data and validating on future data, mimicking production reality where models predict tomorrow's outcomes from today's features. Marketing case study's walk-forward validation revealed performance degrading after 60 days, information standard CV would miss. This finding directly informed production retraining schedule preventing deployment of stale models.

Clustered cross-validation groups related observations (families, stores, customers) ensuring entire clusters held out together. Healthcare case holding out families together prevented optimistic accuracy estimates from models learning family-specific patterns unlikely to generalize to new families. This conservative validation approach builds stakeholder confidence that reported performance reflects true generalization capability.

Business constraints may require custom validation schemes matching evaluation criteria to actual usage. Retail model optimized for demand forecast accuracy weighted by revenue (errors on high-value items penalized more) rather than equal weighting all SKUs. Marketing model validated on precision at fixed recall matching business requirement for 30% targeting coverage, not overall AUC which doesn't reflect operational constraints.

---

# Model Monitoring and Maintenance

**Production Requires Ongoing Attention**

All case studies implemented comprehensive monitoring beyond simple accuracy tracking. Feature drift detection compared current distributions to training distributions using statistical tests (Kolmogorov-Smirnov for continuous features, chi-square for categorical) flagging distributions shifted significantly. Prediction drift tracked whether model output distribution changed substantially even when accuracy remained acceptable, indicating potential calibration issues.

Segment-specific monitoring proved essential because overall metrics can mask subgroup failures. Retail demand monitoring calculated accuracy separately for each product category because some categories (seasonal clothing) inherently harder to predict than others (staple groceries). Healthcare tracked prediction bias across age groups, chronic condition cohorts, and geographic regions to detect potential disparate impact early. Marketing measured response rates across customer value tiers preventing model from optimizing average performance at expense of VIP customer experience.

Retraining strategies balanced scheduled updates (predictable costs, may update unnecessarily) with triggered updates (efficient but reactive, may miss gradual drift). Retail implemented weekly scheduled retraining because demand patterns shifted continuously with promotions and seasons, making fixed schedule efficient. Healthcare used annual scheduled retraining with quarterly triggered updates if population composition changed significantly (e.g., large employer group joins or leaves plan). Marketing employed monthly scheduled retraining with triggered updates when A/B testing detected current model underperforming on recent campaigns.

---

# Stakeholder Communication Frameworks

**Translating Technical Results**

Each case study required communicating regularization benefits to non-technical stakeholders unfamiliar with L1/L2 penalties and bias-variance tradeoffs. Successful communication avoided mathematical details in favor of business-relevant analogies. Retail team explained Ridge as "preventing the model from overreacting to last week's unusual spike by balancing recent patterns with longer-term trends." Credit risk explained LASSO as "automatically identifying which of 200 factors actually matter for predicting defaults rather than forcing loan officers to consider everything."

Visualization proved more effective than metrics for executive audiences. Marketing presented charts showing model-targeted customers converting at 7.2% versus randomly selected customers at 2.0%, immediately conveying value without discussing AUC or precision-recall curves. Healthcare showed distribution of predicted versus actual costs with tight alignment at aggregate level, building confidence in reserve adequacy without discussing RMSE statistics.

Model documentation required different versions for different audiences. Regulatory filings for healthcare model included mathematical notation, coefficient tables, and statistical test results. Executive summaries for senior leadership emphasized business impact metrics (reserve accuracy, premium competitiveness, care management program effectiveness). Operational documentation for pricing analysts focused on practical usage (how to interpret predictions, when to override model, who to contact for questions) rather than internal mechanics.

Ongoing reporting maintained stakeholder engagement and trust. Quarterly business reviews presented model performance trends, highlighted improvements from recent updates, and previewed upcoming enhancements. Monthly operational reports gave frontline users confidence that model quality was actively monitored and maintained. Annual strategic reviews tied model outcomes to business objectives demonstrating continued value delivery.

---

# Lessons Learned Summary

**Critical Success Factors**

Across all case studies, certain patterns consistently separated successful deployments from failed attempts. Strong collaboration between data scientists, domain experts, and business stakeholders ensured models addressed actual business problems with realistic constraints. Retail model succeeded because data scientists worked closely with supply chain experts understanding seasonal patterns and promotional mechanics. Credit risk model gained regulatory approval because risk managers and compliance officers participated throughout development, not just final review.

Iterative development with frequent validation checkpoints prevented investing months building wrong models. Marketing team tested simplified prototype on small customer sample before scaling full feature engineering pipeline, discovering data quality issues that would have derailed large-scale implementation. Healthcare team conducted staged rollout scoring small member cohorts and comparing predictions to claims before using model for premium setting, building confidence in accuracy before high-stakes application.

Realistic validation matching production conditions provided honest accuracy assessment preventing unpleasant surprises after deployment. Standard cross-validation would have overstated performance in all four cases. Walk-forward validation, clustered validation, and stratified validation techniques more accurately predicted production performance by respecting temporal dependencies, clustering effects, and class imbalance realities.

Comprehensive monitoring and maintenance plans sustained model value over time as data distributions shifted and business conditions evolved. Initial model performance rarely persists indefinitely without active maintenance. All successful deployments included explicit monitoring protocols, retraining schedules, and stakeholder communication frameworks.

---

# Anti-Patterns to Avoid

**Common Mistakes and Failures**

Several regularization anti-patterns emerged from project post-mortems discussing what went wrong in failed attempts. Choosing regularization method based on statistical fashion rather than problem characteristics led to poor outcomes. One team used LASSO for interpretability in highly correlated feature space, resulting in unstable feature selection where retrained models selected different features each week. Switching to Elastic Net provided needed stability.

Insufficient validation rigor created optimistic accuracy expectations. A demand forecasting project used standard k-fold CV ignoring temporal dependence, reported 15% MAPE in development, but experienced 35% MAPE in production because validation didn't respect time-series structure. Post-mortem analysis revealed walk-forward validation would have predicted production performance accurately, enabling course correction before costly deployment.

Neglecting feature engineering while focusing exclusively on regularization parameter tuning yielded mediocre results. One credit risk project spent weeks optimizing lambda values across four decimal places but used raw features without domain-informed transformations. Alternative approach investing effort in feature engineering (creating debt burden ratios, payment consistency metrics, employment stability indicators) with standard regularization outperformed the over-tuned model with weak features.

Inadequate monitoring allowed models to degrade silently until business impact became severe. A recommendation model ran 18 months without performance tracking, during which time click-through rates declined 40% as customer preferences shifted and model became stale. By the time degradation was noticed, company had lost significant revenue and customer satisfaction suffered.

---

class: center, middle

# Synthesis: The Complete Regularization Workflow

---

# End-to-End Workflow

**From business problem to deployed model**

Case studies revealed consistent workflow stages independent of specific application domain. Successful projects began with clear business problem definition articulating success criteria, constraints, and stakeholder requirements. Retail team defined success as reducing stockouts by 30% within computational budget allowing daily forecast updates. Credit team defined success as improving default prediction while maintaining regulatory compliance and loan officer usability.

Data exploration and feature engineering consumed 40-60% of total project effort across all case studies. Understanding feature distributions, correlations, missingness patterns, and temporal dynamics informed modeling choices and prevented surprises during validation. Marketing team discovered that email engagement features contained different signal depending on customer tenure, leading to separate feature engineering pipelines for new versus established customers.

Model development involved comparing multiple regularization approaches rather than selecting one method a priori. Retail team systematically evaluated OLS, Ridge, LASSO, and Elastic Net across multiple lambda values, discovering Elastic Net with alpha=0.3 provided optimal tradeoff. Healthcare team compared standard Ridge, grouped Ridge with separate penalties for current versus historical features, and two-part Ridge for zero-cost handling, ultimately selecting two-part approach for superior reserve accuracy.

Production deployment required equal attention to technical infrastructure and organizational processes. Technical implementation included feature pipelines, model serving, monitoring systems, and retraining automation. Organizational implementation included stakeholder communication, escalation procedures, maintenance responsibilities, and governance frameworks. Projects succeeding technically but failing organizationally (poor adoption, stakeholder resistance, inadequate maintenance) delivered limited business value.

---

# Decision Framework

**Systematic approach to regularization selection**

The cumulative evidence from case studies supports structured decision framework for choosing regularization approach. Start by assessing feature correlation structure using pairwise correlation matrices and variance inflation factors. High multicollinearity (VIF > 5 for many features) suggests Ridge or Elastic Net to handle correlated predictors. Low correlation enables LASSO's aggressive feature selection without stability concerns.

Consider interpretability requirements from regulatory, business, or operational perspectives. Healthcare and credit risk applications required either retaining all features (Ridge) or clear feature selection justification (LASSO). Marketing and retail faced fewer interpretability constraints, enabling Elastic Net's flexibility. Regulatory environments favor conservative approaches where coefficient stability matters more than optimal predictive performance.

Evaluate feature quality and relevance. Scenarios with many genuinely irrelevant features favor LASSO's automatic selection reducing noise. Scenarios where domain expertise already filtered features to meaningful predictors favor Ridge's retention of all information. Mixed scenarios with some irrelevant features and some correlated relevant features favor Elastic Net's balanced approach.

Assess computational constraints and deployment requirements. LASSO produces sparse models requiring fewer features in production, reducing inference latency and pipeline complexity. Ridge retains all features potentially complicating production systems requiring diverse data sources. Retail case study chose Elastic Net partially because selected 25K features from 150K candidates reduced production computational costs while maintaining accuracy.

---

# Key Takeaways

**Essential lessons from case study analysis**

Regularization solves real business problems by preventing overfitting that would cause production failures. All four case studies showed dramatic improvements over unregularized OLS. Retail reduced stockouts 48%, credit improved default capture 15 percentage points, marketing increased response rate 3.6x, healthcare improved reserve accuracy from 87% to 96%. These improvements translated to millions of dollars in business value annually.

Regularization method selection requires understanding problem characteristics, not following statistical fashion. Ridge, LASSO, and Elastic Net each excel in different scenarios based on feature correlation, interpretability requirements, and business constraints. Systematic evaluation of alternatives using realistic validation prevents suboptimal choices. The retail team's methodical testing of all approaches ensured optimal Elastic Net selection rather than defaulting to LASSO because of popularity.

Production deployment extends far beyond model training to encompass monitoring, maintenance, stakeholder communication, and organizational integration. Models performing excellently in development can fail in production without comprehensive deployment planning. All successful case studies invested substantial effort in monitoring systems, retraining protocols, documentation, and stakeholder engagement beyond the core modeling work.

Cross-functional collaboration between data scientists, domain experts, and business stakeholders proves essential for success. Technical excellence in regularization techniques cannot overcome misalignment between model objectives and business requirements or inadequate understanding of domain-specific constraints. The most successful projects involved continuous collaboration throughout development rather than sequential handoffs.

---
class: center, middle

# Questions?

---

# Part 9: Advanced Topics & Extensions

---

# Beyond Standard Regularization

**You've mastered the core regularization toolkit**

Ridge, LASSO, and Elastic Net form the foundation of modern regularized regression enabling stable coefficient estimation, automatic feature selection, and optimal prediction under multicollinearity. These techniques solve the majority of practical prediction problems in business analytics.

Advanced extensions address specialized scenarios where standard regularization requires adaptation. Grouped features with known structure benefit from group LASSO preserving or eliminating entire feature sets together. Sparse group LASSO combines group-level and individual-level selection for hierarchical feature spaces. Adaptive LASSO assigns feature-specific penalties improving selection consistency. Fused LASSO enforces smoothness across ordered features for spatial or temporal data. Non-convex penalties like SCAD and MCP provide oracle properties under certain conditions.

This section introduces these advanced methods not for immediate application but to expand your conceptual framework for recognizing when standard approaches require extension and where to look when encountering specialized requirements. Understanding the landscape of regularization techniques enables principled method selection as problem complexity increases.

---

# Group LASSO: Structured Feature Selection

**Motivation from real feature structures**

Many practical problems contain natural feature groupings where selection should respect group structure. Categorical variables encoded as dummy indicators form groups where selecting any dummy implies considering the categorical factor. Interaction terms between two variables form groups with their main effects. Polynomial expansions (x, x¬≤, x¬≥) represent different aspects of the same underlying relationship. Time-lagged features (lag1, lag2, lag3) capture temporal dynamics requiring coordinated selection.

Standard LASSO treats all features independently, potentially selecting some dummies from a categorical variable while excluding others, creating logical inconsistencies in interpretation. If industry_technology and industry_finance are both relevant, excluding industry_retail may reflect insufficient data rather than true irrelevance. Group LASSO forces all-or-nothing selection preserving categorical factor coherence.

Group LASSO penalty: Œª ‚àë ||Œ≤_g||‚ÇÇ where g indexes feature groups, ||Œ≤_g||‚ÇÇ denotes L2 norm of coefficients within group g. This penalty shrinks entire groups toward zero together. As lambda increases, entire groups drop out simultaneously rather than individual features within groups. The result maintains logical feature relationships while achieving sparsity at the group level.

---

# Group LASSO: Mathematical Formulation

**Optimization objective balances fit and group sparsity**

minimize: ¬Ωn ‚àë(y_i - Œ≤‚ÇÄ - ‚àëŒ≤_jx_ij)¬≤ + Œª ‚àë_g ‚àö(p_g)||Œ≤_g||‚ÇÇ

The ‚àö(p_g) term adjusts penalty for group size, preventing discrimination against larger groups. Without this adjustment, groups with many features face harsher penalties than small groups, biasing selection toward small groups regardless of predictive value.

Implementation uses block coordinate descent alternating between optimizing coefficients within each group (holding other groups fixed) and updating groups sequentially. The gglasso R package provides efficient implementation. Convergence typically occurs within dozens of iterations for moderate-sized problems.

Cross-validation tunes lambda using prediction error across folds. Unlike standard LASSO where solution path shows individual features entering at different lambda values, group LASSO solution path shows entire groups entering or exiting together. Visualization plots number of active groups versus lambda rather than individual coefficients.

---

# Sparse Group LASSO: Hierarchical Selection

**Combining group-level and individual-level sparsity**

Group LASSO enforces group-wise sparsity but retains all features within selected groups. Some applications benefit from selecting important groups while performing within-group feature selection. Sparse group LASSO adds individual L1 penalty to group penalty achieving both group sparsity and within-group sparsity.

Penalty: Œª[(1-Œ±)‚àë_g ‚àö(p_g)||Œ≤_g||‚ÇÇ + Œ±‚àë_j |Œ≤_j|]

The mixing parameter Œ± controls balance between group selection (Œ±=0, pure group LASSO) and individual selection (Œ±=1, standard LASSO). Intermediate Œ± values like 0.3-0.5 enforce hierarchical selection first eliminating irrelevant groups then performing within-group selection among remaining features.

Practical applications include genomics where genes group into pathways (select important pathways, then select important genes within pathways), marketing where customer features group by data source (select important sources, then select important features from chosen sources), and finance where economic indicators group by sector (select important sectors, then select important indicators within sectors).

---

# Adaptive LASSO: Oracle Properties

**Feature-specific penalties improve selection consistency**

Standard LASSO applies uniform penalty Œª to all coefficients treating features symmetrically. Adaptive LASSO assigns feature-specific penalties Œªw_j to each coefficient Œ≤_j where weights w_j reflect feature importance. Features believed more important receive smaller penalties allowing larger coefficients, while less important features face stronger penalties driving them toward zero more aggressively.

Weights typically derived from initial unpenalized or weakly penalized estimates. Common choice: w_j = 1/|Œ≤ÃÇ_j^OLS|^Œ≥ where Œ≤ÃÇ_j^OLS denotes OLS estimate and Œ≥ > 0 controls weight adaptation strength. Features with large OLS coefficients receive small penalties, features with small OLS coefficients receive large penalties. This approach assumes initial estimates contain information about true feature importance despite potential instability.

Theoretical advantage: under certain regularity conditions, adaptive LASSO achieves oracle properties selecting exactly the true non-zero coefficients with probability approaching 1 as sample size increases and estimating selected coefficients with same asymptotic distribution as if true model were known a priori. Standard LASSO lacks these oracle properties, tending to over-select features asymptotically.

Practical benefit: improved feature selection consistency across resampling. Adaptive LASSO more reliably identifies truly important features compared to standard LASSO which may include irrelevant features due to correlation with truly relevant features. The price is requiring initial estimates that may themselves be unstable in high-dimensional settings.

---

# Fused LASSO: Smoothness Penalties

**Enforcing local similarity for ordered features**

Fused LASSO adds penalty encouraging similarity between coefficients of adjacent features in addition to standard L1 penalty driving coefficients toward zero. This proves valuable when features have natural ordering where nearby features should have similar effects.

Penalty: Œª‚ÇÅ‚àë|Œ≤_j| + Œª‚ÇÇ‚àë|Œ≤_j - Œ≤_(j-1)|

The first term (L1 penalty) provides standard sparsity. The second term (fusion penalty) encourages adjacent coefficients to be equal, creating piecewise-constant coefficient profiles across the ordered feature space. As Œª‚ÇÇ increases, more adjacent coefficients fuse to identical values reducing effective dimensionality.

Applications include spatial data where geographic regions should have similar coefficients for nearby locations, temporal data where effects should vary smoothly across time, and ordinal variables where ordered categories should show gradual changes rather than erratic jumps. For example, age effects on health outcomes should transition smoothly from younger to older age groups rather than showing arbitrary discontinuities.

The genlasso R package implements fused LASSO and generalizations. Solution path visualization shows coefficients transitioning from distinct values at low Œª‚ÇÇ to fused groups at high Œª‚ÇÇ. The optimal Œª‚ÇÇ balances model flexibility (allowing different coefficients) with smoothness (enforcing local similarity).

---

# Non-Convex Penalties: SCAD and MCP

**Beyond L1: Alternative sparsity-inducing penalties**

LASSO's L1 penalty shrinks all coefficients proportionally to their magnitude, biasing large true coefficients downward. This bias-variance tradeoff accepts bias in large coefficients to reduce variance in small coefficients. Non-convex penalties like SCAD (Smoothly Clipped Absolute Deviation) and MCP (Minimax Concave Penalty) reduce bias for large coefficients while maintaining strong shrinkage for small coefficients.

SCAD penalty piecewise-defined across three regions. For small coefficients (|Œ≤| < Œª), standard L1 penalty Œª|Œ≤| applies. For medium coefficients (Œª < |Œ≤| < aŒª), quadratic transition region smoothly reduces penalty rate. For large coefficients (|Œ≤| > aŒª), constant penalty aŒª applies producing no additional shrinkage. Parameter a typically set to 3.7 based on theoretical considerations.

MCP penalty: Œª|Œ≤| - Œ≤¬≤/(2Œ≥) for |Œ≤| < Œ≥Œª, and Œ≥Œª¬≤/2 for |Œ≤| ‚â• Œ≥Œª. The penalty increases linearly initially then plateaus at Œ≥Œª¬≤/2 for large coefficients. Parameter Œ≥ controls transition between linear and constant regions, typically set between 1.5-3.

Theoretical advantage: under regularity conditions, SCAD and MCP achieve oracle properties similar to adaptive LASSO selecting true model with consistency and unbiased estimation. These methods also satisfy unbiasedness property where sufficiently large true coefficients estimated without shrinkage bias.

---

# Non-Convex Penalties: Practical Considerations

**Computational and practical tradeoffs**

Non-convex penalties require more sophisticated optimization algorithms than convex penalties. LASSO benefits from convex objective with unique global minimum enabling efficient coordinate descent. SCAD and MCP introduce non-convexity potentially creating multiple local minima requiring careful initialization and convergence checking.

The ncvreg R package implements coordinate descent with local quadratic approximation efficiently solving SCAD and MCP penalized regression. Initialization typically uses LASSO solution ensuring algorithm starts in reasonable parameter region. Multiple random initializations verify solution stability and detect potential multiple local optima.

Practical performance gains over LASSO modest in many applications. Simulation studies show SCAD and MCP reducing coefficient bias by 20-40% compared to LASSO when true coefficients are large, but offering minimal advantage when true coefficients are moderate. Cross-validation typically shows similar prediction error across methods unless bias in large coefficients substantially affects predictions.

When to consider non-convex penalties: applications where accurate estimation of large coefficients matters more than prediction error (e.g., understanding dose-response relationships in medical research), scenarios with strong prior belief that many true coefficients are zero but non-zero coefficients are large (sparse but strong signals), and situations where coefficient bias could mislead scientific interpretation despite acceptable prediction performance.

---

# Bayesian Regularization: Prior Distributions

**Regularization as Bayesian priors**

Frequentist regularization (Ridge, LASSO) and Bayesian estimation with informative priors connect mathematically through optimization-prior correspondence. Ridge regression with penalty Œª‚àëŒ≤¬≤_j equivalent to Bayesian estimation with Gaussian priors Œ≤_j ~ N(0, œÉ¬≤/Œª) on coefficients. LASSO with penalty Œª‚àë|Œ≤_j| equivalent to Laplace (double exponential) priors Œ≤_j ~ Laplace(0, œÉ¬≤/Œª).

This correspondence provides alternative interpretation of regularization as encoding prior beliefs about coefficient distributions. Ridge penalty assumes coefficients drawn from normal distribution centered at zero with small variance, expressing belief that most coefficients are small but none exactly zero. LASSO prior assumes coefficients drawn from Laplace distribution with sharp peak at zero and heavy tails, expressing belief that many coefficients exactly zero but non-zero coefficients may be substantial.

Bayesian framework enables uncertainty quantification through posterior distributions rather than point estimates. After observing data, posterior distribution combines prior beliefs with likelihood yielding full distribution over coefficient values. Credible intervals from posterior provide natural uncertainty quantification compared to frequentist confidence intervals requiring bootstrap or asymptotic approximations.

Software like rstanarm and brms implement Bayesian regularized regression using Markov Chain Monte Carlo (MCMC) sampling. These tools return posterior samples enabling visualization of coefficient uncertainty, probability statements about specific hypotheses, and prediction intervals incorporating parameter uncertainty.

---

# Bayesian Regularization: Hierarchical Models

**Automatic relevance determination and spike-and-slab priors**

Hierarchical Bayesian models extend simple prior specifications enabling adaptive regularization where data informs penalty strength for each coefficient. Instead of fixing penalty Œª a priori, hierarchical models place hyperpriors on penalty parameters learning appropriate regularization from data.

Spike-and-slab priors model each coefficient as mixture of two distributions: point mass at zero (spike) representing irrelevant features, and diffuse distribution (slab) representing relevant features. Mixture indicator variables determine which features are relevant with mixture probabilities learned from data. This approach performs Bayesian variable selection identifying relevant features through posterior inclusion probabilities.

Automatic relevance determination (ARD) assigns separate precision hyperparameters to each coefficient allowing data to determine appropriate shrinkage for different features. Features with weak predictive signal receive strong shrinkage through learned hyperparameters, features with strong signal receive minimal shrinkage. ARD adapts regularization automatically without cross-validation.

Practical advantages include principled uncertainty quantification, automatic feature selection through posterior inclusion probabilities, and adaptive regularization without computationally expensive cross-validation. Disadvantages include computational cost (MCMC sampling requires thousands of iterations), convergence diagnostics complexity, and sensitivity to prior specification requiring careful model building.

---

# Regularization for GLMs and Beyond

**Extending regularization to non-Gaussian outcomes**

Regularization principles extend beyond linear regression to generalized linear models handling binary outcomes (logistic regression), count data (Poisson regression), and other exponential family distributions. The glmnet package supports regularized GLMs using penalized maximum likelihood estimation.

Logistic LASSO for binary classification: minimize -1/n ‚àë[y_i log(p_i) + (1-y_i)log(1-p_i)] + Œª‚àë|Œ≤_j| where p_i = 1/(1+exp(-Œ≤‚ÇÄ-‚àëŒ≤_jx_ij)) denotes predicted probability. This objective balances logistic loss (negative log-likelihood) with L1 penalty performing feature selection for classification problems. Applications include predicting customer churn, credit default, disease diagnosis, and fraud detection.

Poisson LASSO for count data: minimize -1/n ‚àë[y_i log(Œº_i) - Œº_i] + Œª‚àë|Œ≤_j| where Œº_i = exp(Œ≤‚ÇÄ+‚àëŒ≤_jx_ij) denotes predicted count. This handles non-negative integer outcomes like website visits, product purchases, insurance claims, and accident frequencies. Regularization prevents overfitting when modeling rare events with many potential predictors.

Cox proportional hazards LASSO for survival analysis: extends regularization to time-to-event data with censoring. Applications include predicting customer lifetime, equipment failure times, and patient survival. The glmnet package supports Cox models through family="cox" specification.

---

# Regularization in High Dimensions

**When p >> n: ultra-high-dimensional settings**

Classical statistical theory assumes sample size n exceeds number of features p enabling consistent estimation as n grows. Modern data increasingly violates this assumption with p >> n scenarios where features outnumber observations, making OLS undefined and requiring specialized methods.

Examples include genomics (measuring 20,000 genes on 100 patients), text analytics (10,000 vocabulary terms from 500 documents), and computer vision (1 million pixel features from 1,000 images). These ultra-high-dimensional settings require aggressive feature selection reducing dimensionality to manageable levels before estimation.

LASSO provides fundamental tool for p >> n settings with theoretical guarantees under sparsity assumptions. If true model contains only s << n non-zero coefficients, LASSO can recover true model with high probability when sample size n scales with s¬∑log(p) rather than p itself. This logarithmic dependence on dimensionality enables analysis even when p in millions if true model is sparse.

Sure Independence Screening (SIS) provides preliminary feature reduction before regularization. SIS ranks features by marginal correlation with outcome, retains top d features (often d ‚âà n/log(n)), then applies LASSO to reduced feature set. This two-stage approach computationally feasible when direct LASSO on full feature set intractable and statistically beneficial by reducing false positive feature selection.

---

# Regularization for Structured Data

**Specialized methods for images, networks, and time series**

Image data requires regularization respecting spatial structure. Total variation (TV) regularization penalizes differences between adjacent pixels encouraging piecewise-smooth solutions: Œª‚àë|Œ≤_ij - Œ≤_i'j'| where (i,j) and (i',j') denote adjacent pixels. This approach enables denoising and inpainting preserving edges while smoothing homogeneous regions.

Network data with graph structure benefits from graph LASSO imposing sparsity on precision matrices (inverse covariances) for multivariate Gaussian data. The penalty Œª‚àë|Œò_ij| where Œò denotes precision matrix encourages sparse graphical models estimating conditional independence structure. Applications include gene regulatory networks, financial correlation networks, and social networks.

Time series data requires regularization accounting for temporal dependence. Vector autoregression (VAR) models with LASSO penalties enable sparse estimation of dynamic relationships between multiple time series. Trend filtering combines fused LASSO with higher-order differences enabling flexible non-parametric trend estimation with automatic smoothness selection.

Panel data (multiple entities observed over time) combines cross-sectional and time-series structure. Fixed effects combined with regularization enable heterogeneous coefficient estimation across entities while borrowing strength through penalization. Applications include analyzing customer behavior panels, economic regions over time, and patient trajectories in healthcare.

---

# Computational Considerations at Scale

**Efficient algorithms for large-scale regularization**

Standard coordinate descent algorithms solve regularized regression problems with thousands of features in seconds on modern hardware. Large-scale applications with millions of features or billions of observations require specialized algorithms exploiting problem structure.

Stochastic gradient descent (SGD) updates parameters using random data subsets rather than full dataset enabling online learning from streaming data and parallelization across distributed systems. SGD trades exact solutions for computational scalability, converging to approximate solutions with statistical error comparable to exact methods given sufficient iterations.

Proximal gradient methods separate smooth (loss function) and non-smooth (penalty) components of objective function enabling efficient optimization for complex penalties. FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) accelerates proximal gradient descent achieving optimal convergence rates for first-order methods.

Distributed computing frameworks like Apache Spark enable regularized regression on massive datasets through parallelized computation. The sparklyr package provides R interface to Spark MLlib implementing distributed Ridge and LASSO. Typical workflow partitions data across cluster nodes, computes local gradients on each partition, aggregates gradients centrally, and updates parameters iteratively until convergence.

Approximate methods sacrifice exact solutions for dramatic speedups. Randomized sketching reduces effective dimensionality by projecting data onto lower-dimensional subspace preserving geometric structure. These techniques enable near-optimal solutions in fraction of time required for exact optimization on billion-scale problems.

---

# Online Learning and Adaptive Regularization

**Regularization in streaming and non-stationary environments**

Batch learning assumes complete dataset available before training begins, enabling cross-validation and comprehensive model selection. Online learning processes streaming data incrementally updating model as new observations arrive without access to complete historical data.

Online gradient descent with adaptive regularization adjusts penalty strength based on observed gradient magnitudes. Features with consistently small gradients (weak predictive signal) receive increasing regularization over time, features with large gradients (strong signal) receive decreasing regularization. This adaptation allows model to focus on relevant features as more data accumulates.

Sliding window approaches maintain models on recent data windows discarding old observations as new data arrives. Combined with regularization, sliding windows enable adaptation to non-stationary environments where relationships change over time. Window size trades responsiveness to change against statistical efficiency, typically chosen through validation on hold-out data or based on domain knowledge about typical change frequencies.

Concept drift detection monitors model performance over time triggering retraining when performance degrades beyond thresholds. Combined with regularization enabling fast retraining on recent data, this approach maintains model quality in evolving environments. Applications include fraud detection (fraud patterns evolve), recommendation systems (user preferences shift), and demand forecasting (market conditions change).

---

# Regularization for Fairness and Interpretability

**Encoding social values through constrained optimization**

Traditional regularization optimizes prediction accuracy subject to coefficient magnitude penalties. Emerging applications require additional constraints encoding fairness requirements preventing discriminatory predictions or interpretability requirements ensuring model transparency.

Fairness-constrained optimization adds constraints requiring similar model performance across demographic groups. Example: constrain difference in false positive rates between protected groups to remain below threshold ensuring equitable treatment. Combined with regularization, this approach balances predictive performance, model complexity, and fairness simultaneously.

Monotonicity constraints ensure coefficient signs align with domain knowledge preventing counterintuitive relationships. Example: in credit scoring, higher income should never predict higher default risk. Isotonic regression enforces monotonicity through constrained optimization. Combined with regularization, monotone models achieve sparsity while respecting directional prior beliefs.

Interaction constraints limit model complexity by restricting which features can interact. Example: allow interactions only between features from same data source or allow only pairwise interactions excluding higher-order terms. These constraints improve interpretability by preventing combinatorial explosion of potential interactions while regularization selects important allowed interactions.

Rule-based regularization encourages models producing human-interpretable decision rules rather than complex coefficient combinations. This approach proves valuable in regulated industries requiring model explainability and in applications where human decision-makers must understand model reasoning to trust and act on predictions.

---

# Integrating Domain Knowledge

**Combining data-driven and knowledge-driven modeling**

Pure data-driven regularization treats all features symmetrically applying uniform penalties. Domain knowledge about feature importance, expected coefficient signs, or functional relationships can improve model quality when encoded appropriately.

Weighted penalties assign feature-specific regularization reflecting prior beliefs about importance. Features known to be crucial receive small penalties allowing large coefficients, speculative features receive strong penalties requiring substantial data evidence before inclusion. This approach formalizes expert knowledge within regularization framework without hard constraints risking model misspecification if knowledge proves incorrect.

Sign constraints force coefficients to have specified signs based on theoretical expectations. Example: economic theory predicts price negatively affects demand, so constrain price coefficient to be negative. Combined with regularization, sign-constrained estimation achieves sparsity while respecting theoretical constraints preventing data artifacts from producing theoretically impossible relationships.

Structural equation models with regularization enable theory-driven path models where domain knowledge specifies causal structure and regularization performs variable selection within specified pathways. This integration balances theoretical frameworks with empirical model selection acknowledging that theories specify general relationships without identifying all relevant mediators and moderators.

Active learning strategies leverage domain expertise to guide data collection. Instead of passively accepting available data, active learning identifies maximally informative observations to collect next based on current model uncertainty. Combined with regularization, active learning reduces data requirements by strategically sampling regions where additional data provides most information about model parameters.

---

# Future Directions

**Emerging trends in regularization research**

Deep learning architectures require regularization at scale with millions of parameters. Dropout randomly zeros activation units during training providing implicit regularization preventing co-adaptation of features. Batch normalization accelerates training while providing regularization effect. Weight decay (L2 penalty) remains standard practice in neural network training. Understanding optimal regularization for deep architectures remains active research area.

Automated machine learning (AutoML) systems optimize entire pipelines including feature engineering, algorithm selection, and hyperparameter tuning. Regularization strength becomes one hyperparameter among many in automated search spaces. Meta-learning approaches learn to predict optimal regularization given dataset characteristics, potentially eliminating manual cross-validation by transferring knowledge across problems.

Causal inference increasingly integrates regularization for high-dimensional treatment effect estimation. Double machine learning uses regularized methods for nuisance parameter estimation enabling valid causal inference even when treatment assignment models and outcome models contain many features. Regularization enables flexible covariate adjustment controlling for many potential confounders without parametric assumptions.

Federated learning trains models across decentralized devices without centralizing sensitive data. Regularization in federated settings must account for communication constraints (minimize parameter updates transmitted across network) and privacy requirements (prevent model memorizing individual-level data through appropriate penalties). This emerging application area combines regularization with privacy-preserving machine learning.

---

class: center, middle

# Classwork 8: Advanced Method Exploration

## Time: 25-30 minutes

## Apply group LASSO, adaptive LASSO, or Bayesian regularization to dataset with known feature structure

---

class: center, middle

# Take a 10-minute break

## When you return, we'll summarize the complete lecture and introduce Homework 2

---

# Part 10: Summary & Homework Introduction

---

# The Complete Journey

**From overfitting to production-ready regularized models**

This lecture covered the full spectrum of regularized regression from fundamental concepts through advanced applications. You began by diagnosing overfitting through bias-variance decomposition and recognizing the characteristic patterns of models that memorize training noise rather than learning generalizable relationships. Understanding the problem deeply enabled appreciating regularization solutions.

Ridge regression introduced L2 penalties shrinking coefficients toward zero proportionally to their magnitude, stabilizing estimates under multicollinearity while retaining all features. LASSO brought L1 penalties driving coefficients to exactly zero, enabling automatic feature selection and sparse models with enhanced interpretability. Elastic Net combined both penalties providing flexible regularization adapting to problem-specific feature correlation structures.

Beyond mechanics, you learned systematic frameworks for model comparison balancing predictive performance with complexity through cross-validation, information criteria, and business-relevant metrics. Deployment considerations prepared you for production realities where monitoring, retraining, and stakeholder communication determine whether models deliver sustained business value. Case studies synthesized concepts showing complete workflows from raw business problems to deployed solutions generating measurable impact.

---

# Core Concepts Mastery

**Essential principles underlying all regularization methods**

The bias-variance tradeoff represents the fundamental constraint in statistical learning. Reducing variance through regularization necessarily introduces bias, with optimal regularization balancing these competing errors to minimize total prediction error. This tradeoff has no universal solution; optimal balance depends on problem characteristics including signal-to-noise ratio, feature correlation structure, and sample size relative to dimensionality.

Regularization paths visualize how coefficients evolve as penalty strength increases, providing intuition about relative feature importance and stability. Features entering paths at low lambda values demonstrate robust predictive relationships, features entering only at very low lambda likely represent noise fitting. Path visualization enables qualitative assessment complementing quantitative cross-validation metrics.

Cross-validation provides empirical guidance for hyperparameter selection when theoretical principles offer insufficient specificity. Stratified cross-validation preserves outcome distributions across folds. Clustered cross-validation respects dependencies in grouped data. Walk-forward validation accounts for temporal ordering. The validation strategy must match the deployment reality to provide honest performance assessment.

Model comparison requires moving beyond single metrics to comprehensive evaluation frameworks. Predictive metrics (RMSE, MAE, R¬≤) assess forecasting accuracy. Selection metrics (AIC, BIC) balance fit with complexity. Business metrics (cost savings, ROI, risk reduction) connect technical performance to organizational value. Optimal model selection integrates multiple perspectives rather than optimizing single criterion.

---

# Practical Implementation Skills

**Technical capabilities for building regularized models**

You can now implement complete regularization workflows in R using the glmnet ecosystem. Data preparation includes handling missing values, encoding categorical variables, scaling continuous features, and splitting data into training, validation, and test sets while respecting temporal ordering or cluster structure when present.

Model fitting proceeds through cv.glmnet() for automated cross-validation selecting optimal lambda or manual glmnet() calls for custom regularization paths. You understand when to specify alpha=0 (Ridge), alpha=1 (LASSO), or intermediate values (Elastic Net) based on feature correlation structure and business requirements. You can extract coefficients at optimal lambda, generate predictions on new data, and calculate performance metrics.

Diagnostic analysis detects problems through residual plots identifying heteroscedasticity or non-linearity, influence diagnostics flagging outliers disproportionately affecting model fit, and multicollinearity assessment through variance inflation factors. These diagnostics inform whether transformations, outlier handling, or alternative modeling approaches could improve results.

Production deployment requires building feature pipelines ensuring consistent feature engineering across training and scoring, implementing monitoring dashboards tracking performance degradation, establishing retraining protocols triggered by drift or accuracy decline, and creating documentation enabling stakeholder understanding and operational maintenance.

---

# Decision Frameworks

**Systematic approach to regularization method selection**

```{r decision-framework, echo=FALSE}
framework <- tibble(
  Consideration = c(
    "Feature Correlation",
    "Interpretability Need",
    "Feature Quality",
    "Business Constraints",
    "Computational Budget",
    "Regulatory Environment"
  ),
  `Ridge (Œ±=0)` = c(
    "High multicollinearity",
    "Coefficient stability matters",
    "All features informative",
    "Must retain all features",
    "Fast scoring required",
    "Must explain all relationships"
  ),
  `LASSO (Œ±=1)` = c(
    "Low correlation",
    "Feature selection critical",
    "Many irrelevant features",
    "Feature costs high",
    "Minimal features needed",
    "Must justify each feature"
  ),
  `Elastic Net (0<Œ±<1)` = c(
    "Moderate correlation",
    "Balance selection and stability",
    "Mixed quality features",
    "Some features expensive",
    "Moderate complexity ok",
    "Flexible requirements"
  )
)

kable(framework, format = "html") %>%
  kable_styling(font_size = 14) %>%
  column_spec(1, bold = TRUE, width = "15em") %>%
  column_spec(2:4, width = "12em")
```

This framework guides initial method selection, but empirical comparison through cross-validation should validate theoretical expectations. When multiple methods achieve similar predictive performance, secondary criteria like interpretability, computational efficiency, or stakeholder preferences determine final selection.

---

# Common Pitfalls and Solutions

**Avoiding frequent mistakes**

Treating regularization as substitute for feature engineering wastes opportunities for domain knowledge integration. Regularization handles correlations and selects from available features but cannot create informative features from uninformative raw data. Invest effort in thoughtful feature engineering before relying on regularization to perform magic.

Optimizing hyperparameters on test data produces optimistic performance estimates and invalidates held-out evaluation. Lambda selection must use only training data through cross-validation. The test set remains untouched until final model evaluation. Iterative hyperparameter adjustment based on test performance constitutes indirect training on test data biasing results.

Ignoring problem-specific structure when applying generic regularization sacrifices performance. Features with natural groupings benefit from group LASSO. Ordered features (time, space) benefit from fused LASSO. Correlated feature sets benefit from Elastic Net. Default LASSO application without considering structure often suboptimal.

Neglecting stakeholder communication leads to technically excellent models failing adoption. Business leaders need results framed in business impact (revenue, cost, risk) not statistical metrics (RMSE, AIC). Operations teams need implementation documentation enabling maintenance without data science intervention. Effective communication requires translating technical achievements into stakeholder-relevant language.

---

# Extensions and Connections

**Regularization's broader role in machine learning**

Regularization principles extend beyond linear regression to virtually all machine learning methods. Regularized logistic regression handles classification tasks with similar L1/L2 penalties preventing overfitting to training data. Regularized Cox models enable survival analysis with censored time-to-event outcomes. Regularized generalized additive models combine penalized splines with automatic smoothing parameter selection.

Tree-based methods employ implicit regularization through constraints on tree depth, minimum node size, and leaf count. Random forests combine regularization through subsampling features and observations. Gradient boosting machines incorporate regularization through learning rate control, tree complexity penalties, and early stopping. These methods represent alternative regularization philosophies achieving similar goals of controlling model complexity.

Neural networks utilize diverse regularization techniques including weight decay (L2 penalty on connection weights), dropout (randomly zeroing activations during training), batch normalization (normalizing layer inputs), and early stopping (terminating training before convergence). Deep learning's success partly attributable to effective regularization enabling models with millions of parameters to generalize from thousands of training examples.

The fundamental insight that simpler models often generalize better than complex models transcends specific methods, representing core principle of statistical learning theory. Regularization formalizes Occam's razor preferring parsimonious explanations unless data strongly support additional complexity.

---

# When Regularization Isn't Enough

**Knowing the limits**

Regularization assumes fundamentally correct model structure with appropriate features but potentially unstable coefficient estimates. When core assumptions fail, regularization cannot rescue the model. Non-linear relationships between features and outcomes require transformations, polynomials, or non-parametric methods before regularization applies. Interactions between features may need explicit encoding if implicit interaction effects are weak.

Severe data quality problems including systematic measurement error, missing not at random patterns, or selection bias require data collection improvements rather than modeling solutions. Regularization cannot fix fundamentally flawed data. Investment in data quality often provides higher returns than sophisticated modeling on poor data.

When true model complexity exceeds data capacity (attempting to estimate 1000 parameters from 100 observations), no amount of regularization enables reliable estimation. These settings require either additional data collection, aggressive dimensionality reduction before modeling, or acceptance that reliable estimation impossible given data constraints. Recognizing when problems exceed available data prevents wasting effort on doomed modeling attempts.

Domain expertise sometimes contradicts data-driven conclusions from regularized models. When LASSO excludes features known from causal theory to be important, this indicates data limitations or model misspecification rather than proving features irrelevant. Blind adherence to algorithmic feature selection ignoring domain knowledge risks building models contradicting established scientific principles.

---

# Success Metrics

**Measuring regularization effectiveness**

Technical success measured through improved validation set performance compared to unregularized alternatives. Regularization succeeds when it reduces validation RMSE, increases validation R¬≤, improves calibration, and enhances coefficient stability across resampling. These metrics directly assess regularization's statistical objectives of controlling overfitting and improving generalization.

Business success measured through outcomes mattering to stakeholders. Demand forecasting regularization succeeds when inventory costs decline and stockouts reduce. Credit scoring regularization succeeds when default predictions improve and regulatory approval obtained. Marketing targeting regularization succeeds when response rates increase and customer satisfaction improves. Technical excellence without business impact represents failed deployment regardless of impressive validation metrics.

Operational success measured through sustainable deployment maintaining performance over time. Models succeeding initially but degrading rapidly due to feature drift or requiring excessive manual intervention represent partial successes at best. Truly successful regularization enables production systems operating with minimal maintenance, automated monitoring detecting issues proactively, and retraining protocols maintaining accuracy as conditions evolve.

Organizational success measured through knowledge transfer and capability building. Individual project success matters less than building organizational competency in regularized modeling enabling future projects without external dependence. Successful deployments include documentation enabling others to understand and maintain models, training enabling stakeholders to interpret results correctly, and processes ensuring lessons learned propagate to subsequent efforts.

---

# Continuous Learning

**Deepening expertise beyond this lecture**

Theoretical foundations in statistical learning theory explain why regularization works through bias-variance decomposition, consistency theory, and asymptotic analysis. Hastie, Tibshirani, and Friedman's "Elements of Statistical Learning" provides comprehensive treatment connecting regularization to broader statistical principles. James, Witten, Hastie, and Tibshirani's "Introduction to Statistical Learning" offers more accessible introduction with R examples.

Specialized regularization methods address domain-specific problems. Network regularization for graph-structured data, spatial regularization for geographic data, tensor regularization for multi-way arrays, and functional regularization for curve data each require specialized techniques beyond standard Ridge/LASSO/Elastic Net. Academic papers in domain-specific journals (Biometrics, Technometrics, Journal of Machine Learning Research) present these extensions.

Software development skills enable production deployment at scale. Learning Docker containerization, Kubernetes orchestration, cloud platform APIs (AWS SageMaker, Azure ML, Google AI Platform), and MLOps tools (MLflow, Kubeflow) transforms academic understanding into production capability. DataCamp, Coursera, and platform-specific training resources provide practical skill development.

Case study analysis reveals patterns across successful and failed deployments. Reading industry blogs (Airbnb Engineering, Netflix TechBlog, Uber Engineering), attending conferences (KDD, ICML, NeurIPS), and participating in online communities (Cross Validated, r/MachineLearning) exposes diverse applications showing how practitioners adapt regularization to specific problems.

---

# Your Regularization Toolkit

**Complete skill inventory**

After completing this lecture and associated homework, you possess comprehensive regularization capabilities. You can diagnose overfitting through diagnostic plots and statistical tests recognizing when models require regularization. You understand the mathematical foundations of Ridge, LASSO, and Elastic Net penalties enabling principled method selection for new problems.

You can implement complete workflows in R using modern tools like glmnet, caret, and tidymodels performing data preprocessing, model fitting, cross-validation, hyperparameter tuning, and performance evaluation. You can visualize regularization paths providing intuitive understanding of feature importance and selection stability. You can generate predictions on new data and quantify uncertainty through appropriate intervals.

You can compare competing models through multiple lenses integrating predictive accuracy, model complexity, business value, and operational feasibility. You can design deployment architectures including feature pipelines, monitoring systems, retraining protocols, and stakeholder communication frameworks. You can document models enabling others to understand, maintain, and improve your work.

You can apply regularization across diverse domains adapting general principles to domain-specific requirements whether predicting customer behavior, financial outcomes, operational metrics, or healthcare costs. You can communicate technical results to non-technical audiences translating statistical findings into business-relevant insights and actionable recommendations.

---

# Homework 2 Overview

**Portfolio-worthy production model**

Homework 2 requires building production-ready regularized regression model demonstrating mastery of complete modeling workflow from raw data through deployed solution. This assignment integrates all lecture concepts into comprehensive project showcasing your capabilities to potential employers.

You will receive real-world dataset with messy characteristics including missing values, outliers, correlated features, and class imbalance requiring thoughtful preprocessing decisions. The dataset contains sufficient complexity to require genuine analytical thinking rather than applying cookbook procedures.

Your deliverables include exploratory data analysis documenting data quality issues and feature relationships, feature engineering pipeline with documented transformations and rationale, model development comparing Ridge, LASSO, and Elastic Net with justified selection, comprehensive evaluation using multiple validation strategies and performance metrics, deployment plan addressing monitoring and maintenance, and executive summary communicating business impact.

This homework represents capstone project for regression module demonstrating practical competency in applying regularization to solve real business problems. High-quality submissions belong in data science portfolios shown to prospective employers illustrating end-to-end modeling capabilities.

---

# Homework 2: Detailed Requirements

**Complete specifications**

**Dataset:** E-commerce customer lifetime value prediction with 100,000 customers, 150 features including demographics, transaction history, browsing behavior, marketing interactions, and customer service contacts. Target variable: 12-month forward-looking revenue (continuous outcome with right skew).

**Required Analysis Components:**

1. Exploratory Data Analysis: Univariate distributions, bivariate relationships, missing value patterns, outlier detection, correlation structure analysis. Deliverable: 5-10 page report with visualizations and written insights.

2. Feature Engineering: Handle missing values with documented imputation strategy, create derived features based on domain logic (recency, frequency, monetary metrics, engagement scores, behavioral flags), engineer interaction terms for important feature pairs, transform skewed variables appropriately. Deliverable: Documented R script with comments explaining each transformation.

3. Model Development: Split data (60% train, 20% validation, 20% test) with stratification on outcome quantiles. Fit and tune Ridge, LASSO, and Elastic Net using 10-fold cross-validation. Compare models using RMSE, MAE, R¬≤, and business metrics. Select optimal model with written justification. Deliverable: R Markdown report showing complete analysis.

---

# Homework 2: Requirements Continued

**Evaluation and deployment specifications**

4. Model Evaluation: Assess final model on held-out test set never used during development. Generate predictions and calculate performance metrics. Analyze residuals checking for patterns indicating model deficiencies. Evaluate performance across customer segments (high-value vs low-value, new vs returning, geographic regions). Calculate prediction intervals quantifying uncertainty. Deliverable: Evaluation report with diagnostic plots and segment analysis.

5. Deployment Plan: Design monitoring dashboard tracking prediction accuracy, feature drift, and business impact. Establish alert thresholds for performance degradation. Define retraining schedule (scheduled and triggered). Document feature requirements and expected ranges. Create rollback procedures for failed deployments. Deliverable: 3-5 page deployment documentation suitable for engineering teams.

6. Executive Summary: One-page business-focused summary for senior leadership. Explain model purpose, performance benchmarks, expected business impact (revenue lift, cost savings, operational efficiency), and deployment timeline. Use non-technical language with business-relevant metrics. Deliverable: Executive memo suitable for C-level distribution.

**Evaluation Criteria:** Technical rigor (40%), business relevance (30%), documentation quality (20%), presentation clarity (10%). High scores require demonstrating deep understanding of regularization principles, making justified modeling decisions, and communicating effectively to multiple audiences.

---

# Homework 2: Submission Guidelines

**Format and timeline requirements**

**Submission Format:** Single R Markdown file knitting to HTML containing all analyses with embedded code and narrative explanations. Separate executive summary as Word document or PDF suitable for standalone distribution. All source data, scripts, and outputs in organized GitHub repository demonstrating professional workflow.

**Timeline:** Assignment released today, due in 14 days allowing two weeks for completion. Interim checkpoint in 7 days where you submit EDA and feature engineering sections for feedback before proceeding to modeling. This checkpoint ensures you're on right track before investing time in extensive model comparison.

**Collaboration Policy:** You may discuss general approaches and technical questions with classmates, but all submitted work must be independently written. Copied code, analyses, or written content constitute academic integrity violations. Seeking help from teaching staff encouraged, copying from internet without attribution prohibited.

**Evaluation Rubric:** Technical analysis quality (40 points), business reasoning (30 points), documentation clarity (20 points), presentation quality (10 points). Detailed rubric provided in assignment document specifying evaluation criteria for each component enabling you to self-assess before submission.

**Late Policy:** 10% penalty per day late up to 3 days, assignments more than 3 days late not accepted without prior arrangement for documented extenuating circumstances. Checkpoint submission optional but strongly recommended for maximizing learning and final grade.

---

# Success Strategies

**Maximizing homework learning and grade**

Start early allowing time for iterative refinement. First pass through analysis often reveals issues requiring revision. Budget time for multiple drafts improving code quality, visualization aesthetics, and narrative clarity. Last-minute submissions typically show rushed work with preventable errors.

Focus on understanding over completion. The homework intends to deepen regularization understanding through hands-on application. Mindlessly copying code without understanding produces poor learning outcomes and obvious submissions. Take time understanding why certain approaches work while others fail.

Seek feedback proactively. Teaching staff available for questions and intermediate draft reviews. Explaining your approach and receiving feedback dramatically improves final submission quality while enhancing understanding. Students reluctant to seek help typically earn lower grades and learn less.

Treat assignment as portfolio piece. This homework represents substantial professional work suitable for showing potential employers. High-quality submissions with clear documentation, thoughtful analysis, and polished presentation demonstrate capabilities far beyond academic exercises. Invest appropriate effort producing work you're proud to showcase.

Balance technical depth with communication clarity. Demonstrating sophisticated technical skills matters, but communicating insights effectively matters equally. Your submissions should showcase both technical competency and communication ability. Impressive analyses explained poorly provide limited value to stakeholders who cannot understand them.

---

# Final Thoughts

**Regularization in context**

Regularization represents fundamental tool in modern data science, not merely academic topic for course completion. The principles you've learned apply broadly across machine learning methods and business applications. Understanding when and how to regularize separates practitioners building reliable production models from those producing impressive but unstable demonstrations.

The bias-variance tradeoff underlying regularization reflects deep truth about learning from finite data. You cannot simultaneously minimize bias and variance; optimal learning requires accepting some of each in appropriate balance. This principle extends beyond statistics to reasoning under uncertainty generally.

Professional data science requires balancing multiple objectives including predictive accuracy, interpretability, computational efficiency, operational maintainability, and stakeholder acceptance. Rarely does single criterion dominate; successful projects require integrated thinking across technical and organizational dimensions. Regularization illustrates this reality where statistical optimality must accommodate business constraints.

Your learning journey continues beyond this lecture. The concepts covered provide foundation for ongoing skill development through practice, reading, experimentation, and exposure to diverse applications. Mastery emerges from applying principles to new problems, failing, understanding why, and improving. Embrace this iterative learning process.

---

# Course Integration

**Connecting to other lectures**

Regularization builds on regression fundamentals from Lecture 1 extending basic OLS to handle complex real-world data. The diagnostic skills, statistical inference, and model evaluation frameworks from Lecture 1 remain essential; regularization adds tools for handling overfitting and high-dimensional data.

Upcoming lectures apply regularization principles to new problem types. Lecture 3 on classification uses regularized logistic regression for binary and multi-class outcomes. Lecture 4 on clustering employs regularization for high-dimensional clustering algorithms. Lecture 6 on neural networks incorporates regularization through dropout and weight decay. The patterns you've learned recur throughout the course.

Portfolio projects integrating multiple lectures will require combining regularized regression with other techniques. Predicting customer lifetime value might use regularized regression for monetary value prediction, clustering for customer segmentation, and classification for churn prediction. Real applications rarely partition cleanly into lecture topics; integration across methods becomes essential.

The analytical thinking developed through regularization study transfers broadly. Recognizing bias-variance tradeoffs, evaluating model complexity appropriately, validating honestly without peeking at test data, and communicating technical results effectively apply universally. These meta-skills matter as much as specific regularization techniques.

---

# Parting Wisdom

**Lessons beyond regularization**

Simple models often outperform complex models despite seeming less sophisticated. Regularization formalizes this insight by penalizing unnecessary complexity. In professional practice, remember that elaborate models requiring extensive maintenance and producing marginal improvements over simple alternatives rarely justify their complexity. Favor simplicity unless complexity demonstrably adds value.

Understanding methods deeply enables knowing when to deviate from standard practice. You've learned Ridge, LASSO, and Elastic Net thoroughly enough to recognize situations requiring alternatives or extensions. Shallow understanding produces rigid cookbook application; deep understanding enables flexible problem-solving. Continue building depth through practice and study.

Collaboration between data scientists and domain experts produces better results than either alone. Data scientists bring statistical rigor and computational tools; domain experts bring causal understanding and practical constraints. Regularized models benefit enormously from domain expertise guiding feature engineering, interpreting coefficients, and identifying unrealistic predictions. Cultivate collaborative relationships valuing diverse expertise.

Data science serves business objectives, not the reverse. Technical excellence matters only insofar as it delivers business value. When regularization improves predictions generating better decisions and measurable outcomes, it succeeds. When regularization produces marginally better validation metrics without business impact, it fails despite statistical superiority. Keep business objectives central to technical work.

---

class: center, middle

# Thank You!

## Questions?

### Office hours: [Schedule]
### Assignment posted on course website
### Next lecture: Classification Methods

---

class: center, middle

# Lecture 2 Complete

## You now possess comprehensive regularization capabilities

### Apply them wisely in Homework 2 and beyond

---
