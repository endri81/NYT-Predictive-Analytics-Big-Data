# DATA SCIENCE GLOSSARY

Quick reference for key terms used throughout the course.

## A

**Activation Function**: Non-linear transformation in neural networks that enables learning complex patterns (e.g., ReLU, sigmoid, tanh).

**AUC (Area Under Curve)**: Metric for classification model performance measuring area under ROC curve. Ranges from 0 to 1, with 0.5 being random guessing.

## B

**Backpropagation**: Algorithm for training neural networks by computing gradients of loss with respect to weights.

**Bias (Model)**: Systematic error from overly simplistic assumptions. High bias leads to underfitting.

**Bias (Algorithmic)**: Systematic unfairness in model predictions across different groups.

## C

**Classification**: Supervised learning task predicting categorical outcomes.

**Clustering**: Unsupervised learning task grouping similar observations.

**Confusion Matrix**: Table showing true positives, false positives, true negatives, false negatives.

**Cross-Validation**: Technique for assessing model performance using multiple train-test splits.

## D

**Decision Boundary**: The threshold or surface separating different predicted classes.

**Dimensionality Reduction**: Techniques reducing number of features while preserving information (e.g., PCA).

## E

**Ensemble**: Combining multiple models to improve predictions.

**Epoch**: One complete pass through training data during neural network training.

## F

**F1 Score**: Harmonic mean of precision and recall, useful for imbalanced datasets.

**Feature Engineering**: Creating new input variables from existing data to improve model performance.

## G

**Gradient Descent**: Optimization algorithm iteratively moving toward minimum loss.

## H

**Hyperparameter**: Model configuration set before training (e.g., learning rate, number of clusters).

## I

**Imbalanced Data**: Dataset where classes have very different frequencies.

## L

**Loss Function**: Metric quantifying model error that optimization seeks to minimize.

## M

**MAPE (Mean Absolute Percentage Error)**: Regression metric expressing error as percentage.

**Multicollinearity**: High correlation among predictor variables causing instability.

## N

**Normalization**: Scaling features to common range (e.g., 0 to 1).

## O

**Overfitting**: Model learning training data too well, performing poorly on new data.

**Optimization**: Process of finding model parameters that minimize loss.

## P

**Precision**: Among positive predictions, proportion that are correct. TP/(TP+FP).

**Principal Component Analysis (PCA)**: Dimensionality reduction finding orthogonal directions of maximum variance.

## R

**R-Squared**: Proportion of variance in dependent variable explained by model.

**Recall**: Among actual positives, proportion correctly identified. TP/(TP+FN).

**Regularization**: Techniques penalizing model complexity to prevent overfitting (Ridge, Lasso, Elastic Net).

**RMSE (Root Mean Squared Error)**: Common regression metric, square root of average squared errors.

**ROC Curve**: Plot of true positive rate vs false positive rate across classification thresholds.

## S

**Standardization**: Scaling features to mean 0 and standard deviation 1.

**Stochastic Gradient Descent (SGD)**: Gradient descent using small batches rather than entire dataset.

## T

**TF-IDF (Term Frequency-Inverse Document Frequency)**: Statistic reflecting word importance in document corpus.

**Train-Test Split**: Dividing data into training set for model building and test set for evaluation.

## U

**Underfitting**: Model too simple to capture patterns in data.

## V

**Validation Set**: Data subset used for hyperparameter tuning, separate from test set.

**Variance (Model)**: Error from model sensitivity to training data fluctuations. High variance leads to overfitting.

---

*This glossary will be expanded as course progresses.*

